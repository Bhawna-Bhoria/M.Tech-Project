{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e7be526-e684-4234-9066-11695e486172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b64f7e2e-4f71-414a-95a6-60ce05d4aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7322f932-f01d-46a7-aa40-a771253ea708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9df37908-4e42-4080-a952-6d5acb926ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "862deadb-8c89-4715-989a-0d4fd81ecc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install ipywidgets --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda7d8fb-6af1-4808-b077-33842fa7931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "39e79cec-790c-4173-8f22-e20fef736ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to confirm that GPU is available\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541b2bb4-0c2c-48cd-9957-0326738a172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 19 23:08:41 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.84       Driver Version: 460.84       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 19%   24C    P0    58W / 250W |      0MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 17%   25C    P0    58W / 250W |      0MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:07:00.0 Off |                  N/A |\n",
      "| 20%   22C    P8     7W / 250W |  10727MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "| 17%   26C    P0    57W / 250W |      0MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce GTX 108...  Off  | 00000000:0C:00.0 Off |                  N/A |\n",
      "| 18%   32C    P0    58W / 250W |      0MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce GTX 108...  Off  | 00000000:0D:00.0 Off |                  N/A |\n",
      "| 17%   25C    P0    58W / 250W |      0MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce GTX 108...  Off  | 00000000:0E:00.0 Off |                  N/A |\n",
      "| 17%   28C    P0    57W / 250W |      0MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce GTX 108...  Off  | 00000000:0F:00.0 Off |                  N/A |\n",
      "| 15%   26C    P0    58W / 250W |      0MiB / 11178MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    2   N/A  N/A     15019      C   /usr/bin/python3                10725MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d25fb8a0-6364-40d0-b9a0-9d171d6a3caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, \"0\" to  \"7\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "torch.cuda.set_device(0)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(n_gpu)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f77c38-79be-4d88-9afb-cadf35acb342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e3bc79ab-b939-4a83-846d-64069069b574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf279900-82a9-4dbe-a26a-f22e8ac3bb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba25f070-5016-4b85-845c-8e8663b84455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 19 23:09:42 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.84       Driver Version: 460.84       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 20%   28C    P2    52W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 17%   28C    P0    58W / 250W |      0MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:07:00.0 Off |                  N/A |\n",
      "| 20%   22C    P8     7W / 250W |  10727MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "| 17%   28C    P0    57W / 250W |      0MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce GTX 108...  Off  | 00000000:0C:00.0 Off |                  N/A |\n",
      "| 18%   34C    P0    58W / 250W |      0MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce GTX 108...  Off  | 00000000:0D:00.0 Off |                  N/A |\n",
      "| 17%   28C    P0    59W / 250W |      0MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce GTX 108...  Off  | 00000000:0E:00.0 Off |                  N/A |\n",
      "| 17%   30C    P0    57W / 250W |      0MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce GTX 108...  Off  | 00000000:0F:00.0 Off |                  N/A |\n",
      "| 15%   28C    P0    59W / 250W |      0MiB / 11178MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    2   N/A  N/A     15019      C   /usr/bin/python3                10725MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check we have a GPU and check the memory size of the GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "340acc79-3319-47aa-8140-07059948a04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /DATA/gupta92/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3640843f-766e-4110-b52e-59780f12f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a seed\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7712b672-2f06-47dd-906a-42ab51326a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fc22694-eff1-4618-ad9c-8b6110972cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4aaf87c-52e6-4e3d-870c-1955b9afd376",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e4420cd-d086-467d-8384-bc29bb4d91ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('DATA/english/c4_200m_550k.csv')\n",
    "df = df.head(50000)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ea1b9e5-8268-4d6b-a325-96882c95fc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The steps below describe how to remove data for one or more specifies areas and how to put on the data from a snapshot to the index</td>\n",
       "      <td>The steps below describe how to remove data for one ore more specific areas and how to put back the data from a snapshot to the index.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I wake up it\\'s usually comes out dreamsI\\'m thinking so my thoughts are very weird.</td>\n",
       "      <td>When I wake up it\\'s usually dreams I\\'m thinking about so my thoughts are very weird.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of the cardinal factors to be considered trying to decide on which kind of shipping to customer settle is the! market difference.</td>\n",
       "      <td>One of the cardinal factors to consider when trying to decide on which kind of shipping to settle for is the market difference.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Answers » Regions » Is in Nagorno-Karabakt region that part in Armenia?</td>\n",
       "      <td>Answers » Regions » Is Nagorno-Karabakh region part of Armenia?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flaneuring in fun at maple creek SK!</td>\n",
       "      <td>Flaneuring Fun in Maple Creek SK!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                   input  \\\n",
       "0    The steps below describe how to remove data for one or more specifies areas and how to put on the data from a snapshot to the index   \n",
       "1                                              When I wake up it\\'s usually comes out dreamsI\\'m thinking so my thoughts are very weird.   \n",
       "2  One of the cardinal factors to be considered trying to decide on which kind of shipping to customer settle is the! market difference.   \n",
       "3                                                                Answers » Regions » Is in Nagorno-Karabakt region that part in Armenia?   \n",
       "4                                                                                                   Flaneuring in fun at maple creek SK!   \n",
       "\n",
       "                                                                                                                                   output  \n",
       "0  The steps below describe how to remove data for one ore more specific areas and how to put back the data from a snapshot to the index.  \n",
       "1                                                  When I wake up it\\'s usually dreams I\\'m thinking about so my thoughts are very weird.  \n",
       "2         One of the cardinal factors to consider when trying to decide on which kind of shipping to settle for is the market difference.  \n",
       "3                                                                         Answers » Regions » Is Nagorno-Karabakh region part of Armenia?  \n",
       "4                                                                                                       Flaneuring Fun in Maple Creek SK!  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c81e7ec2-d0fa-45d6-be30-7c2e2877dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration, T5Tokenizer, \n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "  )\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b199d7a2-2b5d-4904-b204-a379c34aefca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /DATA/gupta92/.cache/huggingface/hub/models--t5-base/snapshots/a4986abc64c580966eb8dd0ed61ceafaeb0d1d45/spiece.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--t5-base/snapshots/a4986abc64c580966eb8dd0ed61ceafaeb0d1d45/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--t5-base/snapshots/a4986abc64c580966eb8dd0ed61ceafaeb0d1d45/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /DATA/gupta92/.cache/huggingface/hub/models--t5-base/snapshots/a4986abc64c580966eb8dd0ed61ceafaeb0d1d45/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = 't5-base'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aca04df4-d7de-4f44-9800-94eb1c5296ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_token_len(example):\n",
    "    #return len(tokenizer(example).input_ids)\n",
    "    return len(tokenizer(example[:512]).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9821b86-5a8b-4956-856c-d4c88d482f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45000, 2), (5000, 2))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train - Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.10, shuffle=True)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c514352d-1338-4a42-95a6-36151e6b0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['input_token_len'] = test_df['input'].apply(calc_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9df2945-fb4d-4f63-b24b-2571dceba51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>input_token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33553</th>\n",
       "      <td>1 office space 175,111 square feet</td>\n",
       "      <td>1 office space 175,111 square feet.</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9427</th>\n",
       "      <td>I request the input Danielle!</td>\n",
       "      <td>I appreciate the input Danielle!</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>In the last day, we will transfer you in to either Airport or turn you back to Tangier. End of our service.</td>\n",
       "      <td>On the last day, we will transfer you either to the Airport, or turn you back to Tangier. End of our service.</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>It does not use a 1 32 bit Download free screensavers cineravors time clock mode is The # 1 Free Employee Time Clock with advanced online scheduling and payroll timereproduction clockdisplayed sea Payroll cp free ever download for using forever Link in info area 1980x1080 MOV Alarm Clocks Online Alarm Clock Time Zone Clocks Military Time Military Time Converter Analog Alarm Clock White Noise Generator Video Alarm ountClock CaClock Countdown Timers Countdown.</td>\n",
       "      <td>It does not use a. 1 32 bit]. Download Free Screensavers 3D. Time Clock Hub is The # 1 Free Employee Time Clock with Advanced Online Scheduling and Payroll Timelapse clock: Clock Time Lapse Video Download CC Free to Use Forever Link in Info Area 9mb 1980x1080 MOV Alarm Clocks Online Alarm Clock Time Zone Clocks Military Time Military Time Converter Analog Alarm Clock White Noise Generator Video Alarm Clock Countdown Timers Countdown.</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39489</th>\n",
       "      <td>The rise of Chinese supertech giant: What is the implication of food companies?</td>\n",
       "      <td>The Rise of China's Supertech Giants: What Is the Implication for Food Companies?</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                input  \\\n",
       "33553                                                                                                                                                                                                                                                                                                                                                                                                                                              1 office space 175,111 square feet   \n",
       "9427                                                                                                                                                                                                                                                                                                                                                                                                                                                    I request the input Danielle!   \n",
       "199                                                                                                                                                                                                                                                                                                                                                                       In the last day, we will transfer you in to either Airport or turn you back to Tangier. End of our service.   \n",
       "12447  It does not use a 1 32 bit Download free screensavers cineravors time clock mode is The # 1 Free Employee Time Clock with advanced online scheduling and payroll timereproduction clockdisplayed sea Payroll cp free ever download for using forever Link in info area 1980x1080 MOV Alarm Clocks Online Alarm Clock Time Zone Clocks Military Time Military Time Converter Analog Alarm Clock White Noise Generator Video Alarm ountClock CaClock Countdown Timers Countdown.   \n",
       "39489                                                                                                                                                                                                                                                                                                                                                                                                 The rise of Chinese supertech giant: What is the implication of food companies?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                      output  \\\n",
       "33553                                                                                                                                                                                                                                                                                                                                                                                                                    1 office space 175,111 square feet.   \n",
       "9427                                                                                                                                                                                                                                                                                                                                                                                                                        I appreciate the input Danielle!   \n",
       "199                                                                                                                                                                                                                                                                                                                                            On the last day, we will transfer you either to the Airport, or turn you back to Tangier. End of our service.   \n",
       "12447  It does not use a. 1 32 bit]. Download Free Screensavers 3D. Time Clock Hub is The # 1 Free Employee Time Clock with Advanced Online Scheduling and Payroll Timelapse clock: Clock Time Lapse Video Download CC Free to Use Forever Link in Info Area 9mb 1980x1080 MOV Alarm Clocks Online Alarm Clock Time Zone Clocks Military Time Military Time Converter Analog Alarm Clock White Noise Generator Video Alarm Clock Countdown Timers Countdown.   \n",
       "39489                                                                                                                                                                                                                                                                                                                                                                      The Rise of China's Supertech Giants: What Is the Implication for Food Companies?   \n",
       "\n",
       "       input_token_len  \n",
       "33553                9  \n",
       "9427                 8  \n",
       "199                 27  \n",
       "12447              111  \n",
       "39489               18  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd321d92-6f3a-40ee-ba4a-907be30ea244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5000.000000\n",
       "mean       32.904400\n",
       "std        22.308106\n",
       "min         5.000000\n",
       "25%        17.000000\n",
       "50%        27.000000\n",
       "75%        42.000000\n",
       "max       307.000000\n",
       "Name: input_token_len, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['input_token_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5e933a2-ae5f-4c20-87b4-4dbb99b2c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use a token length of 64 since it will cover the vast majority of examples\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ca6fab2-ae06-441f-807e-da87e843249b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'input_token_len', '__index_level_0__'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d75fca39-9ccb-4bce-8a14-ca5d573cb60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GrammarDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer,print_text=False):         \n",
    "        self.dataset = dataset\n",
    "        self.pad_to_max_length = False\n",
    "        self.tokenizer = tokenizer\n",
    "        self.print_text = print_text\n",
    "        self.max_len = 64\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "    def tokenize_data(self, example):\n",
    "        input_, target_ = example['input'], example['output']\n",
    "\n",
    "        # tokenize inputs\n",
    "        tokenized_inputs = tokenizer(input_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "    \n",
    "        tokenized_targets = tokenizer(target_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "\n",
    "        inputs={\"input_ids\": tokenized_inputs['input_ids'],\n",
    "            \"attention_mask\": tokenized_inputs['attention_mask'],\n",
    "            \"labels\": tokenized_targets['input_ids']\n",
    "        }\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenize_data(self.dataset[index])\n",
    "        \n",
    "        if self.print_text:\n",
    "            for k in inputs.keys():\n",
    "                print(k, len(inputs[k]))\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60f992d8-8a36-4b53-a1a6-fa1282178e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids 24\n",
      "attention_mask 24\n",
      "labels 22\n",
      "{'input_ids': [2449, 691, 91, 128, 126, 1726, 4113, 44, 37, 10144, 8257, 3111, 44, 8, 5331, 3254, 11347, 1166, 44, 8561, 26, 9, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2449, 691, 91, 128, 126, 1726, 3598, 44, 37, 10144, 8257, 3111, 44, 8, 5331, 3254, 11347, 1166, 30, 1701, 5, 1]}\n"
     ]
    }
   ],
   "source": [
    "dataset = GrammarDataset(test_dataset, tokenizer, True)\n",
    "print(dataset[56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e6bd3fc-b11e-47a1-b473-10705ebe1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Evaluator\n",
    "#!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f50438a6-1ed2-460a-8bb0-307fa7b23806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf186c82-6bea-4bb3-b9f3-1b00df72ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b108ae3-2088-44fa-af0f-8b3583bd157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# defining training related arguments\n",
    "batch_size = 8\n",
    "args = Seq2SeqTrainingArguments(output_dir=\"models/english/c4_200m/weights\",\n",
    "                        evaluation_strategy=\"steps\",\n",
    "                        per_device_train_batch_size=batch_size,\n",
    "                        per_device_eval_batch_size=batch_size,\n",
    "                        learning_rate=2e-5,\n",
    "                        num_train_epochs=1,\n",
    "                        weight_decay=0.01,\n",
    "                        save_total_limit=2,\n",
    "                        predict_with_generate=True,\n",
    "                        fp16 = True,\n",
    "                        gradient_accumulation_steps = 6,\n",
    "                        eval_steps = 100,\n",
    "                        save_steps = 100,\n",
    "                        load_best_model_at_end=True,\n",
    "                        logging_dir=\"logs\",\n",
    "                        report_to = None    )\n",
    "                        #report_to=\"wandb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71128b6a-e41c-4418-9807-d1af017db8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /DATA/gupta92/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f437ee91-d2f4-48dc-b077-7f0f16b9f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d9743c7-8b58-4bee-9ab1-2fcd8032b17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# defining trainer using 🤗\n",
    "\n",
    "trainer = Seq2SeqTrainer(model=model, \n",
    "                args=args, \n",
    "                train_dataset= GrammarDataset(train_dataset, tokenizer),\n",
    "                eval_dataset=GrammarDataset(test_dataset, tokenizer),\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0b1333e-5e74-4a8f-981d-5fb3fe6e138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb login\n",
    "#bcdbdd5ee9d76a20c90b5f2a246eb45f14b341a9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fbcb9642-866a-44df-8b1f-ee0b26ca9a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#bcdbdd5ee9d76a20c90b5f2a246eb45f14b341a9\n",
    "\"\"\"\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"gec\", entity=\"gupta92\")\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "031ca0bc-a7a7-45d2-be7f-689ac3d853c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 45000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 6\n",
      "  Total optimization steps = 937\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='937' max='937' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [937/937 23:04, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.655472</td>\n",
       "      <td>71.211300</td>\n",
       "      <td>60.586800</td>\n",
       "      <td>70.458600</td>\n",
       "      <td>70.489300</td>\n",
       "      <td>17.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/english/c4_200m/weights/checkpoint-500\n",
      "Configuration saved in models/english/c4_200m/weights/checkpoint-500/config.json\n",
      "Model weights saved in models/english/c4_200m/weights/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in models/english/c4_200m/weights/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in models/english/c4_200m/weights/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from models/english/c4_200m/weights/checkpoint-500 (score: 0.6554718017578125).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=937, training_loss=0.7505102330617996, metrics={'train_runtime': 1385.8191, 'train_samples_per_second': 32.472, 'train_steps_per_second': 0.676, 'total_flos': 3050079774474240.0, 'train_loss': 0.7505102330617996, 'epoch': 1.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Wandb API Key: bcdbdd5ee9d76a20c90b5f2a246eb45f14b341a9\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49072bec-5ad7-492c-93b5-c0efcd92e631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to t5_gec_model_english_new\n",
      "Configuration saved in t5_gec_model_english_new/config.json\n",
      "Model weights saved in t5_gec_model_english_new/pytorch_model.bin\n",
      "tokenizer config file saved in t5_gec_model_english_new/tokenizer_config.json\n",
      "Special tokens file saved in t5_gec_model_english_new/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "#Save the trained Model\n",
    "trainer.save_model('t5_gec_model_english_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49ab8676-45c9-4007-aeef-6856b8b1c393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: t5_gec_model_english_new/ (stored 0%)\n",
      "  adding: t5_gec_model_english_new/spiece.model (deflated 48%)\n",
      "  adding: t5_gec_model_english_new/pytorch_model.bin (deflated 9%)\n",
      "  adding: t5_gec_model_english_new/special_tokens_map.json (deflated 86%)\n",
      "  adding: t5_gec_model_english_new/config.json (deflated 62%)\n",
      "  adding: t5_gec_model_english_new/training_args.bin (deflated 48%)\n",
      "  adding: t5_gec_model_english_new/tokenizer_config.json (deflated 82%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r 't5_gec_model_english_new.zip' 't5_gec_model_english_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95b60cb8-d0cb-4788-80a3-01e71d4d6af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /DATA/gupta92/.cache/huggingface/hub/models--t5-base/snapshots/a4986abc64c580966eb8dd0ed61ceafaeb0d1d45/spiece.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--t5-base/snapshots/a4986abc64c580966eb8dd0ed61ceafaeb0d1d45/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file t5_gec_model_english_new/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file t5_gec_model_english_new/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5_gec_model_english_new.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "model_name = 't5_gec_model_english_new'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model1 = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "model=model.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6ac5b88d-b87a-4bb6-b7fc-877cd898b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grammar(input_text,num_return_sequences):\n",
    "    batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\").to(torch_device)\n",
    "    translated = model1.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "30feaf63-60c9-4bf0-83c4-6f66d30e476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She is moving here.', 'She are moving here.']\n"
     ]
    }
   ],
   "source": [
    "text = 'She are moving here.'\n",
    "print(correct_grammar(text, num_return_sequences=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6e95fdbd-0bf9-4f05-9875-aa484313764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cat drink milk.', 'Cat drinks milk.', 'Cats drink milk.', 'Cat drinking milk.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Cat drink milk.'\n",
    "print(correct_grammar(text, num_return_sequences=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "949828cf-7213-4942-b12f-e039880e3b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You going home.', \"You're going home.\", 'You go home.', 'You are going home.']\n"
     ]
    }
   ],
   "source": [
    "text = 'You going home.'\n",
    "print(correct_grammar(text, num_return_sequences=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "74b00800-6d53-4382-be34-878ee95240c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /DATA/gupta92/.cache/huggingface/hub/models--deep-learning-analytics--GrammarCorrector/snapshots/6ca90bd771c373a0542d4257a5c34d26cd0d3c59/spiece.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--deep-learning-analytics--GrammarCorrector/snapshots/6ca90bd771c373a0542d4257a5c34d26cd0d3c59/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--deep-learning-analytics--GrammarCorrector/snapshots/6ca90bd771c373a0542d4257a5c34d26cd0d3c59/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--deep-learning-analytics--GrammarCorrector/snapshots/6ca90bd771c373a0542d4257a5c34d26cd0d3c59/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"deep-learning-analytics/GrammarCorrector\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /DATA/gupta92/.cache/huggingface/hub/models--deep-learning-analytics--GrammarCorrector/snapshots/6ca90bd771c373a0542d4257a5c34d26cd0d3c59/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at deep-learning-analytics/GrammarCorrector.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "model_name = 'deep-learning-analytics/GrammarCorrector'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32dcc95f-e908-47c5-a90b-957ef6c1dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grammar(input_text,num_return_sequences):\n",
    "    batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\").to(torch_device)\n",
    "    translated = model.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a54d899f-9c2d-44c4-b855-9bebe8f69fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He is moving here.', \"He's moving here.\"]\n"
     ]
    }
   ],
   "source": [
    "text = 'He are moving here.'\n",
    "print(correct_grammar(text, num_return_sequences=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c34bf06-a1b0-4d68-941a-0b53adcac40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cat drinks milk.', 'Cat drink milk.', 'Cat drank milk.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Cat drinked milk.'\n",
    "print(correct_grammar(text, num_return_sequences=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0e95aa-12c1-4437-a21e-d6c7f641b9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5_gec_model_english_new\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "model_name = 'deep-learning-analytics/GrammarCorrector'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#output_dir = '/content/drive/My Drive/saved_model/BERT_model'\n",
    "output_dir = 't5_gec_model_english_new'\n",
    "\n",
    "\n",
    "print(output_dir)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(output_dir)\n",
    "model = T5ForConditionalGeneration.from_pretrained(output_dir).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c843fa53-7a05-4c15-89da-3e006adbe1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36d7e4b6-e41e-45c9-ba71-fc0bfb7486ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5_gec_model_english_new\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "model_name = 'deep-learning-analytics/GrammarCorrector'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#output_dir = '/content/drive/My Drive/saved_model/BERT_model'\n",
    "output_dir = 't5_gec_model_english_new'\n",
    "\n",
    "\n",
    "print(output_dir)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(output_dir)\n",
    "model = T5ForConditionalGeneration.from_pretrained(output_dir).to(torch_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8c77f5-7cd9-4977-a1eb-1041510e431c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: Cat drinked milk.\n",
      "Predicted Sentence: Cat drank milk.\n",
      "___________________________________________________________________________\n",
      "Incorrect Sentence: I doing my M.Tech Project.\n",
      "Predicted Sentence: I'm doing my M.Tech Project.\n",
      "___________________________________________________________________________\n",
      "Incorrect Sentence: You is a kind person.\n",
      "Predicted Sentence: You're a kind person.\n",
      "___________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text = 'Cat drinked milk.'\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=4)[1]\n",
    "print(\"Predicted Sentence:\",predicted_s)\n",
    "print(\"___________________________________________________________________________\")\n",
    "\n",
    "text = 'I doing my M.Tech Project.'\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=4)[1]\n",
    "print(\"Predicted Sentence:\",predicted_s)\n",
    "print(\"___________________________________________________________________________\")\n",
    "\n",
    "text = 'You is a kind person.'\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=4)[1]\n",
    "print(\"Predicted Sentence:\",predicted_s)\n",
    "print(\"___________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c81b7bab-17bb-4d2c-a370-ec820282cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat drank milk.\n"
     ]
    }
   ],
   "source": [
    "text = 'Cat drinked milk.'\n",
    "print(correct_grammar(text, num_return_sequences=4)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725be20-db59-4666-9857-6434b502f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Cat drinked milk.'\n",
    "print(correct_grammar(text, num_return_sequences=4)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "13f37bd6-b443-4f06-bb48-48788c027851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing my M.Tech Project.\n"
     ]
    }
   ],
   "source": [
    "text = 'I doing my M.Tech Project.'\n",
    "print(correct_grammar(text, num_return_sequences=4)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0949a969-e1f6-4452-af92-f347f02be4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a kind person.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:                    train/epoch ▁\n",
      "wandb:              train/global_step ▁\n",
      "wandb:               train/total_flos ▁\n",
      "wandb:               train/train_loss ▁\n",
      "wandb:            train/train_runtime ▁\n",
      "wandb: train/train_samples_per_second ▁\n",
      "wandb:   train/train_steps_per_second ▁\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:                    train/epoch 0.96\n",
      "wandb:              train/global_step 18\n",
      "wandb:               train/total_flos 58888131010560.0\n",
      "wandb:               train/train_loss 1.25195\n",
      "wandb:            train/train_runtime 22.9697\n",
      "wandb: train/train_samples_per_second 39.182\n",
      "wandb:   train/train_steps_per_second 0.784\n",
      "wandb: \n",
      "wandb: You can sync this run to the cloud by running:\n",
      "wandb: wandb sync /DATA/gupta92/wandb/offline-run-20230119_230959-1y6x1lqf\n",
      "wandb: Find logs at: ./wandb/offline-run-20230119_230959-1y6x1lqf/logs\n"
     ]
    }
   ],
   "source": [
    "text = 'You is a kind person.'\n",
    "print(correct_grammar(text, num_return_sequences=4)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e57a1-cb52-4740-9080-ce4469060823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f13e2f-5b8a-45a6-aea8-db49dd92579c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
