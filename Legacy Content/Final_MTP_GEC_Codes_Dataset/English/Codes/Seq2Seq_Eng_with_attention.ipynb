{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a863645-a51e-42dd-8af4-c13efbce8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING THE REQUIRED LIBRARIES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm \n",
    "import tensorflow as tf\n",
    "from  tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from  sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87cbae0-97c1-44e6-9176-1d920d51bce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 22:15:43.478946: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-10-17 22:15:43.478977: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba487c1-d2c1-4c68-9292-0f97368b2acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, \"0\" to  \"7\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3,4,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f50a776-e696-4585-893e-823144a54064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "      <th>dec_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and he took in my favorite subject like soccer</td>\n",
       "      <td>and he took in my favorite subjects like soccer</td>\n",
       "      <td>and he took in my favorite subjects like soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actually who let me know about lang  8 was him</td>\n",
       "      <td>actually he was the one who let me know about ...</td>\n",
       "      <td>actually he was the one who let me know about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>his kanji is ability is much better than me</td>\n",
       "      <td>his kanji ability is much better than mine</td>\n",
       "      <td>his kanji ability is much better than mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we have known each other for only half a year ...</td>\n",
       "      <td>we have known each other for only half a year ...</td>\n",
       "      <td>we have known each other for only half a year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i heard a sentence last night when i watched tv</td>\n",
       "      <td>i heard a sentence last night when i was watch...</td>\n",
       "      <td>i heard a sentence last night when i was watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503896</th>\n",
       "      <td>i like thailand language because that pronounc...</td>\n",
       "      <td>i like thai because the pronunciation sounds cute</td>\n",
       "      <td>i like thai because the pronunciation sounds cute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503897</th>\n",
       "      <td>i ate kaomangai  rise with boild chikin  tomya...</td>\n",
       "      <td>i ate kaomangai  rice with boiled chickin  tom...</td>\n",
       "      <td>i ate kaomangai  rice with boiled chickin  tom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503898</th>\n",
       "      <td>i think it is important thing to become to lik...</td>\n",
       "      <td>i think it is important to like coriander in o...</td>\n",
       "      <td>i think it is important to like coriander in o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503899</th>\n",
       "      <td>yesterday i went to umeda station to date</td>\n",
       "      <td>i went to umeda station for dating yesterday</td>\n",
       "      <td>i went to umeda station for dating yesterday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503900</th>\n",
       "      <td>it said she want to make the meeting time at e...</td>\n",
       "      <td>she said she want to change the meeting time t...</td>\n",
       "      <td>she said she want to change the meeting time t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503901 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                enc_input  \\\n",
       "0          and he took in my favorite subject like soccer   \n",
       "1          actually who let me know about lang  8 was him   \n",
       "2             his kanji is ability is much better than me   \n",
       "3       we have known each other for only half a year ...   \n",
       "4         i heard a sentence last night when i watched tv   \n",
       "...                                                   ...   \n",
       "503896  i like thailand language because that pronounc...   \n",
       "503897  i ate kaomangai  rise with boild chikin  tomya...   \n",
       "503898  i think it is important thing to become to lik...   \n",
       "503899          yesterday i went to umeda station to date   \n",
       "503900  it said she want to make the meeting time at e...   \n",
       "\n",
       "                                                dec_input  \\\n",
       "0         and he took in my favorite subjects like soccer   \n",
       "1       actually he was the one who let me know about ...   \n",
       "2              his kanji ability is much better than mine   \n",
       "3       we have known each other for only half a year ...   \n",
       "4       i heard a sentence last night when i was watch...   \n",
       "...                                                   ...   \n",
       "503896  i like thai because the pronunciation sounds cute   \n",
       "503897  i ate kaomangai  rice with boiled chickin  tom...   \n",
       "503898  i think it is important to like coriander in o...   \n",
       "503899       i went to umeda station for dating yesterday   \n",
       "503900  she said she want to change the meeting time t...   \n",
       "\n",
       "                                               dec_output  \n",
       "0         and he took in my favorite subjects like soccer  \n",
       "1       actually he was the one who let me know about ...  \n",
       "2              his kanji ability is much better than mine  \n",
       "3       we have known each other for only half a year ...  \n",
       "4       i heard a sentence last night when i was watch...  \n",
       "...                                                   ...  \n",
       "503896  i like thai because the pronunciation sounds cute  \n",
       "503897  i ate kaomangai  rice with boiled chickin  tom...  \n",
       "503898  i think it is important to like coriander in o...  \n",
       "503899       i went to umeda station for dating yesterday  \n",
       "503900  she said she want to change the meeting time t...  \n",
       "\n",
       "[503901 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Dataset\n",
    "## LOADING THE PROCESSED DATASET\n",
    "df= pd.read_csv(\"DATA/processed_data_lang8.csv\")\n",
    "df.columns = [\"enc_input\",\"dec_input\"] \n",
    "df[\"dec_output\"] = df.dec_input\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5cc6ee3-3865-4358-a074-6143aaae3103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "      <th>dec_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and he took in my favorite subject like soccer</td>\n",
       "      <td>&lt;start&gt; and he took in my favorite subjects li...</td>\n",
       "      <td>and he took in my favorite subjects like socce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actually who let me know about lang  8 was him</td>\n",
       "      <td>&lt;start&gt; actually he was the one who let me kno...</td>\n",
       "      <td>actually he was the one who let me know about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>his kanji is ability is much better than me</td>\n",
       "      <td>&lt;start&gt; his kanji ability is much better than ...</td>\n",
       "      <td>his kanji ability is much better than mine &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we have known each other for only half a year ...</td>\n",
       "      <td>&lt;start&gt; we have known each other for only half...</td>\n",
       "      <td>we have known each other for only half a year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i heard a sentence last night when i watched tv</td>\n",
       "      <td>&lt;start&gt; i heard a sentence last night when i w...</td>\n",
       "      <td>i heard a sentence last night when i was watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503896</th>\n",
       "      <td>i like thailand language because that pronounc...</td>\n",
       "      <td>&lt;start&gt; i like thai because the pronunciation ...</td>\n",
       "      <td>i like thai because the pronunciation sounds c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503897</th>\n",
       "      <td>i ate kaomangai  rise with boild chikin  tomya...</td>\n",
       "      <td>&lt;start&gt; i ate kaomangai  rice with boiled chic...</td>\n",
       "      <td>i ate kaomangai  rice with boiled chickin  tom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503898</th>\n",
       "      <td>i think it is important thing to become to lik...</td>\n",
       "      <td>&lt;start&gt; i think it is important to like corian...</td>\n",
       "      <td>i think it is important to like coriander in o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503899</th>\n",
       "      <td>yesterday i went to umeda station to date</td>\n",
       "      <td>&lt;start&gt; i went to umeda station for dating yes...</td>\n",
       "      <td>i went to umeda station for dating yesterday &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503900</th>\n",
       "      <td>it said she want to make the meeting time at e...</td>\n",
       "      <td>&lt;start&gt; she said she want to change the meetin...</td>\n",
       "      <td>she said she want to change the meeting time t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503901 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                enc_input  \\\n",
       "0          and he took in my favorite subject like soccer   \n",
       "1          actually who let me know about lang  8 was him   \n",
       "2             his kanji is ability is much better than me   \n",
       "3       we have known each other for only half a year ...   \n",
       "4         i heard a sentence last night when i watched tv   \n",
       "...                                                   ...   \n",
       "503896  i like thailand language because that pronounc...   \n",
       "503897  i ate kaomangai  rise with boild chikin  tomya...   \n",
       "503898  i think it is important thing to become to lik...   \n",
       "503899          yesterday i went to umeda station to date   \n",
       "503900  it said she want to make the meeting time at e...   \n",
       "\n",
       "                                                dec_input  \\\n",
       "0       <start> and he took in my favorite subjects li...   \n",
       "1       <start> actually he was the one who let me kno...   \n",
       "2       <start> his kanji ability is much better than ...   \n",
       "3       <start> we have known each other for only half...   \n",
       "4       <start> i heard a sentence last night when i w...   \n",
       "...                                                   ...   \n",
       "503896  <start> i like thai because the pronunciation ...   \n",
       "503897  <start> i ate kaomangai  rice with boiled chic...   \n",
       "503898  <start> i think it is important to like corian...   \n",
       "503899  <start> i went to umeda station for dating yes...   \n",
       "503900  <start> she said she want to change the meetin...   \n",
       "\n",
       "                                               dec_output  \n",
       "0       and he took in my favorite subjects like socce...  \n",
       "1       actually he was the one who let me know about ...  \n",
       "2        his kanji ability is much better than mine <end>  \n",
       "3       we have known each other for only half a year ...  \n",
       "4       i heard a sentence last night when i was watch...  \n",
       "...                                                   ...  \n",
       "503896  i like thai because the pronunciation sounds c...  \n",
       "503897  i ate kaomangai  rice with boiled chickin  tom...  \n",
       "503898  i think it is important to like coriander in o...  \n",
       "503899  i went to umeda station for dating yesterday <...  \n",
       "503900  she said she want to change the meeting time t...  \n",
       "\n",
       "[503901 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## THE INPUTS TO THE DECODER REQUIRES SPECIAL TOKENS FOR THE START AND THE END SO WE ARE GOING TO USE \n",
    "## <start> AS BEGINING TOKEN\n",
    "## <end>  AS END TOKEN\n",
    "\n",
    "df[\"dec_input\"]= \"<start> \" + df[\"dec_input\"]\n",
    "df[\"dec_output\"] =  df[\"dec_output\"] + \" <end>\" \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dde05eb-8464-4e51-8a2f-85b8bd8ac98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(503901, 3)\n",
      "(100780, 3)\n"
     ]
    }
   ],
   "source": [
    "# Splitting And Sampling around 100k datapoints\n",
    "#THE TOTAL DATASET HAS 500K DATAPOINTS WHICH WILL TAKE MUCH HIGHER TRAINING TIME. THEREFORE I AM SAMPLING ONE-FIFTH OF THE TOTAL DATASET\n",
    "\n",
    "#df_sampled = pd.concat((df[df.enc_input].sample(frac= 0.2,random_state=1)))\n",
    "df_sampled = df.sample(frac = 0.2)\n",
    "print(df.shape)\n",
    "print(df_sampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd12f8b-d947-431d-8f6f-f184df2879b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ONCE THE DATA IS SAMPLED WE ARE SPLITTIND THE DATA IN TO TRAIN AND TEST\n",
    "df_train ,df_val = train_test_split(df_sampled,test_size=0.2,random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f50002f5-a360-400b-82f5-f605eaa15556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "      <th>dec_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>298469</th>\n",
       "      <td>i try to write english</td>\n",
       "      <td>&lt;start&gt; i try to write in english &lt;end&gt;</td>\n",
       "      <td>i try to write in english &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431393</th>\n",
       "      <td>we could not speak korea neither</td>\n",
       "      <td>&lt;start&gt; we could not speak korean either</td>\n",
       "      <td>we could not speak korean either &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193781</th>\n",
       "      <td>i wonder if book stores will be able to surviv...</td>\n",
       "      <td>&lt;start&gt; i wonder  nbsp  how book stores will b...</td>\n",
       "      <td>i wonder  nbsp  how book stores will be able t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13435</th>\n",
       "      <td>i wonder how much it cost to buy furniture ele...</td>\n",
       "      <td>&lt;start&gt; i wonder how much it will cost to buy ...</td>\n",
       "      <td>i wonder how much it will cost to buy furnitur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403221</th>\n",
       "      <td>the cost is covers by insurance company</td>\n",
       "      <td>&lt;start&gt; the cost is covered by an insurance co...</td>\n",
       "      <td>the cost is covered by an insurance company &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241570</th>\n",
       "      <td>today i was twenty minutes late to my first cl...</td>\n",
       "      <td>&lt;start&gt; today i was twenty minutes late to fir...</td>\n",
       "      <td>today i was twenty minutes late to first perio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306559</th>\n",
       "      <td>otherwise i would never be able to help my cus...</td>\n",
       "      <td>&lt;start&gt; otherwise i would never be able to hel...</td>\n",
       "      <td>otherwise i would never be able to help my cus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401908</th>\n",
       "      <td>if he  she is fascinated by the amazing scener...</td>\n",
       "      <td>&lt;start&gt; if one is fascinated by the amazing sc...</td>\n",
       "      <td>if one is fascinated by the amazing scenery of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152383</th>\n",
       "      <td>after drinking i saw some unbelievable young g...</td>\n",
       "      <td>&lt;start&gt; after drinking i saw some unbelievable...</td>\n",
       "      <td>after drinking i saw some unbelievable young g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151501</th>\n",
       "      <td>so i am quite busy and these days i am stressful</td>\n",
       "      <td>&lt;start&gt; so i am quite busy and stressed these ...</td>\n",
       "      <td>so i am quite busy and stressed these days &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80624 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                enc_input  \\\n",
       "298469                             i try to write english   \n",
       "431393                   we could not speak korea neither   \n",
       "193781  i wonder if book stores will be able to surviv...   \n",
       "13435   i wonder how much it cost to buy furniture ele...   \n",
       "403221            the cost is covers by insurance company   \n",
       "...                                                   ...   \n",
       "241570  today i was twenty minutes late to my first cl...   \n",
       "306559  otherwise i would never be able to help my cus...   \n",
       "401908  if he  she is fascinated by the amazing scener...   \n",
       "152383  after drinking i saw some unbelievable young g...   \n",
       "151501   so i am quite busy and these days i am stressful   \n",
       "\n",
       "                                                dec_input  \\\n",
       "298469            <start> i try to write in english <end>   \n",
       "431393           <start> we could not speak korean either   \n",
       "193781  <start> i wonder  nbsp  how book stores will b...   \n",
       "13435   <start> i wonder how much it will cost to buy ...   \n",
       "403221  <start> the cost is covered by an insurance co...   \n",
       "...                                                   ...   \n",
       "241570  <start> today i was twenty minutes late to fir...   \n",
       "306559  <start> otherwise i would never be able to hel...   \n",
       "401908  <start> if one is fascinated by the amazing sc...   \n",
       "152383  <start> after drinking i saw some unbelievable...   \n",
       "151501  <start> so i am quite busy and stressed these ...   \n",
       "\n",
       "                                               dec_output  \n",
       "298469                    i try to write in english <end>  \n",
       "431393             we could not speak korean either <end>  \n",
       "193781  i wonder  nbsp  how book stores will be able t...  \n",
       "13435   i wonder how much it will cost to buy furnitur...  \n",
       "403221  the cost is covered by an insurance company <end>  \n",
       "...                                                   ...  \n",
       "241570  today i was twenty minutes late to first perio...  \n",
       "306559  otherwise i would never be able to help my cus...  \n",
       "401908  if one is fascinated by the amazing scenery of...  \n",
       "152383  after drinking i saw some unbelievable young g...  \n",
       "151501   so i am quite busy and stressed these days <end>  \n",
       "\n",
       "[80624 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## IN THE COLUMN WHICH HAS DECODER INPUTS ADDING \"<end>\" TOKEN TO BE LEARNED BY THE TOKENIZER\n",
    "df_train[\"dec_input\"].iloc[0]  = df_train.iloc[0][\"dec_input\"] + \" <end>\"\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afb03693-28e3-4bd3-923b-194ff39f1420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "      <th>dec_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>357371</th>\n",
       "      <td>eventually i returned to my usual pc that runs xp</td>\n",
       "      <td>&lt;start&gt; eventually i returned to my usual comp...</td>\n",
       "      <td>eventually i returned to my usual computer whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416692</th>\n",
       "      <td>i contrived to arrive in time after all but my...</td>\n",
       "      <td>&lt;start&gt; i managed to arrive in time after all ...</td>\n",
       "      <td>i managed to arrive in time after all but my k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218309</th>\n",
       "      <td>so i am in office now because i have to transl...</td>\n",
       "      <td>&lt;start&gt; so i am in office now because i have t...</td>\n",
       "      <td>so i am in office now because i have to transl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385646</th>\n",
       "      <td>whether or not we can win the tournament this ...</td>\n",
       "      <td>&lt;start&gt; whether or not we can win the tourname...</td>\n",
       "      <td>whether or not we can win the tournament this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314161</th>\n",
       "      <td>but ordinary one is sometimes very delicious</td>\n",
       "      <td>&lt;start&gt; but sometimes ordinary ones are very d...</td>\n",
       "      <td>but sometimes ordinary ones are very delicious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157003</th>\n",
       "      <td>instead of that the entrance fee is reasonable...</td>\n",
       "      <td>&lt;start&gt; instead the entrance fee is reasonable...</td>\n",
       "      <td>instead the entrance fee is reasonable and the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35566</th>\n",
       "      <td>when i was college student i worked at coffee ...</td>\n",
       "      <td>&lt;start&gt; when i was college student i worked at...</td>\n",
       "      <td>when i was college student i worked at a coffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36390</th>\n",
       "      <td>it is so happy for me because it is a chance t...</td>\n",
       "      <td>&lt;start&gt; it is such a happy time for me becasus...</td>\n",
       "      <td>it is such a happy time for me becasuse it is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224812</th>\n",
       "      <td>i participate in the examination called sia</td>\n",
       "      <td>&lt;start&gt; i will participate in the examination ...</td>\n",
       "      <td>i will participate in the examination called s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89505</th>\n",
       "      <td>later while he was not at his desk his boss ca...</td>\n",
       "      <td>&lt;start&gt; later while he was not at his desk his...</td>\n",
       "      <td>later while he was not at his desk his boss an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20156 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                enc_input  \\\n",
       "357371  eventually i returned to my usual pc that runs xp   \n",
       "416692  i contrived to arrive in time after all but my...   \n",
       "218309  so i am in office now because i have to transl...   \n",
       "385646  whether or not we can win the tournament this ...   \n",
       "314161       but ordinary one is sometimes very delicious   \n",
       "...                                                   ...   \n",
       "157003  instead of that the entrance fee is reasonable...   \n",
       "35566   when i was college student i worked at coffee ...   \n",
       "36390   it is so happy for me because it is a chance t...   \n",
       "224812        i participate in the examination called sia   \n",
       "89505   later while he was not at his desk his boss ca...   \n",
       "\n",
       "                                                dec_input  \\\n",
       "357371  <start> eventually i returned to my usual comp...   \n",
       "416692  <start> i managed to arrive in time after all ...   \n",
       "218309  <start> so i am in office now because i have t...   \n",
       "385646  <start> whether or not we can win the tourname...   \n",
       "314161  <start> but sometimes ordinary ones are very d...   \n",
       "...                                                   ...   \n",
       "157003  <start> instead the entrance fee is reasonable...   \n",
       "35566   <start> when i was college student i worked at...   \n",
       "36390   <start> it is such a happy time for me becasus...   \n",
       "224812  <start> i will participate in the examination ...   \n",
       "89505   <start> later while he was not at his desk his...   \n",
       "\n",
       "                                               dec_output  \n",
       "357371  eventually i returned to my usual computer whi...  \n",
       "416692  i managed to arrive in time after all but my k...  \n",
       "218309  so i am in office now because i have to transl...  \n",
       "385646  whether or not we can win the tournament this ...  \n",
       "314161  but sometimes ordinary ones are very delicious...  \n",
       "...                                                   ...  \n",
       "157003  instead the entrance fee is reasonable and the...  \n",
       "35566   when i was college student i worked at a coffe...  \n",
       "36390   it is such a happy time for me becasuse it is ...  \n",
       "224812  i will participate in the examination called s...  \n",
       "89505   later while he was not at his desk his boss an...  \n",
       "\n",
       "[20156 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## VALIDATION DATA\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e439697b-9cea-4366-a3e1-1c2e0eda0cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "      <th>dec_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144764</th>\n",
       "      <td>na ka is the politely final particle in thai ...</td>\n",
       "      <td>&lt;start&gt;  na ka is the politely final particle ...</td>\n",
       "      <td>na ka is the politely final particle in thai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455909</th>\n",
       "      <td>even thought i know i should do all my best to...</td>\n",
       "      <td>&lt;start&gt; even though i know i should do all my ...</td>\n",
       "      <td>even though i know i should do all my best to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13948</th>\n",
       "      <td>a dialogue in a bus</td>\n",
       "      <td>&lt;start&gt; a dialogue on a bus</td>\n",
       "      <td>a dialogue on a bus  &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129170</th>\n",
       "      <td>as a result this wise emperor not only worked ...</td>\n",
       "      <td>&lt;start&gt; as a result this wise emperor not only...</td>\n",
       "      <td>as a result this wise emperor not only worked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402708</th>\n",
       "      <td>i do not like the exam of pediatrics for it is...</td>\n",
       "      <td>&lt;start&gt; i do not like the exam for pediatrics ...</td>\n",
       "      <td>i do not like the exam for pediatrics it is di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352610</th>\n",
       "      <td>it is about going to the poor country as a vol...</td>\n",
       "      <td>&lt;start&gt; it is about going to a poor countrysid...</td>\n",
       "      <td>it is about going to a poor countryside as a v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191594</th>\n",
       "      <td>we can only speak english there and there are ...</td>\n",
       "      <td>&lt;start&gt; while there we are only allowed to spe...</td>\n",
       "      <td>while there we are only allowed to speak engli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179915</th>\n",
       "      <td>whenever i have a time i will visit there to s...</td>\n",
       "      <td>&lt;start&gt; whenever i have the time i will visit ...</td>\n",
       "      <td>whenever i have the time i will visit there to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38631</th>\n",
       "      <td>what is more teachers can change the way of in...</td>\n",
       "      <td>&lt;start&gt; what is more teachers can change their...</td>\n",
       "      <td>what is more teachers can change their way of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388350</th>\n",
       "      <td>an american guest who was the only foreigner i...</td>\n",
       "      <td>&lt;start&gt; an american guest who was the only for...</td>\n",
       "      <td>an american guest who was the only foreigner o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                enc_input  \\\n",
       "144764   na ka is the politely final particle in thai ...   \n",
       "455909  even thought i know i should do all my best to...   \n",
       "13948                                a dialogue in a bus    \n",
       "129170  as a result this wise emperor not only worked ...   \n",
       "402708  i do not like the exam of pediatrics for it is...   \n",
       "...                                                   ...   \n",
       "352610  it is about going to the poor country as a vol...   \n",
       "191594  we can only speak english there and there are ...   \n",
       "179915  whenever i have a time i will visit there to s...   \n",
       "38631   what is more teachers can change the way of in...   \n",
       "388350  an american guest who was the only foreigner i...   \n",
       "\n",
       "                                                dec_input  \\\n",
       "144764  <start>  na ka is the politely final particle ...   \n",
       "455909  <start> even though i know i should do all my ...   \n",
       "13948                        <start> a dialogue on a bus    \n",
       "129170  <start> as a result this wise emperor not only...   \n",
       "402708  <start> i do not like the exam for pediatrics ...   \n",
       "...                                                   ...   \n",
       "352610  <start> it is about going to a poor countrysid...   \n",
       "191594  <start> while there we are only allowed to spe...   \n",
       "179915  <start> whenever i have the time i will visit ...   \n",
       "38631   <start> what is more teachers can change their...   \n",
       "388350  <start> an american guest who was the only for...   \n",
       "\n",
       "                                               dec_output  \n",
       "144764   na ka is the politely final particle in thai ...  \n",
       "455909  even though i know i should do all my best to ...  \n",
       "13948                          a dialogue on a bus  <end>  \n",
       "129170  as a result this wise emperor not only worked ...  \n",
       "402708  i do not like the exam for pediatrics it is di...  \n",
       "...                                                   ...  \n",
       "352610  it is about going to a poor countryside as a v...  \n",
       "191594  while there we are only allowed to speak engli...  \n",
       "179915  whenever i have the time i will visit there to...  \n",
       "38631   what is more teachers can change their way of ...  \n",
       "388350  an american guest who was the only foreigner o...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## HERE I AM SAMPLING 1000 POINTS FROM THE DATAFRAME AS TEST DATA WHICH ARE NOT PRESEENT IN THE TRAIN AND VALIDAION DATA\n",
    "np.random.seed(5) \n",
    "df_test = df.loc[np.random.choice(np.array([x for x in df.index.values if x not in df_sampled.index.values]),1000,replace= False,)]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4ab7621-b51d-4212-8ea5-d1c4ce4e1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92cb1da3-c76f-4133-a17c-b2eb06928469",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TOKENIZER FOR ENCODER INPUT\n",
    "tk_inp = Tokenizer()\n",
    "tk_inp.fit_on_texts(df_train.enc_input.apply(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54f2d588-540c-49f0-b5c4-30f1e779cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER FOR DECODER INPUT\n",
    "tk_out = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n' )\n",
    "tk_out.fit_on_texts(df_train.dec_input.apply(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "945e76da-1639-49bc-9d36-82b8790ae542",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS CLASS CONVERTS TEXT DATA TO INTEGER SEQUENCES AND RETURNS THE PADDED SEQUENCES\n",
    "class Dataset :\n",
    "    def __init__(self, data , tk_inp ,tk_out, max_len):\n",
    "        ## SETTING THE REQUIRED ATTRIBUTES\n",
    "        self.encoder_inp = data[\"enc_input\"].apply(str).values\n",
    "        self.decoder_inp = data[\"dec_input\"].apply(str).values\n",
    "        self.decoder_out = data[\"dec_output\"].apply(str).values\n",
    "        self.tk_inp = tk_inp\n",
    "        self.tk_out = tk_out\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self,i):\n",
    "        # INPUT SEQUENCES\n",
    "        self.encoder_seq = self.tk_inp.texts_to_sequences([self.encoder_inp[i]])\n",
    "        # DECODER INPUT SEQUENCES \n",
    "        self.decoder_inp_seq = self.tk_out.texts_to_sequences([self.decoder_inp[i]])\n",
    "        # DECODER INPUT SEQUENCES\n",
    "        self.decoder_out_seq = self.tk_out.texts_to_sequences([self.decoder_out[i]])\n",
    "        \n",
    "        # PADDING THE ENCODER INPUT SEQUENCES\n",
    "        self.encoder_seq = pad_sequences(self.encoder_seq, padding=\"post\",maxlen = self.max_len)\n",
    "        # PADDING THE DECODER INPUT SEQUENCES\n",
    "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, padding=\"post\",maxlen = self.max_len)\n",
    "        # PADDING DECODER OUTPUT SEQUENCES\n",
    "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq ,padding=\"post\", maxlen = self.max_len)\n",
    "        return self.encoder_seq ,  self.decoder_inp_seq,  self.decoder_out_seq\n",
    "    \n",
    "    def __len__(self):\n",
    "        # RETURN THE LEN OF INPUT ENDODER\n",
    "        return len(self.encoder_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fbe536d-9ccb-43ab-958c-5fc95fda60ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS CLASS CONVERTES THE DATASET INTO THE REQUIRED BATCH SIZE\n",
    "\n",
    "class Dataloader(tf.keras.utils.Sequence):\n",
    "    def __init__(self,batch_size,dataset):\n",
    "        # INTIALIZING THE REQUIRED VARIABLES \n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.totl_points = self.dataset.encoder_inp.shape[0]\n",
    "        \n",
    "    def __getitem__(self,i):\n",
    "        # STATING THE START AND STOP VATIABLE CONTAINGING INDEX VALUES FOR EACH BATCH\n",
    "        start = i * self.batch_size\n",
    "        stop = (i+1)*self.batch_size\n",
    "        \n",
    "        # PLACEHOLDERS FOR BATCHED DATA\n",
    "        batch_enc =[]\n",
    "        batch_dec_input = []\n",
    "        batch_dec_out =[]\n",
    "\n",
    "        for j in range(start,stop): \n",
    "            \n",
    "            a,b,c = self.dataset[j] \n",
    "            batch_enc.append(a[0]) \n",
    "            batch_dec_input.append(b[0])\n",
    "            batch_dec_out.append(c[0]) \n",
    "        \n",
    "        # Conveting list to array   \n",
    "        batch_enc = (np.array(batch_enc)) \n",
    "        batch_dec_input = np.array(batch_dec_input)\n",
    "        batch_dec_out = np.array(batch_dec_out)\n",
    "        \n",
    "        return [batch_enc , batch_dec_input],batch_dec_out\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Returning the number of batches\n",
    "        return int(self.totl_points/self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3de6bf4b-b677-48fe-9d97-77ec683bd5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMING OBJECTS OF DATASET AND DATALOADER FOR TRAIN DATASET\n",
    "train_dataset = Dataset(df_train,tk_inp,tk_out,35)\n",
    "train_dataloader = Dataloader( batch_size = 512, dataset=train_dataset)\n",
    "\n",
    "# FORMING OBJECTS OF DATASET AND DATALOADER FOR VALIDATION DATASET\n",
    "val_dataset = Dataset(df_val , tk_inp,tk_out,35)\n",
    "val_dataloader = Dataloader(batch_size=512 , dataset=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86fab78b-5478-4d5d-9c03-7bd754099b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfb6a217-cf1c-4535-9d11-67c85e3010ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINING THE ENCODER LAYER AS A FUNCTION\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self, vocab_size,emb_dims, enc_units, input_length,batch_size):\n",
    "        super().__init__()\n",
    "        # INITIALIZING THE REQUIRED VARIABLES\n",
    "        self.batch_size=batch_size # BATHCH SIZE\n",
    "        self.enc_units = enc_units # ENCODER UNITS\n",
    "\n",
    "        # EMBEDDING LAYER\n",
    "        self.embedding= layers.Embedding(vocab_size ,emb_dims) \n",
    "        # LSTM LAYER WITH RETURN SEQ AND RETURN STATES\n",
    "        self.lstm = layers.LSTM(self.enc_units,return_state= True,return_sequences =  True) \n",
    "    def call(self, enc_input , states):\n",
    "      \n",
    "        # FORMING THE EMBEDDED VECTOR \n",
    "        emb = self.embedding(enc_input)\n",
    "        # PASSING THE EMBEDDED VECTIO THROUGH LSTM LAYERS \n",
    "        enc_output,state_h,state_c = self.lstm(emb,initial_state=states)\n",
    "        #RETURNING THE OUTPUT OF LSTM LAYER\n",
    "        return enc_output,state_h,state_c \n",
    "    def initialize(self,batch_size):\n",
    "\n",
    "        return tf.zeros(shape=(batch_size,self.enc_units)),tf.zeros(shape=(batch_size,self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "404a18ed-0bd8-465d-82d8-ceb74d7019f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS ATTNETION LAYER FOR DOT MODEL\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,units):\n",
    "        super().__init__()\n",
    "        # INITIALIZING THE DENSE LAYER W1\n",
    "        self.W1 = layers.Dense(units)\n",
    "        # INITIALIZING THE DENSE LAYER W2\n",
    "        self.W2 = layers.Dense(units)\n",
    "        # INITIALIZING THE DENSE LAYER V\n",
    "        self.v = layers.Dense(1)\n",
    "        \n",
    "    def call(self,enc_output,dec_state):\n",
    "        # EXPANDING THE DIMENSION OF DECODER STATE  EG. FROM (16,32) TO (16,32,1)\n",
    "        dec_state =  tf.expand_dims(dec_state,axis=1)\n",
    "        \n",
    "        # FINDING THE SCORE FOR CONCAT MODEL\n",
    "        score = self.v(tf.nn.tanh(\n",
    "            self.W1(dec_state)+ self.W2(enc_output)\n",
    "        ))\n",
    "        # APPLYING SOFTMAX TO THE AXIS 1\n",
    "        # OUPUT SHAPE = (16,13,1)\n",
    "        att_weights = tf.nn.softmax(score,axis=1)\n",
    "        \n",
    "        # CALCULATING THE CONTEXT VECTOR BY FIRST ELEMENTWISE MULTIPLICATION AND THEN ADDING THE AXIS 1\n",
    "        # (16,13,1)*(16,13,32)=(16,13,32)\n",
    "        context_vec  = att_weights* enc_output\n",
    "        \n",
    "        # (16,13,32) SUM AND REDUCE THE DIMENSION AT AXIS 1 => (16,32)\n",
    "        context_vec = tf.reduce_sum(context_vec,axis=1)\n",
    "        \n",
    "        # RETURNING THE CONTEXT VECTOR AND ATTENTION WEIGHTS\n",
    "        return context_vec,att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89bb7392-f7b7-452f-b9b7-f446100739e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Onestepdecoder(tf.keras.Model):\n",
    "    '''THIS MODEL OUTPUTS THE RESULT OF DECODER FOR ONE TIME SETP GIVEN THE INPUT FOR PRECIOVE TIME STEP'''\n",
    "    \n",
    "    def __init__(self, vocab_size,emb_dims, dec_units, input_len,att_units,batch_size):\n",
    "        super().__init__()\n",
    "        # INTITALIZING THE REQUIRED VARIABLES\n",
    "        # EMBEDDING LAYERS\n",
    "        self.emb = layers.Embedding(vocab_size,emb_dims,input_length= input_len)\n",
    "        # ATTENTION LAYER\n",
    "        self.att = Attention(att_units)\n",
    "        # LSTM LAYER\n",
    "        self.lstm = layers.LSTM(dec_units,return_sequences=True,return_state=True)\n",
    "        # DENSE LAYER\n",
    "        self.dense = layers.Dense(vocab_size,activation=\"softmax\")\n",
    "\n",
    "    def call(self, encoder_output , input , state_h,state_c):\n",
    "        # FORMING THE EMBEDDED VECTOR FOR THE WORD\n",
    "        # (32,1)=>(32,1,12)\n",
    "        emb = self.emb(input)\n",
    "\n",
    "        dec_output,dec_state_h,dec_state_c = self.lstm(emb, initial_state = [state_h,state_c] )\n",
    "\n",
    "        # GETTING THE CONTEXT VECTOR AND ATTENTION WEIGHTS BASED ON THE ENCODER OUTPUT AND  DECODER STATE_H\n",
    "        context_vec,alphas = self.att(encoder_output,dec_state_h)\n",
    "        \n",
    "        # CONCATINATING THE CONTEXT VECTOR(BY EXPANDING DIMENSION) AND ENBEDDED VECTOR\n",
    "        dense_input =  tf.concat([tf.expand_dims(context_vec,1),dec_output],axis=-1)\n",
    "        \n",
    "        # PASSING THE DECODER OUTPUT THROUGH DENSE LAYER WITH UNITS EQUAL TO VOCAB SIZE\n",
    "        fc = self.dense(dense_input)\n",
    "        \n",
    "        # RETURNING THE OUTPUT\n",
    "        return fc , dec_state_h , dec_state_c , alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41baa755-eeaf-47d4-9c29-438fad38ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    '''THIS MODEL PERFORMS THE WHOLE DECODER OPERATION FOR THE COMPLETE SENTENCE'''\n",
    "    def __init__(self, vocab_size,emb_dims, dec_units, input_len,att_units,batch_size):\n",
    "        super().__init__()\n",
    "        # INITIALIZING THE VARIABLES\n",
    "        # LENGTH OF INPUT SENTENCE\n",
    "        self.input_len = input_len\n",
    "        # ONE STEP DECODER\n",
    "        self.onestepdecoder = Onestepdecoder(vocab_size,emb_dims, dec_units, input_len,att_units,batch_size)\n",
    "\n",
    "    def call(self,dec_input,enc_output,state_h,state_c):\n",
    "        # THIS VATIABLE STORES THE VALUE OF STATE_H FOR THE PREVIOUS STATE\n",
    "        current_state_h = state_h \n",
    "        current_state_c = state_c\n",
    "        # THIS STORES THE DECODER OUTPUT FOR EACH TIME STEP\n",
    "        pred = []\n",
    "        # THIS STORED THE ALPHA VALUES\n",
    "        alpha_values = []\n",
    "        # FOR EACH WORD IN THE INPUT SENTENCE\n",
    "        for i in range(self.input_len):\n",
    "            \n",
    "            # CURRENT WORD TO INPUT TO ONE STEP DECODER\n",
    "            current_vec = dec_input[:,i]\n",
    "\n",
    "            # EXPANDING THE DIMENSION FOR THE WORD\n",
    "            current_vec = tf.expand_dims(current_vec,axis=-1)\n",
    "\n",
    "            # PERFORMING THE ONE STEP DECODER OPERATION \n",
    "            dec_output,dec_state_h,dec_state_c,alphas = self.onestepdecoder(enc_output ,current_vec,current_state_h,current_state_c)\n",
    "\n",
    "            #UPDATING THE CURRENT STATE_H\n",
    "            current_state_h = dec_state_h\n",
    "            current_state_c = dec_state_c\n",
    "\n",
    "            #APPENDING THE DECODER OUTPUT TO \"pred\" LIST\n",
    "            pred.append(dec_output)\n",
    "\n",
    "            # APPENDING THE ALPHA VALUES\n",
    "            alpha_values.append(alphas)\n",
    "            \n",
    "        # CONCATINATING ALL THE VALUES IN THE LIST\n",
    "        output = tf.concat(pred,axis=1)\n",
    "        # CONCATINATING ALL THE ALPHA VALUES IN THE LIST\n",
    "        alpha_values = tf.concat(alpha_values,axis = -1)\n",
    "        # RETURNING THE OUTPUT\n",
    "        return output , alpha_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "502a1cbe-d9bb-43ee-a757-9a5ff8178b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "    '''THIS MODEL COMBINES ALL THE LAYERS AND FORM IN ENCODER DECODER MODEL WITH ATTENTION MECHANISM'''\n",
    "    def __init__(self,enc_vocab_size,enc_emb_dim,enc_units,enc_input_length,\n",
    "             dec_vocab_size,dec_emb_dim,dec_units,dec_input_length ,att_units, batch_size):\n",
    "        # INITAILIZING ALL VARIABLES\n",
    "        super().__init__()\n",
    "        # BATCH SIZE\n",
    "        self.batch_size = batch_size\n",
    "        # INITIALIZING ENCODER LAYER\n",
    "        self.encoder = Encoder(enc_vocab_size, enc_emb_dim,enc_units, enc_input_length,batch_size)\n",
    "        # INITALIZING DECODER LAYER\n",
    "        self.decoder = Decoder(dec_vocab_size ,dec_emb_dim,dec_units,dec_input_length  ,att_units, batch_size)\n",
    "\n",
    "    def call(self,data):\n",
    "        # THE INPUT OF DATALOADER IS IN A LIST FORM FOR EACH BATCH IT GIVER TWO INPUTS\n",
    "        # INPUT1 IS FOR ENCODER\n",
    "        # INPUT2 IS FOR DECODER\n",
    "        inp1 , inp2 = data\n",
    "        # PASSING THE INPUT1 TO ENCODER LAYER\n",
    "        enc_output, enc_state_h, enc_state_c = self.encoder(inp1,self.encoder.initialize(self.batch_size))\n",
    "        # PASSING INPUT2 TO THE DECODER LAYER\n",
    "        dec_output , alphas = self.decoder(inp2 , enc_output,enc_state_h,enc_state_c)\n",
    "        # THE OUTPUT OF MODEL IS ONLY DECODER OUTPUT THE ALPHA VALUES ARE IGNORED HERE\n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f9a45ac-3e2b-4822-9d2f-e676e79027d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 22:16:13.902485: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# INITAILZING THE MODEL\n",
    "model = encoder_decoder(enc_vocab_size=len(tk_inp.word_index)+1,\n",
    "                         enc_emb_dim = 300,\n",
    "                         enc_units=256,enc_input_length=35,\n",
    "                         dec_vocab_size =len(tk_out.word_index)+1,\n",
    "                         dec_emb_dim =300,\n",
    "                         dec_units=256,\n",
    "                         dec_input_length = 35,\n",
    "                         \n",
    "                         att_units=256,\n",
    "                         batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b82db-e576-40d6-8af0-6e2b05b9ba46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7eeb09fe-82de-4a92-907f-3cee408fb00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback =[ tf.keras.callbacks.ModelCheckpoint( \"model_save/attention_concat_best.h5\",save_best_only=True,mode=\"min\" ,save_weights_only=True),\n",
    "           tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,verbose=1,min_delta=0.0001),\n",
    "            tf.keras.callbacks.TensorBoard(\"model_save/attention_concat_logs_save\",histogram_freq=1)\n",
    "]\n",
    "\n",
    "train_steps = train_dataloader.__len__()\n",
    "val_steps  = val_dataloader.__len__()\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f639839-6e88-4255-9762-6df79a637fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "157/157 [==============================] - 769s 5s/step - loss: 2.9408 - val_loss: 2.1825\n",
      "Epoch 2/50\n",
      "157/157 [==============================] - 683s 4s/step - loss: 2.1631 - val_loss: 2.0018\n",
      "Epoch 3/50\n",
      "157/157 [==============================] - 684s 4s/step - loss: 1.9732 - val_loss: 1.8471\n",
      "Epoch 4/50\n",
      "157/157 [==============================] - 681s 4s/step - loss: 1.8514 - val_loss: 1.7665\n",
      "Epoch 5/50\n",
      "157/157 [==============================] - 679s 4s/step - loss: 1.7676 - val_loss: 1.7069\n",
      "Epoch 6/50\n",
      "157/157 [==============================] - 677s 4s/step - loss: 1.6901 - val_loss: 1.6419\n",
      "Epoch 7/50\n",
      "157/157 [==============================] - 678s 4s/step - loss: 1.6016 - val_loss: 1.5506\n",
      "Epoch 8/50\n",
      "157/157 [==============================] - 677s 4s/step - loss: 1.4665 - val_loss: 1.4015\n",
      "Epoch 9/50\n",
      "157/157 [==============================] - 675s 4s/step - loss: 1.2531 - val_loss: 1.1556\n",
      "Epoch 10/50\n",
      "157/157 [==============================] - 678s 4s/step - loss: 0.9809 - val_loss: 0.9342\n",
      "Epoch 11/50\n",
      "157/157 [==============================] - 677s 4s/step - loss: 0.7859 - val_loss: 0.8205\n",
      "Epoch 12/50\n",
      "157/157 [==============================] - 676s 4s/step - loss: 0.6632 - val_loss: 0.7626\n",
      "Epoch 13/50\n",
      "157/157 [==============================] - 676s 4s/step - loss: 0.5782 - val_loss: 0.7154\n",
      "Epoch 14/50\n",
      "157/157 [==============================] - 676s 4s/step - loss: 0.5144 - val_loss: 0.6966\n",
      "Epoch 15/50\n",
      "157/157 [==============================] - 679s 4s/step - loss: 0.4611 - val_loss: 0.6696\n",
      "Epoch 16/50\n",
      "157/157 [==============================] - 676s 4s/step - loss: 0.4167 - val_loss: 0.6610\n",
      "Epoch 17/50\n",
      "157/157 [==============================] - 676s 4s/step - loss: 0.3825 - val_loss: 0.6517\n",
      "Epoch 18/50\n",
      "157/157 [==============================] - 677s 4s/step - loss: 0.3546 - val_loss: 0.6459\n",
      "Epoch 19/50\n",
      "157/157 [==============================] - 675s 4s/step - loss: 0.3324 - val_loss: 0.6430\n",
      "Epoch 20/50\n",
      "157/157 [==============================] - 676s 4s/step - loss: 0.3138 - val_loss: 0.6430\n",
      "Epoch 21/50\n",
      "157/157 [==============================] - 677s 4s/step - loss: 0.2974 - val_loss: 0.6442\n",
      "Epoch 22/50\n",
      "157/157 [==============================] - 675s 4s/step - loss: 0.2837 - val_loss: 0.6487\n",
      "Epoch 23/50\n",
      "157/157 [==============================] - 674s 4s/step - loss: 0.2722 - val_loss: 0.6495\n",
      "Epoch 24/50\n",
      "157/157 [==============================] - 675s 4s/step - loss: 0.2610 - val_loss: 0.6519\n",
      "Epoch 00024: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f52da0cfe10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataloader, steps_per_epoch=train_steps,epochs= 50,validation_data = val_dataloader,validation_steps =val_steps,callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "baca2ff7-551f-472b-a56b-f2f30257d97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Encoder)           multiple                  10050368  \n",
      "                                                                 \n",
      " decoder (Decoder)           multiple                  22305245  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,355,613\n",
      "Trainable params: 32,355,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build([(512,35),(512,35)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6385ae0-abd7-4656-9cd1-6c5537d667be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"model_save/attention_concat_best.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c161ed9c-070d-4aa2-ac0f-221e3098cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ita_text,model):\n",
    "    '''THIS FUNCTION IS USED IN INFERENCE TIME WHICH GIVEN ANY SENTENCE IN ITALIAN OUTPUTS THE ENGLISH SENTENCE AND ALPHA VALUES'''\n",
    "    # FORMING TOKENIZED SEQUENCES FOR INPUT SENTENCE\n",
    "    seq = tk_inp.texts_to_sequences([ita_text])\n",
    "    # PADDING THE SEQUENCES\n",
    "    seq = pad_sequences(seq,maxlen = 20 , padding=\"post\")\n",
    "    # INITIALIZING THE STATES FOR INPUTING TO ENCODER\n",
    "    state = model.layers[0].initialize(1)\n",
    "    # GETTING THE ENCODED OUTPUT\n",
    "    enc_output,state_h,state_c= model.layers[0](seq,state)\n",
    "    # VARIABLE TO STORE PREDICTED SENTENCE\n",
    "    pred = []\n",
    "    # THIS VARIABLE STORES THE STATE TO BE INPUTED TO ONE STEP ENCODER\n",
    "    input_state_h = state_h\n",
    "    input_state_c = state_c\n",
    "    # THIS VARIABLE STORES THE VECTOR TO VE INPUTED TO ONE STEP ENCODER\n",
    "    current_vec = tf.ones((1,1))\n",
    "    # THIS VARIABLE WILL STORE ALL THE ALPHA VALUES OUTPUTS\n",
    "    alpha_values = []\n",
    "\n",
    "    for i in range(20):\n",
    "        # PASSING THE REQUIRED VARIABLE TO ONE STEP ENCODER LAYER\n",
    "        fc , dec_state_h ,dec_state_c, alphas = model.layers[1].layers[0](enc_output , current_vec ,input_state_h ,input_state_c)\n",
    "        #APPENDING THE ALPHA VALUES TO THE LIST \"alpha_values\"\n",
    "        alpha_values.append(alphas)\n",
    "         # UPDATING THE CURRENT VECTOR \n",
    "        current_vec = np.argmax(fc , axis = -1)\n",
    "         # UPDATING THE INPUT STATE\n",
    "        input_state_h = dec_state_h\n",
    "        input_state_c = dec_state_c\n",
    "        # GETTING THE ACTUAL WORDS FRO THE TOKENIZED INDEXES\n",
    "        pred.append(tk_out.index_word[current_vec[0][0]])\n",
    "        # IF THE WORD \"<end>\" COMES THE LOOP WILL BREAK\n",
    "        if tk_out.index_word[current_vec[0][0]]==\"<end>\":\n",
    "              break\n",
    "    # JOINING THE PREDICTED WORDS\n",
    "    pred_sent = \" \".join(pred)\n",
    "    # CONCATINATING ALL THE ALPHA VALUES\n",
    "    alpha_values = tf.squeeze(tf.concat(alpha_values,axis=-1),axis=0)\n",
    "    # RETURNING THE PREDICTED SENTENCE AND ALPHA VALUES\n",
    "    return  pred_sent , alpha_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b19c8d5b-2cc3-413a-aa46-68c5db1ce705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score as bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26953b01-d04e-4dee-9d03-afc091ccd837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [07:16,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BELU =  0.4035954595719226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BLEU = []\n",
    "index = []\n",
    "np.random.seed(1)\n",
    "test_data = df_val.loc[np.random.choice(df_val.index,size = 2000,replace=False)]\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        pred = predict(str(i.enc_input),model)[0].split()\n",
    "        act = [str(i.dec_output).split()]\n",
    "        b = bleu.sentence_bleu(act,pred)\n",
    "        BLEU.append(b)\n",
    "    except:\n",
    "        index.append(ind)\n",
    "        continue\n",
    "print(\"BELU = \", np.mean(BLEU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d17bad6f-cfb2-4192-829c-11a39ebeb10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  he went to nagano in school trip and ski there\n",
      "PREDICTED SENTENCE ===>  he went to nagano in school trip and the ski festival <end>\n",
      "ACTUAL SENTENCE ===>  he went to nagano once on a school trip and skied there <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[19])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[19],model)[0])\n",
    "print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adab3b53-0da9-4fa9-83da-3e5956229331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  yesterday i checked how to set the environment up for developing iphone applications using other is account\n",
      "PREDICTED SENTENCE ===>  yesterday i checked how to set the environment up for developing the iphone applications using other is account <end>\n",
      "ACTUAL SENTENCE ===>  yesterday i checked how to set the environment up for developing iphone applications using another person is account <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[50])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[50],model)[0])\n",
    "print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b212a30-b84b-4f58-85bb-568c1dbe2b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 340 ms, sys: 0 ns, total: 340 ms\n",
      "Wall time: 336 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yesterday i checked how to set the environment up for developing the iphone applications using other is account <end>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predict(df_test.enc_input.values[50],model)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0b2466c-8376-436b-ac05-c27685aeee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(input,model,k):\n",
    "    seq = tk_inp.texts_to_sequences([input])\n",
    "    seq = pad_sequences(seq,maxlen = 35,padding=\"post\")\n",
    "\n",
    "    state = model.layers[0].initialize(1)\n",
    "    # GETTING THE ENCODED OUTPUT\n",
    "    enc_output,enc_state_h,enc_state_c = model.layers[0](seq,state)\n",
    "    \n",
    "\n",
    "    input_state_h = enc_state_h\n",
    "    input_state_c = enc_state_c \n",
    "    k_beams = [[tf.ones((1,1),dtype=tf.int32),0.0]]\n",
    "    for i in range(35):\n",
    "        candidates = []\n",
    "        for sent_pred , prob in k_beams :\n",
    "            if tk_out.word_index[\"<end>\"] in sent_pred.numpy() :\n",
    "                candidates += [[sent_pred , prob]]\n",
    "            else:\n",
    "               \n",
    "                dec_input = model.layers[1].layers[0].layers[0](sent_pred)\n",
    "                dec_output , dec_state_h , dec_state_c   =  model.layers[1].layers[0].layers[2](dec_input ,  initial_state =  [input_state_h , input_state_c])\n",
    "\n",
    "                context_vec , alphas =  model.layers[1].layers[0].layers[1](enc_output,dec_state_h)\n",
    "\n",
    "                # CONCATINATING THE CONTEXT VECTOR(BY EXPANDING DIMENSION) AND ENBEDDED VECTOR\n",
    "                dense_input =  tf.concat([tf.expand_dims(context_vec,1),tf.expand_dims(dec_state_h,1)],axis=-1)\n",
    "                \n",
    "                # PASSING THE DECODER OUTPUT THROUGH DENSE LAYER WITH UNITS EQUAL TO VOCAB SIZE\n",
    "                dense = model.layers[1].layers[0].layers[3](dense_input)\n",
    "\n",
    "                pred = tf.argsort(dense, direction= 'DESCENDING')[:,:,:k]\n",
    "                for w in range(k):\n",
    "                  candidates += [[tf.concat((sent_pred, pred[:,:,w]) , axis=-1) , (prob + tf.math.log(dense[:,:,pred[:,:,w][0][0]])[0][0])]  ]\n",
    "        k_beams = sorted(candidates,key=lambda tup:tup[1],reverse=True)[:k]\n",
    "\n",
    "    all_sent = []\n",
    "    for i,score in k_beams:\n",
    "        sent = \"\"\n",
    "        for j in range(1,35):\n",
    "            sent +=  tk_out.index_word[i.numpy()[:,j][0]] +  \" \" \n",
    "            if tk_out.index_word[i.numpy()[:,j][0]] ==\"<end>\":\n",
    "                break\n",
    "        all_sent.append((sent.strip(),score.numpy()))\n",
    "    return all_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cecb99d-4c3d-4654-98a7-375568a87d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [36:44,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BELU Score =  0.442634669702357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION BELU SCORE\n",
    "BLEU_beam = []\n",
    "index = []\n",
    "np.random.seed(1)\n",
    "test_data = df_val.loc[np.random.choice(df_val.index,size = 2000,replace=False)]\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        pred = beam_search(str(i.enc_input),model,3)[0][0].split()\n",
    "        act = [str(i.dec_output).split()]\n",
    "        b =bleu.sentence_bleu(act,pred)\n",
    "        BLEU_beam.append(b)\n",
    "    except:\n",
    "        index.append(ind)\n",
    "        continue\n",
    "\n",
    "print(\"BELU Score = \",np.mean(BLEU_beam)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e819843-590b-49b8-bd21-7811934b4a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  he went to nagano in school trip and ski there\n",
      "==================================================\n",
      "ACTUAL OUTPUT ===>  he went to nagano once on a school trip and skied there <end>\n",
      "==================================================\n",
      "BEAM SEARCH OUTPUT ,  SCORE\n",
      "('he went to nagano in the school trip and skiing there <end>', -4.4937983)\n",
      "('he went to nagano in school trip and skiing there <end>', -4.640549)\n",
      "('he went to nagano in the school trip and skiing ski there <end>', -7.8885984)\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[19])\n",
    "print(\"=\"*50)\n",
    "print(\"ACTUAL OUTPUT ===> \",df_test.dec_output.values[19])\n",
    "print(\"=\"*50)\n",
    "print(\"BEAM SEARCH OUTPUT ,  SCORE\")\n",
    "bm = (beam_search(df_test.enc_input.values[19],model,3))\n",
    "for i in bm:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40f7623c-e7fd-4d7b-92f5-08f033cf21ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  yesterday i checked how to set the environment up for developing iphone applications using other is account\n",
      "==================================================\n",
      "ACTUAL OUTPUT ===>  yesterday i checked how to set the environment up for developing iphone applications using another person is account <end>\n",
      "==================================================\n",
      "BEAM SEARCH OUTPUT ,  SCORE\n",
      "('yesterday i checked how to set the environment up for developing the iphone applications using other <end>', -4.739544)\n",
      "('yesterday i checked how to set the environment up for developing the iphone applications using other is account <end>', -5.0009623)\n",
      "('yesterday i checked how to set the environment up for developing the iphone applications using other thanksgiving <end>', -5.190677)\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[50])\n",
    "print(\"=\"*50)\n",
    "print(\"ACTUAL OUTPUT ===> \",df_test.dec_output.values[50])\n",
    "print(\"=\"*50)\n",
    "print(\"BEAM SEARCH OUTPUT ,  SCORE\")\n",
    "bm = (beam_search(df_test.enc_input.values[50],model,3))\n",
    "for i in bm:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50766232-d01d-4c1c-995a-e73c0f96b112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344a35e-db59-4400-a7a2-ae5fa6be903c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
