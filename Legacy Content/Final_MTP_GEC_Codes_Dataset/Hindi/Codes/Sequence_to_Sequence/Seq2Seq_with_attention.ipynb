{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a863645-a51e-42dd-8af4-c13efbce8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING THE REQUIRED LIBRARIES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm \n",
    "import tensorflow as tf\n",
    "from  tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from  sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba487c1-d2c1-4c68-9292-0f97368b2acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, \"0\" to  \"7\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3,4,6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e55603-0c73-4c5c-9d10-e38597a29ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f50a776-e696-4585-893e-823144a54064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "      <th>dec_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>परन्तु वे दोनों उन बातों को ज़्यादा समय तक अप ...</td>\n",
       "      <td>परन्तु वे दोनों उन बातों को ज़्यादा समय तक अपन...</td>\n",
       "      <td>परन्तु वे दोनों उन बातों को ज़्यादा समय तक अपन...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>देश में हिन्दी को विस्थापित कर का षड़यंत्र चल ...</td>\n",
       "      <td>देश में हिन्दी को विस्थापित करने का षड़यंत्र च...</td>\n",
       "      <td>देश में हिन्दी को विस्थापित करने का षड़यंत्र च...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>तीन साल पहले कातिलाना हमले के प्रकरण में एफआर ...</td>\n",
       "      <td>तीन साल पहले कातिलाना हमले के प्रकरण में एफआर ...</td>\n",
       "      <td>तीन साल पहले कातिलाना हमले के प्रकरण में एफआर ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम स...</td>\n",
       "      <td>रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम स...</td>\n",
       "      <td>रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम स...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>तब तक के लिए हमें विराम ले की अनुमति दीजिए ।</td>\n",
       "      <td>तब तक के लिए हमें विराम लेने की अनुमति दीजिए ।</td>\n",
       "      <td>तब तक के लिए हमें विराम लेने की अनुमति दीजिए ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139995</th>\n",
       "      <td>लेकिन फिर भी फिल्म को लोगों जित निगेटिव कमेंट्...</td>\n",
       "      <td>लेकिन फिर भी फिल्म को लोगों ने जितने निगेटिव क...</td>\n",
       "      <td>लेकिन फिर भी फिल्म को लोगों ने जितने निगेटिव क...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139996</th>\n",
       "      <td>बेटी से छेड़छाड़ के साथ वह कई बार दुराचार की क...</td>\n",
       "      <td>बेटी से छेड़छाड़ के साथ उसने कई बार दुराचार की...</td>\n",
       "      <td>बेटी से छेड़छाड़ के साथ उसने कई बार दुराचार की...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139997</th>\n",
       "      <td>आज यह कलाकार बालीवुड की चकाचौंध से दूर अपने गा...</td>\n",
       "      <td>आज यह कलाकार बालीवुड की चकाचौंध से दूर अपने गा...</td>\n",
       "      <td>आज यह कलाकार बालीवुड की चकाचौंध से दूर अपने गा...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139998</th>\n",
       "      <td>यदि ये इस वर्ष अपनी प्रतियोगी परीक्षाओं में सफ...</td>\n",
       "      <td>यदि ये इस वर्ष अपनी प्रतियोगी परीक्षाओं में सफ...</td>\n",
       "      <td>यदि ये इस वर्ष अपनी प्रतियोगी परीक्षाओं में सफ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139999</th>\n",
       "      <td>होश हैं तो जोश हैं</td>\n",
       "      <td>होश है तो जोश है</td>\n",
       "      <td>होश है तो जोश है</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                enc_input  \\\n",
       "0       परन्तु वे दोनों उन बातों को ज़्यादा समय तक अप ...   \n",
       "1       देश में हिन्दी को विस्थापित कर का षड़यंत्र चल ...   \n",
       "2       तीन साल पहले कातिलाना हमले के प्रकरण में एफआर ...   \n",
       "3       रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम स...   \n",
       "4            तब तक के लिए हमें विराम ले की अनुमति दीजिए ।   \n",
       "...                                                   ...   \n",
       "139995  लेकिन फिर भी फिल्म को लोगों जित निगेटिव कमेंट्...   \n",
       "139996  बेटी से छेड़छाड़ के साथ वह कई बार दुराचार की क...   \n",
       "139997  आज यह कलाकार बालीवुड की चकाचौंध से दूर अपने गा...   \n",
       "139998  यदि ये इस वर्ष अपनी प्रतियोगी परीक्षाओं में सफ...   \n",
       "139999                                 होश हैं तो जोश हैं   \n",
       "\n",
       "                                                dec_input  \\\n",
       "0       परन्तु वे दोनों उन बातों को ज़्यादा समय तक अपन...   \n",
       "1       देश में हिन्दी को विस्थापित करने का षड़यंत्र च...   \n",
       "2       तीन साल पहले कातिलाना हमले के प्रकरण में एफआर ...   \n",
       "3       रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम स...   \n",
       "4         तब तक के लिए हमें विराम लेने की अनुमति दीजिए ।    \n",
       "...                                                   ...   \n",
       "139995  लेकिन फिर भी फिल्म को लोगों ने जितने निगेटिव क...   \n",
       "139996  बेटी से छेड़छाड़ के साथ उसने कई बार दुराचार की...   \n",
       "139997  आज यह कलाकार बालीवुड की चकाचौंध से दूर अपने गा...   \n",
       "139998  यदि ये इस वर्ष अपनी प्रतियोगी परीक्षाओं में सफ...   \n",
       "139999                                   होश है तो जोश है   \n",
       "\n",
       "                                               dec_output  \n",
       "0       परन्तु वे दोनों उन बातों को ज़्यादा समय तक अपन...  \n",
       "1       देश में हिन्दी को विस्थापित करने का षड़यंत्र च...  \n",
       "2       तीन साल पहले कातिलाना हमले के प्रकरण में एफआर ...  \n",
       "3       रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम स...  \n",
       "4         तब तक के लिए हमें विराम लेने की अनुमति दीजिए ।   \n",
       "...                                                   ...  \n",
       "139995  लेकिन फिर भी फिल्म को लोगों ने जितने निगेटिव क...  \n",
       "139996  बेटी से छेड़छाड़ के साथ उसने कई बार दुराचार की...  \n",
       "139997  आज यह कलाकार बालीवुड की चकाचौंध से दूर अपने गा...  \n",
       "139998  यदि ये इस वर्ष अपनी प्रतियोगी परीक्षाओं में सफ...  \n",
       "139999                                   होश है तो जोश है  \n",
       "\n",
       "[140000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Dataset\n",
    "## LOADING THE PROCESSED DATASET\n",
    "df= pd.read_csv(\"DATA/etoori_train.csv\")\n",
    "df.columns = [\"enc_input\",\"dec_input\"] \n",
    "df[\"dec_output\"] = df.dec_input\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5cc6ee3-3865-4358-a074-6143aaae3103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "      <th>dec_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>परन्तु वे दोनों उन बातों को ज़्यादा समय तक अप ...</td>\n",
       "      <td>&lt;start&gt; परन्तु वे दोनों उन बातों को ज़्यादा सम...</td>\n",
       "      <td>परन्तु वे दोनों उन बातों को ज़्यादा समय तक अपन...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>देश में हिन्दी को विस्थापित कर का षड़यंत्र चल ...</td>\n",
       "      <td>&lt;start&gt; देश में हिन्दी को विस्थापित करने का षड...</td>\n",
       "      <td>देश में हिन्दी को विस्थापित करने का षड़यंत्र च...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>तीन साल पहले कातिलाना हमले के प्रकरण में एफआर ...</td>\n",
       "      <td>&lt;start&gt; तीन साल पहले कातिलाना हमले के प्रकरण म...</td>\n",
       "      <td>तीन साल पहले कातिलाना हमले के प्रकरण में एफआर ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम स...</td>\n",
       "      <td>&lt;start&gt; रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवें...</td>\n",
       "      <td>रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम स...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>तब तक के लिए हमें विराम ले की अनुमति दीजिए ।</td>\n",
       "      <td>&lt;start&gt; तब तक के लिए हमें विराम लेने की अनुमति...</td>\n",
       "      <td>तब तक के लिए हमें विराम लेने की अनुमति दीजिए ।...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139995</th>\n",
       "      <td>लेकिन फिर भी फिल्म को लोगों जित निगेटिव कमेंट्...</td>\n",
       "      <td>&lt;start&gt; लेकिन फिर भी फिल्म को लोगों ने जितने न...</td>\n",
       "      <td>लेकिन फिर भी फिल्म को लोगों ने जितने निगेटिव क...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139996</th>\n",
       "      <td>बेटी से छेड़छाड़ के साथ वह कई बार दुराचार की क...</td>\n",
       "      <td>&lt;start&gt; बेटी से छेड़छाड़ के साथ उसने कई बार दु...</td>\n",
       "      <td>बेटी से छेड़छाड़ के साथ उसने कई बार दुराचार की...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139997</th>\n",
       "      <td>आज यह कलाकार बालीवुड की चकाचौंध से दूर अपने गा...</td>\n",
       "      <td>&lt;start&gt; आज यह कलाकार बालीवुड की चकाचौंध से दूर...</td>\n",
       "      <td>आज यह कलाकार बालीवुड की चकाचौंध से दूर अपने गा...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139998</th>\n",
       "      <td>यदि ये इस वर्ष अपनी प्रतियोगी परीक्षाओं में सफ...</td>\n",
       "      <td>&lt;start&gt; यदि ये इस वर्ष अपनी प्रतियोगी परीक्षाओ...</td>\n",
       "      <td>यदि ये इस वर्ष अपनी प्रतियोगी परीक्षाओं में सफ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139999</th>\n",
       "      <td>होश हैं तो जोश हैं</td>\n",
       "      <td>&lt;start&gt; होश है तो जोश है</td>\n",
       "      <td>होश है तो जोश है &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                enc_input  \\\n",
       "0       परन्तु वे दोनों उन बातों को ज़्यादा समय तक अप ...   \n",
       "1       देश में हिन्दी को विस्थापित कर का षड़यंत्र चल ...   \n",
       "2       तीन साल पहले कातिलाना हमले के प्रकरण में एफआर ...   \n",
       "3       रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम स...   \n",
       "4            तब तक के लिए हमें विराम ले की अनुमति दीजिए ।   \n",
       "...                                                   ...   \n",
       "139995  लेकिन फिर भी फिल्म को लोगों जित निगेटिव कमेंट्...   \n",
       "139996  बेटी से छेड़छाड़ के साथ वह कई बार दुराचार की क...   \n",
       "139997  आज यह कलाकार बालीवुड की चकाचौंध से दूर अपने गा...   \n",
       "139998  यदि ये इस वर्ष अपनी प्रतियोगी परीक्षाओं में सफ...   \n",
       "139999                                 होश हैं तो जोश हैं   \n",
       "\n",
       "                                                dec_input  \\\n",
       "0       <start> परन्तु वे दोनों उन बातों को ज़्यादा सम...   \n",
       "1       <start> देश में हिन्दी को विस्थापित करने का षड...   \n",
       "2       <start> तीन साल पहले कातिलाना हमले के प्रकरण म...   \n",
       "3       <start> रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवें...   \n",
       "4       <start> तब तक के लिए हमें विराम लेने की अनुमति...   \n",
       "...                                                   ...   \n",
       "139995  <start> लेकिन फिर भी फिल्म को लोगों ने जितने न...   \n",
       "139996  <start> बेटी से छेड़छाड़ के साथ उसने कई बार दु...   \n",
       "139997  <start> आज यह कलाकार बालीवुड की चकाचौंध से दूर...   \n",
       "139998  <start> यदि ये इस वर्ष अपनी प्रतियोगी परीक्षाओ...   \n",
       "139999                           <start> होश है तो जोश है   \n",
       "\n",
       "                                               dec_output  \n",
       "0       परन्तु वे दोनों उन बातों को ज़्यादा समय तक अपन...  \n",
       "1       देश में हिन्दी को विस्थापित करने का षड़यंत्र च...  \n",
       "2       तीन साल पहले कातिलाना हमले के प्रकरण में एफआर ...  \n",
       "3       रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम स...  \n",
       "4       तब तक के लिए हमें विराम लेने की अनुमति दीजिए ।...  \n",
       "...                                                   ...  \n",
       "139995  लेकिन फिर भी फिल्म को लोगों ने जितने निगेटिव क...  \n",
       "139996  बेटी से छेड़छाड़ के साथ उसने कई बार दुराचार की...  \n",
       "139997  आज यह कलाकार बालीवुड की चकाचौंध से दूर अपने गा...  \n",
       "139998  यदि ये इस वर्ष अपनी प्रतियोगी परीक्षाओं में सफ...  \n",
       "139999                             होश है तो जोश है <end>  \n",
       "\n",
       "[140000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## THE INPUTS TO THE DECODER REQUIRES SPECIAL TOKENS FOR THE START AND THE END SO WE ARE GOING TO USE \n",
    "## <start> AS BEGINING TOKEN\n",
    "## <end>  AS END TOKEN\n",
    "\n",
    "df[\"dec_input\"]= \"<start> \" + df[\"dec_input\"]\n",
    "df[\"dec_output\"] =  df[\"dec_output\"] + \" <end>\" \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dde05eb-8464-4e51-8a2f-85b8bd8ac98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Splitting And Sampling around 100k datapoints\n",
    "#THE TOTAL DATASET HAS 500K DATAPOINTS WHICH WILL TAKE MUCH HIGHER TRAINING TIME. THEREFORE I AM SAMPLING ONE-FIFTH OF THE TOTAL DATASET\n",
    "\n",
    "#df_sampled = pd.concat((df[df.enc_input].sample(frac= 0.2,random_state=1)))\n",
    "#df_sampled = df.sample(frac = 0.4)\n",
    "print(df.shape)\n",
    "#print(df_sampled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd12f8b-d947-431d-8f6f-f184df2879b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ONCE THE DATA IS SAMPLED WE ARE SPLITTIND THE DATA IN TO TRAIN AND TEST\n",
    "df_train ,df_val = train_test_split(df,test_size=0.2,random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f50002f5-a360-400b-82f5-f605eaa15556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "      <th>dec_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>अवध के माटी रतनों को ढूंढने वाली इस संस्था और ...</td>\n",
       "      <td>&lt;start&gt; अवध के माटी रतनों को ढूंढने वाली इस सं...</td>\n",
       "      <td>अवध के माटी रतनों को ढूंढने वाली इस संस्था और ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107460</th>\n",
       "      <td>यही तो कला के दिन है ।</td>\n",
       "      <td>&lt;start&gt; यही तो कला के दिन हैं ।</td>\n",
       "      <td>यही तो कला के दिन हैं ।  &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134546</th>\n",
       "      <td>इसीलिए तो जिधर सिर मुडाता हूँ ओले उसी तरफ पड़न...</td>\n",
       "      <td>&lt;start&gt; इसीलिए तो जिधर सिर मुडाता हूँ ओले उसी ...</td>\n",
       "      <td>इसीलिए तो जिधर सिर मुडाता हूँ ओले उसी तरफ पड़न...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120290</th>\n",
       "      <td>फोन पर बात बंद हुई तो सीआई रोते हुए बताया कि त...</td>\n",
       "      <td>&lt;start&gt; फोन पर बात बंद हुई तो सीआई ने रोते हुए...</td>\n",
       "      <td>फोन पर बात बंद हुई तो सीआई ने रोते हुए बताया क...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109794</th>\n",
       "      <td>परिजनों ने उसे थेने लाए ।</td>\n",
       "      <td>&lt;start&gt; परिजनों ने उसे थाने लाए ।</td>\n",
       "      <td>परिजनों ने उसे थाने लाए ।  &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11261</th>\n",
       "      <td>प्रायः झपट्टा मारती है ।</td>\n",
       "      <td>&lt;start&gt; प्रायः झपट्टा मारती हैं ।</td>\n",
       "      <td>प्रायः झपट्टा मारती हैं ।  &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104704</th>\n",
       "      <td>अप गांव को उन्नत कर का उनका एक दीर्घकालीन सपना...</td>\n",
       "      <td>&lt;start&gt; अपने गांव को उन्नत करने का उनका एक दीर...</td>\n",
       "      <td>अपने गांव को उन्नत करने का उनका एक दीर्घकालीन ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48056</th>\n",
       "      <td>लेकिन कई क्षेत्र ऐसे हैंं जहां इस लाइन के बिछा...</td>\n",
       "      <td>&lt;start&gt; लेकिन कई क्षेत्र ऐसे हैं जहां इस लाइन ...</td>\n",
       "      <td>लेकिन कई क्षेत्र ऐसे हैं जहां इस लाइन के बिछा ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77049</th>\n",
       "      <td>वास्तव में हज़रत अली अलैहिस्सलाम साहस और निर्भ...</td>\n",
       "      <td>&lt;start&gt; वास्तव में हज़रत अली अलैहिस्सलाम साहस ...</td>\n",
       "      <td>वास्तव में हज़रत अली अलैहिस्सलाम साहस और निर्भ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71530</th>\n",
       "      <td>सुप्रीम कोर्ट दो साल से ज्यादा की सजा पाए ताओं...</td>\n",
       "      <td>&lt;start&gt; सुप्रीम कोर्ट ने दो साल से ज्यादा की स...</td>\n",
       "      <td>सुप्रीम कोर्ट ने दो साल से ज्यादा की सजा पाए न...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                enc_input  \\\n",
       "1522    अवध के माटी रतनों को ढूंढने वाली इस संस्था और ...   \n",
       "107460                             यही तो कला के दिन है ।   \n",
       "134546  इसीलिए तो जिधर सिर मुडाता हूँ ओले उसी तरफ पड़न...   \n",
       "120290  फोन पर बात बंद हुई तो सीआई रोते हुए बताया कि त...   \n",
       "109794                          परिजनों ने उसे थेने लाए ।   \n",
       "...                                                   ...   \n",
       "11261                            प्रायः झपट्टा मारती है ।   \n",
       "104704  अप गांव को उन्नत कर का उनका एक दीर्घकालीन सपना...   \n",
       "48056   लेकिन कई क्षेत्र ऐसे हैंं जहां इस लाइन के बिछा...   \n",
       "77049   वास्तव में हज़रत अली अलैहिस्सलाम साहस और निर्भ...   \n",
       "71530   सुप्रीम कोर्ट दो साल से ज्यादा की सजा पाए ताओं...   \n",
       "\n",
       "                                                dec_input  \\\n",
       "1522    <start> अवध के माटी रतनों को ढूंढने वाली इस सं...   \n",
       "107460                   <start> यही तो कला के दिन हैं ।    \n",
       "134546  <start> इसीलिए तो जिधर सिर मुडाता हूँ ओले उसी ...   \n",
       "120290  <start> फोन पर बात बंद हुई तो सीआई ने रोते हुए...   \n",
       "109794                 <start> परिजनों ने उसे थाने लाए ।    \n",
       "...                                                   ...   \n",
       "11261                  <start> प्रायः झपट्टा मारती हैं ।    \n",
       "104704  <start> अपने गांव को उन्नत करने का उनका एक दीर...   \n",
       "48056   <start> लेकिन कई क्षेत्र ऐसे हैं जहां इस लाइन ...   \n",
       "77049   <start> वास्तव में हज़रत अली अलैहिस्सलाम साहस ...   \n",
       "71530   <start> सुप्रीम कोर्ट ने दो साल से ज्यादा की स...   \n",
       "\n",
       "                                               dec_output  \n",
       "1522    अवध के माटी रतनों को ढूंढने वाली इस संस्था और ...  \n",
       "107460                     यही तो कला के दिन हैं ।  <end>  \n",
       "134546  इसीलिए तो जिधर सिर मुडाता हूँ ओले उसी तरफ पड़न...  \n",
       "120290  फोन पर बात बंद हुई तो सीआई ने रोते हुए बताया क...  \n",
       "109794                   परिजनों ने उसे थाने लाए ।  <end>  \n",
       "...                                                   ...  \n",
       "11261                    प्रायः झपट्टा मारती हैं ।  <end>  \n",
       "104704  अपने गांव को उन्नत करने का उनका एक दीर्घकालीन ...  \n",
       "48056   लेकिन कई क्षेत्र ऐसे हैं जहां इस लाइन के बिछा ...  \n",
       "77049   वास्तव में हज़रत अली अलैहिस्सलाम साहस और निर्भ...  \n",
       "71530   सुप्रीम कोर्ट ने दो साल से ज्यादा की सजा पाए न...  \n",
       "\n",
       "[112000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## IN THE COLUMN WHICH HAS DECODER INPUTS ADDING \"<end>\" TOKEN TO BE LEARNED BY THE TOKENIZER\n",
    "df_train[\"dec_input\"].iloc[0]  = df_train.iloc[0][\"dec_input\"] + \" <end>\"\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afb03693-28e3-4bd3-923b-194ff39f1420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "      <th>dec_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90522</th>\n",
       "      <td>महुआ का तेल जलाने के काम में लाया जाता हैं ।</td>\n",
       "      <td>&lt;start&gt; महुआ का तेल जलाने के काम में लाया जाता...</td>\n",
       "      <td>महुआ का तेल जलाने के काम में लाया जाता है ।  &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133504</th>\n",
       "      <td>आजकल ट्रेन में एक से ज़्यादा टीटी भी होते हैंं...</td>\n",
       "      <td>&lt;start&gt; आजकल ट्रेन में एक से ज़्यादा टीटी भी ह...</td>\n",
       "      <td>आजकल ट्रेन में एक से ज़्यादा टीटी भी होते हैं ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>शाम के वक्त बारी बारी से घर के सभी सदस्य रश्मि...</td>\n",
       "      <td>&lt;start&gt; शाम के वक्त बारी बारी से घर के सभी सदस...</td>\n",
       "      <td>शाम के वक्त बारी बारी से घर के सभी सदस्य रश्मि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136466</th>\n",
       "      <td>उनमें शिवकुमार दीपक प्रमुख है ।</td>\n",
       "      <td>&lt;start&gt; उनमें शिवकुमार दीपक प्रमुख हैं ।</td>\n",
       "      <td>उनमें शिवकुमार दीपक प्रमुख हैं ।  &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>हमारे देश में संविधान द्वारा अब स्त्रियों को अ...</td>\n",
       "      <td>&lt;start&gt; हमारे देश में संविधान द्वारा अब स्त्रि...</td>\n",
       "      <td>हमारे देश में संविधान द्वारा अब स्त्रियों को अ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111735</th>\n",
       "      <td>सरकार किसी पुल या सड़क का लोकार्पण करती हैं तो...</td>\n",
       "      <td>&lt;start&gt; सरकार किसी पुल या सड़क का लोकार्पण करत...</td>\n",
       "      <td>सरकार किसी पुल या सड़क का लोकार्पण करती है तो ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19247</th>\n",
       "      <td>उन्होंने हमें सिखाया कि कैसे संघर्ष किया जाता ...</td>\n",
       "      <td>&lt;start&gt; उन्होंने हमें सिखाया कि कैसे संघर्ष कि...</td>\n",
       "      <td>उन्होंने हमें सिखाया कि कैसे संघर्ष किया जाता ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57036</th>\n",
       "      <td>लेकिन यह इतिहास का अन्त नहीं हैं ।</td>\n",
       "      <td>&lt;start&gt; लेकिन यह इतिहास का अन्त नहीं है ।</td>\n",
       "      <td>लेकिन यह इतिहास का अन्त नहीं है ।  &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73689</th>\n",
       "      <td>रतन टाटा मल्टीब्रांड रिटेल में एफडीआई का स्वाग...</td>\n",
       "      <td>&lt;start&gt; रतन टाटा ने मल्टीब्रांड रिटेल में एफडी...</td>\n",
       "      <td>रतन टाटा ने मल्टीब्रांड रिटेल में एफडीआई का स्...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134554</th>\n",
       "      <td>पाकिस्तान के शहर कराची में इससे पहले भी कई मंद...</td>\n",
       "      <td>&lt;start&gt; पाकिस्तान के शहर कराची में इससे पहले भ...</td>\n",
       "      <td>पाकिस्तान के शहर कराची में इससे पहले भी कई मंद...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                enc_input  \\\n",
       "90522        महुआ का तेल जलाने के काम में लाया जाता हैं ।   \n",
       "133504  आजकल ट्रेन में एक से ज़्यादा टीटी भी होते हैंं...   \n",
       "2373    शाम के वक्त बारी बारी से घर के सभी सदस्य रश्मि...   \n",
       "136466                    उनमें शिवकुमार दीपक प्रमुख है ।   \n",
       "5198    हमारे देश में संविधान द्वारा अब स्त्रियों को अ...   \n",
       "...                                                   ...   \n",
       "111735  सरकार किसी पुल या सड़क का लोकार्पण करती हैं तो...   \n",
       "19247   उन्होंने हमें सिखाया कि कैसे संघर्ष किया जाता ...   \n",
       "57036                  लेकिन यह इतिहास का अन्त नहीं हैं ।   \n",
       "73689   रतन टाटा मल्टीब्रांड रिटेल में एफडीआई का स्वाग...   \n",
       "134554  पाकिस्तान के शहर कराची में इससे पहले भी कई मंद...   \n",
       "\n",
       "                                                dec_input  \\\n",
       "90522   <start> महुआ का तेल जलाने के काम में लाया जाता...   \n",
       "133504  <start> आजकल ट्रेन में एक से ज़्यादा टीटी भी ह...   \n",
       "2373    <start> शाम के वक्त बारी बारी से घर के सभी सदस...   \n",
       "136466          <start> उनमें शिवकुमार दीपक प्रमुख हैं ।    \n",
       "5198    <start> हमारे देश में संविधान द्वारा अब स्त्रि...   \n",
       "...                                                   ...   \n",
       "111735  <start> सरकार किसी पुल या सड़क का लोकार्पण करत...   \n",
       "19247   <start> उन्होंने हमें सिखाया कि कैसे संघर्ष कि...   \n",
       "57036          <start> लेकिन यह इतिहास का अन्त नहीं है ।    \n",
       "73689   <start> रतन टाटा ने मल्टीब्रांड रिटेल में एफडी...   \n",
       "134554  <start> पाकिस्तान के शहर कराची में इससे पहले भ...   \n",
       "\n",
       "                                               dec_output  \n",
       "90522   महुआ का तेल जलाने के काम में लाया जाता है ।  <...  \n",
       "133504  आजकल ट्रेन में एक से ज़्यादा टीटी भी होते हैं ...  \n",
       "2373    शाम के वक्त बारी बारी से घर के सभी सदस्य रश्मि...  \n",
       "136466            उनमें शिवकुमार दीपक प्रमुख हैं ।  <end>  \n",
       "5198    हमारे देश में संविधान द्वारा अब स्त्रियों को अ...  \n",
       "...                                                   ...  \n",
       "111735  सरकार किसी पुल या सड़क का लोकार्पण करती है तो ...  \n",
       "19247   उन्होंने हमें सिखाया कि कैसे संघर्ष किया जाता ...  \n",
       "57036            लेकिन यह इतिहास का अन्त नहीं है ।  <end>  \n",
       "73689   रतन टाटा ने मल्टीब्रांड रिटेल में एफडीआई का स्...  \n",
       "134554  पाकिस्तान के शहर कराची में इससे पहले भी कई मंद...  \n",
       "\n",
       "[28000 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## VALIDATION DATA\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e439697b-9cea-4366-a3e1-1c2e0eda0cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "      <th>dec_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80240</th>\n",
       "      <td>हमें आने वाले हफ्तों में रोवियो मोबाइल द्वारा ...</td>\n",
       "      <td>&lt;start&gt; हमें आने वाले हफ्तों में रोवियो मोबाइल...</td>\n",
       "      <td>हमें आने वाले हफ्तों में रोवियो मोबाइल द्वारा ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32433</th>\n",
       "      <td>आज कमेन्ट मोदेरेशन सक्षम हैंं ।</td>\n",
       "      <td>&lt;start&gt; आज कमेन्ट मोदेरेशन सक्षम हैं ।</td>\n",
       "      <td>आज कमेन्ट मोदेरेशन सक्षम हैं ।  &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135530</th>\n",
       "      <td>राजस् थेन स्कूल टीचर्स यूनियन के अध्यक्ष आर पी...</td>\n",
       "      <td>&lt;start&gt; राजस् थान स्कूल टीचर्स यूनियन के अध्यक...</td>\n",
       "      <td>राजस् थान स्कूल टीचर्स यूनियन के अध्यक्ष आर पी...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3354</th>\n",
       "      <td>सेक्स का कारक ग्रह भी शुक्र ही हैं तथा शुक्र ए...</td>\n",
       "      <td>&lt;start&gt; सेक्स का कारक ग्रह भी शुक्र ही है तथा ...</td>\n",
       "      <td>सेक्स का कारक ग्रह भी शुक्र ही है तथा शुक्र एक...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125119</th>\n",
       "      <td>वह एक गिलास शर्बत</td>\n",
       "      <td>&lt;start&gt; उसने एक गिलास शर्बत</td>\n",
       "      <td>उसने एक गिलास शर्बत &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123821</th>\n",
       "      <td>बीकॉम ऑनर्स और इको ऑनर्स कोर्स स्टूडेंट की पसं...</td>\n",
       "      <td>&lt;start&gt; बीकॉम ऑनर्स और इको ऑनर्स कोर्स स्टूडें...</td>\n",
       "      <td>बीकॉम ऑनर्स और इको ऑनर्स कोर्स स्टूडेंट की पसं...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40525</th>\n",
       "      <td>मरने वालों में बस में सवार यात्री थी ।</td>\n",
       "      <td>&lt;start&gt; मरने वालों में बस में सवार यात्री थे ।</td>\n",
       "      <td>मरने वालों में बस में सवार यात्री थे ।  &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72286</th>\n",
       "      <td>यान्त्रिक रूप से क्षमा मांगना सम्पर्क बढ़ा का ...</td>\n",
       "      <td>&lt;start&gt; यान्त्रिक रूप से क्षमा मांगना सम्पर्क ...</td>\n",
       "      <td>यान्त्रिक रूप से क्षमा मांगना सम्पर्क बढ़ाने क...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90240</th>\n",
       "      <td>मैंनेने इस शहर में खुद का स् वागत बांहें खोलकर...</td>\n",
       "      <td>&lt;start&gt; मैंने इस शहर में खुद का स् वागत बांहें...</td>\n",
       "      <td>मैंने इस शहर में खुद का स् वागत बांहें खोलकर न...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47760</th>\n",
       "      <td>मैं माउथ फ्रेशनर खाया और उसके घर पहुँचा ।</td>\n",
       "      <td>&lt;start&gt; मैंने माउथ फ्रेशनर खाया और उसके घर पहु...</td>\n",
       "      <td>मैंने माउथ फ्रेशनर खाया और उसके घर पहुँचा ।  &lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                enc_input  \\\n",
       "80240   हमें आने वाले हफ्तों में रोवियो मोबाइल द्वारा ...   \n",
       "32433                     आज कमेन्ट मोदेरेशन सक्षम हैंं ।   \n",
       "135530  राजस् थेन स्कूल टीचर्स यूनियन के अध्यक्ष आर पी...   \n",
       "3354    सेक्स का कारक ग्रह भी शुक्र ही हैं तथा शुक्र ए...   \n",
       "125119                                  वह एक गिलास शर्बत   \n",
       "...                                                   ...   \n",
       "123821  बीकॉम ऑनर्स और इको ऑनर्स कोर्स स्टूडेंट की पसं...   \n",
       "40525              मरने वालों में बस में सवार यात्री थी ।   \n",
       "72286   यान्त्रिक रूप से क्षमा मांगना सम्पर्क बढ़ा का ...   \n",
       "90240   मैंनेने इस शहर में खुद का स् वागत बांहें खोलकर...   \n",
       "47760           मैं माउथ फ्रेशनर खाया और उसके घर पहुँचा ।   \n",
       "\n",
       "                                                dec_input  \\\n",
       "80240   <start> हमें आने वाले हफ्तों में रोवियो मोबाइल...   \n",
       "32433             <start> आज कमेन्ट मोदेरेशन सक्षम हैं ।    \n",
       "135530  <start> राजस् थान स्कूल टीचर्स यूनियन के अध्यक...   \n",
       "3354    <start> सेक्स का कारक ग्रह भी शुक्र ही है तथा ...   \n",
       "125119                        <start> उसने एक गिलास शर्बत   \n",
       "...                                                   ...   \n",
       "123821  <start> बीकॉम ऑनर्स और इको ऑनर्स कोर्स स्टूडें...   \n",
       "40525     <start> मरने वालों में बस में सवार यात्री थे ।    \n",
       "72286   <start> यान्त्रिक रूप से क्षमा मांगना सम्पर्क ...   \n",
       "90240   <start> मैंने इस शहर में खुद का स् वागत बांहें...   \n",
       "47760   <start> मैंने माउथ फ्रेशनर खाया और उसके घर पहु...   \n",
       "\n",
       "                                               dec_output  \n",
       "80240   हमें आने वाले हफ्तों में रोवियो मोबाइल द्वारा ...  \n",
       "32433               आज कमेन्ट मोदेरेशन सक्षम हैं ।  <end>  \n",
       "135530  राजस् थान स्कूल टीचर्स यूनियन के अध्यक्ष आर पी...  \n",
       "3354    सेक्स का कारक ग्रह भी शुक्र ही है तथा शुक्र एक...  \n",
       "125119                          उसने एक गिलास शर्बत <end>  \n",
       "...                                                   ...  \n",
       "123821  बीकॉम ऑनर्स और इको ऑनर्स कोर्स स्टूडेंट की पसं...  \n",
       "40525       मरने वालों में बस में सवार यात्री थे ।  <end>  \n",
       "72286   यान्त्रिक रूप से क्षमा मांगना सम्पर्क बढ़ाने क...  \n",
       "90240   मैंने इस शहर में खुद का स् वागत बांहें खोलकर न...  \n",
       "47760   मैंने माउथ फ्रेशनर खाया और उसके घर पहुँचा ।  <...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## HERE I AM SAMPLING 1000 POINTS FROM THE DATAFRAME AS TEST DATA WHICH ARE NOT PRESEENT IN THE TRAIN AND VALIDAION DATA\n",
    "np.random.seed(5) \n",
    "df_test = df.loc[np.random.choice(np.array([x for x in df.index.values]),1000,replace= False,)]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4ab7621-b51d-4212-8ea5-d1c4ce4e1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92cb1da3-c76f-4133-a17c-b2eb06928469",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TOKENIZER FOR ENCODER INPUT\n",
    "tk_inp = Tokenizer()\n",
    "tk_inp.fit_on_texts(df_train.enc_input.apply(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54f2d588-540c-49f0-b5c4-30f1e779cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER FOR DECODER INPUT\n",
    "tk_out = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n' )\n",
    "tk_out.fit_on_texts(df_train.dec_input.apply(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "945e76da-1639-49bc-9d36-82b8790ae542",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS CLASS CONVERTS TEXT DATA TO INTEGER SEQUENCES AND RETURNS THE PADDED SEQUENCES\n",
    "class Dataset :\n",
    "    def __init__(self, data , tk_inp ,tk_out, max_len):\n",
    "        ## SETTING THE REQUIRED ATTRIBUTES\n",
    "        self.encoder_inp = data[\"enc_input\"].apply(str).values\n",
    "        self.decoder_inp = data[\"dec_input\"].apply(str).values\n",
    "        self.decoder_out = data[\"dec_output\"].apply(str).values\n",
    "        self.tk_inp = tk_inp\n",
    "        self.tk_out = tk_out\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self,i):\n",
    "        # INPUT SEQUENCES\n",
    "        self.encoder_seq = self.tk_inp.texts_to_sequences([self.encoder_inp[i]])\n",
    "        # DECODER INPUT SEQUENCES \n",
    "        self.decoder_inp_seq = self.tk_out.texts_to_sequences([self.decoder_inp[i]])\n",
    "        # DECODER INPUT SEQUENCES\n",
    "        self.decoder_out_seq = self.tk_out.texts_to_sequences([self.decoder_out[i]])\n",
    "        \n",
    "        # PADDING THE ENCODER INPUT SEQUENCES\n",
    "        self.encoder_seq = pad_sequences(self.encoder_seq, padding=\"post\",maxlen = self.max_len)\n",
    "        # PADDING THE DECODER INPUT SEQUENCES\n",
    "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, padding=\"post\",maxlen = self.max_len)\n",
    "        # PADDING DECODER OUTPUT SEQUENCES\n",
    "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq ,padding=\"post\", maxlen = self.max_len)\n",
    "        return self.encoder_seq ,  self.decoder_inp_seq,  self.decoder_out_seq\n",
    "    \n",
    "    def __len__(self):\n",
    "        # RETURN THE LEN OF INPUT ENDODER\n",
    "        return len(self.encoder_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fbe536d-9ccb-43ab-958c-5fc95fda60ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS CLASS CONVERTES THE DATASET INTO THE REQUIRED BATCH SIZE\n",
    "\n",
    "class Dataloader(tf.keras.utils.Sequence):\n",
    "    def __init__(self,batch_size,dataset):\n",
    "        # INTIALIZING THE REQUIRED VARIABLES \n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.totl_points = self.dataset.encoder_inp.shape[0]\n",
    "        \n",
    "    def __getitem__(self,i):\n",
    "        # STATING THE START AND STOP VATIABLE CONTAINGING INDEX VALUES FOR EACH BATCH\n",
    "        start = i * self.batch_size\n",
    "        stop = (i+1)*self.batch_size\n",
    "        \n",
    "        # PLACEHOLDERS FOR BATCHED DATA\n",
    "        batch_enc =[]\n",
    "        batch_dec_input = []\n",
    "        batch_dec_out =[]\n",
    "\n",
    "        for j in range(start,stop): \n",
    "            \n",
    "            a,b,c = self.dataset[j] \n",
    "            batch_enc.append(a[0]) \n",
    "            batch_dec_input.append(b[0])\n",
    "            batch_dec_out.append(c[0]) \n",
    "        \n",
    "        # Conveting list to array   \n",
    "        batch_enc = (np.array(batch_enc)) \n",
    "        batch_dec_input = np.array(batch_dec_input)\n",
    "        batch_dec_out = np.array(batch_dec_out)\n",
    "        \n",
    "        return [batch_enc , batch_dec_input],batch_dec_out\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Returning the number of batches\n",
    "        return int(self.totl_points/self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3de6bf4b-b677-48fe-9d97-77ec683bd5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMING OBJECTS OF DATASET AND DATALOADER FOR TRAIN DATASET\n",
    "train_dataset = Dataset(df_train,tk_inp,tk_out,51)\n",
    "train_dataloader = Dataloader( batch_size = 32, dataset=train_dataset)\n",
    "\n",
    "# FORMING OBJECTS OF DATASET AND DATALOADER FOR VALIDATION DATASET\n",
    "val_dataset = Dataset(df_val , tk_inp,tk_out,51)\n",
    "val_dataloader = Dataloader(batch_size=32 , dataset=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86fab78b-5478-4d5d-9c03-7bd754099b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfb6a217-cf1c-4535-9d11-67c85e3010ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINING THE ENCODER LAYER AS A FUNCTION\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self, vocab_size,emb_dims, enc_units, input_length,batch_size):\n",
    "        super().__init__()\n",
    "        # INITIALIZING THE REQUIRED VARIABLES\n",
    "        self.batch_size=batch_size # BATHCH SIZE\n",
    "        self.enc_units = enc_units # ENCODER UNITS\n",
    "\n",
    "        # EMBEDDING LAYER\n",
    "        self.embedding= layers.Embedding(vocab_size ,emb_dims) \n",
    "        # LSTM LAYER WITH RETURN SEQ AND RETURN STATES\n",
    "        self.lstm = layers.LSTM(self.enc_units,return_state= True,return_sequences =  True) \n",
    "    def call(self, enc_input , states):\n",
    "      \n",
    "        # FORMING THE EMBEDDED VECTOR \n",
    "        emb = self.embedding(enc_input)\n",
    "        # PASSING THE EMBEDDED VECTIO THROUGH LSTM LAYERS \n",
    "        enc_output,state_h,state_c = self.lstm(emb,initial_state=states)\n",
    "        #RETURNING THE OUTPUT OF LSTM LAYER\n",
    "        return enc_output,state_h,state_c \n",
    "    def initialize(self,batch_size):\n",
    "\n",
    "        return tf.zeros(shape=(batch_size,self.enc_units)),tf.zeros(shape=(batch_size,self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "404a18ed-0bd8-465d-82d8-ceb74d7019f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS ATTNETION LAYER FOR DOT MODEL\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,units):\n",
    "        super().__init__()\n",
    "        # INITIALIZING THE DENSE LAYER W1\n",
    "        self.W1 = layers.Dense(units)\n",
    "        # INITIALIZING THE DENSE LAYER W2\n",
    "        self.W2 = layers.Dense(units)\n",
    "        # INITIALIZING THE DENSE LAYER V\n",
    "        self.v = layers.Dense(1)\n",
    "        \n",
    "    def call(self,enc_output,dec_state):\n",
    "        # EXPANDING THE DIMENSION OF DECODER STATE  EG. FROM (16,32) TO (16,32,1)\n",
    "        dec_state =  tf.expand_dims(dec_state,axis=1)\n",
    "        \n",
    "        # FINDING THE SCORE FOR CONCAT MODEL\n",
    "        score = self.v(tf.nn.tanh(\n",
    "            self.W1(dec_state)+ self.W2(enc_output)\n",
    "        ))\n",
    "        # APPLYING SOFTMAX TO THE AXIS 1\n",
    "        # OUPUT SHAPE = (16,13,1)\n",
    "        att_weights = tf.nn.softmax(score,axis=1)\n",
    "        \n",
    "        # CALCULATING THE CONTEXT VECTOR BY FIRST ELEMENTWISE MULTIPLICATION AND THEN ADDING THE AXIS 1\n",
    "        # (16,13,1)*(16,13,32)=(16,13,32)\n",
    "        context_vec  = att_weights* enc_output\n",
    "        \n",
    "        # (16,13,32) SUM AND REDUCE THE DIMENSION AT AXIS 1 => (16,32)\n",
    "        context_vec = tf.reduce_sum(context_vec,axis=1)\n",
    "        \n",
    "        # RETURNING THE CONTEXT VECTOR AND ATTENTION WEIGHTS\n",
    "        return context_vec,att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89bb7392-f7b7-452f-b9b7-f446100739e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Onestepdecoder(tf.keras.Model):\n",
    "    '''THIS MODEL OUTPUTS THE RESULT OF DECODER FOR ONE TIME SETP GIVEN THE INPUT FOR PRECIOVE TIME STEP'''\n",
    "    \n",
    "    def __init__(self, vocab_size,emb_dims, dec_units, input_len,att_units,batch_size):\n",
    "        super().__init__()\n",
    "        # INTITALIZING THE REQUIRED VARIABLES\n",
    "        # EMBEDDING LAYERS\n",
    "        self.emb = layers.Embedding(vocab_size,emb_dims,input_length= input_len)\n",
    "        # ATTENTION LAYER\n",
    "        self.att = Attention(att_units)\n",
    "        # LSTM LAYER\n",
    "        self.lstm = layers.LSTM(dec_units,return_sequences=True,return_state=True)\n",
    "        # DENSE LAYER\n",
    "        self.dense = layers.Dense(vocab_size,activation=\"softmax\")\n",
    "\n",
    "    def call(self, encoder_output , input , state_h,state_c):\n",
    "        # FORMING THE EMBEDDED VECTOR FOR THE WORD\n",
    "        # (32,1)=>(32,1,12)\n",
    "        emb = self.emb(input)\n",
    "\n",
    "        dec_output,dec_state_h,dec_state_c = self.lstm(emb, initial_state = [state_h,state_c] )\n",
    "\n",
    "        # GETTING THE CONTEXT VECTOR AND ATTENTION WEIGHTS BASED ON THE ENCODER OUTPUT AND  DECODER STATE_H\n",
    "        context_vec,alphas = self.att(encoder_output,dec_state_h)\n",
    "        \n",
    "        # CONCATINATING THE CONTEXT VECTOR(BY EXPANDING DIMENSION) AND ENBEDDED VECTOR\n",
    "        dense_input =  tf.concat([tf.expand_dims(context_vec,1),dec_output],axis=-1)\n",
    "        \n",
    "        # PASSING THE DECODER OUTPUT THROUGH DENSE LAYER WITH UNITS EQUAL TO VOCAB SIZE\n",
    "        fc = self.dense(dense_input)\n",
    "        \n",
    "        # RETURNING THE OUTPUT\n",
    "        return fc , dec_state_h , dec_state_c , alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41baa755-eeaf-47d4-9c29-438fad38ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    '''THIS MODEL PERFORMS THE WHOLE DECODER OPERATION FOR THE COMPLETE SENTENCE'''\n",
    "    def __init__(self, vocab_size,emb_dims, dec_units, input_len,att_units,batch_size):\n",
    "        super().__init__()\n",
    "        # INITIALIZING THE VARIABLES\n",
    "        # LENGTH OF INPUT SENTENCE\n",
    "        self.input_len = input_len\n",
    "        # ONE STEP DECODER\n",
    "        self.onestepdecoder = Onestepdecoder(vocab_size,emb_dims, dec_units, input_len,att_units,batch_size)\n",
    "\n",
    "    def call(self,dec_input,enc_output,state_h,state_c):\n",
    "        # THIS VATIABLE STORES THE VALUE OF STATE_H FOR THE PREVIOUS STATE\n",
    "        current_state_h = state_h \n",
    "        current_state_c = state_c\n",
    "        # THIS STORES THE DECODER OUTPUT FOR EACH TIME STEP\n",
    "        pred = []\n",
    "        # THIS STORED THE ALPHA VALUES\n",
    "        alpha_values = []\n",
    "        # FOR EACH WORD IN THE INPUT SENTENCE\n",
    "        for i in range(self.input_len):\n",
    "            \n",
    "            # CURRENT WORD TO INPUT TO ONE STEP DECODER\n",
    "            current_vec = dec_input[:,i]\n",
    "\n",
    "            # EXPANDING THE DIMENSION FOR THE WORD\n",
    "            current_vec = tf.expand_dims(current_vec,axis=-1)\n",
    "\n",
    "            # PERFORMING THE ONE STEP DECODER OPERATION \n",
    "            dec_output,dec_state_h,dec_state_c,alphas = self.onestepdecoder(enc_output ,current_vec,current_state_h,current_state_c)\n",
    "\n",
    "            #UPDATING THE CURRENT STATE_H\n",
    "            current_state_h = dec_state_h\n",
    "            current_state_c = dec_state_c\n",
    "\n",
    "            #APPENDING THE DECODER OUTPUT TO \"pred\" LIST\n",
    "            pred.append(dec_output)\n",
    "\n",
    "            # APPENDING THE ALPHA VALUES\n",
    "            alpha_values.append(alphas)\n",
    "            \n",
    "        # CONCATINATING ALL THE VALUES IN THE LIST\n",
    "        output = tf.concat(pred,axis=1)\n",
    "        # CONCATINATING ALL THE ALPHA VALUES IN THE LIST\n",
    "        alpha_values = tf.concat(alpha_values,axis = -1)\n",
    "        # RETURNING THE OUTPUT\n",
    "        return output , alpha_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "502a1cbe-d9bb-43ee-a757-9a5ff8178b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "    '''THIS MODEL COMBINES ALL THE LAYERS AND FORM IN ENCODER DECODER MODEL WITH ATTENTION MECHANISM'''\n",
    "    def __init__(self,enc_vocab_size,enc_emb_dim,enc_units,enc_input_length,\n",
    "             dec_vocab_size,dec_emb_dim,dec_units,dec_input_length ,att_units, batch_size):\n",
    "        # INITAILIZING ALL VARIABLES\n",
    "        super().__init__()\n",
    "        # BATCH SIZE\n",
    "        self.batch_size = batch_size\n",
    "        # INITIALIZING ENCODER LAYER\n",
    "        self.encoder = Encoder(enc_vocab_size, enc_emb_dim,enc_units, enc_input_length,batch_size)\n",
    "        # INITALIZING DECODER LAYER\n",
    "        self.decoder = Decoder(dec_vocab_size ,dec_emb_dim,dec_units,dec_input_length  ,att_units, batch_size)\n",
    "\n",
    "    def call(self,data):\n",
    "        # THE INPUT OF DATALOADER IS IN A LIST FORM FOR EACH BATCH IT GIVER TWO INPUTS\n",
    "        # INPUT1 IS FOR ENCODER\n",
    "        # INPUT2 IS FOR DECODER\n",
    "        inp1 , inp2 = data\n",
    "        # PASSING THE INPUT1 TO ENCODER LAYER\n",
    "        enc_output, enc_state_h, enc_state_c = self.encoder(inp1,self.encoder.initialize(self.batch_size))\n",
    "        # PASSING INPUT2 TO THE DECODER LAYER\n",
    "        dec_output , alphas = self.decoder(inp2 , enc_output,enc_state_h,enc_state_c)\n",
    "        # THE OUTPUT OF MODEL IS ONLY DECODER OUTPUT THE ALPHA VALUES ARE IGNORED HERE\n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f9a45ac-3e2b-4822-9d2f-e676e79027d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITAILZING THE MODEL\n",
    "model = encoder_decoder(enc_vocab_size=len(tk_inp.word_index)+1,\n",
    "                         enc_emb_dim = 300,\n",
    "                         enc_units=256,enc_input_length=51,\n",
    "                         dec_vocab_size =len(tk_out.word_index)+1,\n",
    "                         dec_emb_dim =300,\n",
    "                         dec_units=256,\n",
    "                         dec_input_length = 51,\n",
    "                         att_units=256,\n",
    "                         batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b82db-e576-40d6-8af0-6e2b05b9ba46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eeb09fe-82de-4a92-907f-3cee408fb00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback =[ tf.keras.callbacks.ModelCheckpoint( \"models/attention_concat_best.h5\",save_best_only=True,mode=\"min\" ,save_weights_only=True),\n",
    "           tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,verbose=1,min_delta=0.0001),\n",
    "            tf.keras.callbacks.TensorBoard(\"models/attention_concat_logs_save\",histogram_freq=1)\n",
    "]\n",
    "\n",
    "train_steps = train_dataloader.__len__()\n",
    "val_steps  = val_dataloader.__len__()\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f639839-6e88-4255-9762-6df79a637fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 00:16:12.004043: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "3500/3500 [==============================] - 1504s 405ms/step - loss: 1.4748 - accuracy: 0.8135 - val_loss: 0.4440 - val_accuracy: 0.9466\n",
      "Epoch 2/60\n",
      "3500/3500 [==============================] - 1400s 400ms/step - loss: 0.3130 - accuracy: 0.9626 - val_loss: 0.1835 - val_accuracy: 0.9762\n",
      "Epoch 3/60\n",
      "3500/3500 [==============================] - 1400s 400ms/step - loss: 0.1297 - accuracy: 0.9816 - val_loss: 0.1181 - val_accuracy: 0.9839\n",
      "Epoch 4/60\n",
      "3500/3500 [==============================] - 1399s 400ms/step - loss: 0.0569 - accuracy: 0.9910 - val_loss: 0.0961 - val_accuracy: 0.9862\n",
      "Epoch 5/60\n",
      "3500/3500 [==============================] - 1399s 400ms/step - loss: 0.0300 - accuracy: 0.9954 - val_loss: 0.0876 - val_accuracy: 0.9872\n",
      "Epoch 6/60\n",
      "3500/3500 [==============================] - 1399s 400ms/step - loss: 0.0215 - accuracy: 0.9968 - val_loss: 0.0815 - val_accuracy: 0.9879\n",
      "Epoch 7/60\n",
      "3500/3500 [==============================] - 1400s 400ms/step - loss: 0.0173 - accuracy: 0.9977 - val_loss: 0.0756 - val_accuracy: 0.9887\n",
      "Epoch 8/60\n",
      "3500/3500 [==============================] - 1399s 400ms/step - loss: 0.0146 - accuracy: 0.9982 - val_loss: 0.0732 - val_accuracy: 0.9890\n",
      "Epoch 9/60\n",
      "3500/3500 [==============================] - 1398s 400ms/step - loss: 0.0130 - accuracy: 0.9985 - val_loss: 0.0707 - val_accuracy: 0.9893\n",
      "Epoch 10/60\n",
      "3500/3500 [==============================] - 1397s 399ms/step - loss: 0.0122 - accuracy: 0.9988 - val_loss: 0.0696 - val_accuracy: 0.9895\n",
      "Epoch 11/60\n",
      "3500/3500 [==============================] - 1397s 399ms/step - loss: 0.0116 - accuracy: 0.9989 - val_loss: 0.0687 - val_accuracy: 0.9897\n",
      "Epoch 12/60\n",
      "3500/3500 [==============================] - 1396s 399ms/step - loss: 0.0111 - accuracy: 0.9991 - val_loss: 0.0698 - val_accuracy: 0.9895\n",
      "Epoch 13/60\n",
      "3500/3500 [==============================] - 1397s 399ms/step - loss: 0.0107 - accuracy: 0.9991 - val_loss: 0.0680 - val_accuracy: 0.9900\n",
      "Epoch 14/60\n",
      "3500/3500 [==============================] - 1397s 399ms/step - loss: 0.0104 - accuracy: 0.9992 - val_loss: 0.0675 - val_accuracy: 0.9901\n",
      "Epoch 15/60\n",
      "3500/3500 [==============================] - 1396s 399ms/step - loss: 0.0103 - accuracy: 0.9993 - val_loss: 0.0677 - val_accuracy: 0.9900\n",
      "Epoch 16/60\n",
      "3500/3500 [==============================] - 1397s 399ms/step - loss: 0.0101 - accuracy: 0.9993 - val_loss: 0.0668 - val_accuracy: 0.9903\n",
      "Epoch 17/60\n",
      "3500/3500 [==============================] - 1397s 399ms/step - loss: 0.0100 - accuracy: 0.9993 - val_loss: 0.0686 - val_accuracy: 0.9900\n",
      "Epoch 18/60\n",
      "3500/3500 [==============================] - 1397s 399ms/step - loss: 0.0099 - accuracy: 0.9994 - val_loss: 0.0690 - val_accuracy: 0.9900\n",
      "Epoch 19/60\n",
      "3500/3500 [==============================] - 1396s 399ms/step - loss: 0.0097 - accuracy: 0.9994 - val_loss: 0.0685 - val_accuracy: 0.9903\n",
      "Epoch 20/60\n",
      "3500/3500 [==============================] - 1396s 399ms/step - loss: 0.0097 - accuracy: 0.9994 - val_loss: 0.0686 - val_accuracy: 0.9902\n",
      "Epoch 21/60\n",
      "3500/3500 [==============================] - 1396s 399ms/step - loss: 0.0095 - accuracy: 0.9995 - val_loss: 0.0691 - val_accuracy: 0.9902\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f44f0f9f080>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataloader, steps_per_epoch=train_steps,epochs= 60,validation_data = val_dataloader,validation_steps =val_steps,callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "baca2ff7-551f-472b-a56b-f2f30257d97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_decoder_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_1 (Encoder)         multiple                  22719068  \n",
      "                                                                 \n",
      " decoder_1 (Decoder)         multiple                  60027632  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,746,700\n",
      "Trainable params: 82,746,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model.build([(512,35),(512,35)])\n",
    "#(batch_size,maxlen)\n",
    "model.build([(32,51),(32,51)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6385ae0-abd7-4656-9cd1-6c5537d667be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"models/attention_concat_best.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c161ed9c-070d-4aa2-ac0f-221e3098cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ita_text,model):\n",
    "    '''THIS FUNCTION IS USED IN INFERENCE TIME WHICH GIVEN ANY SENTENCE IN ITALIAN OUTPUTS THE ENGLISH SENTENCE AND ALPHA VALUES'''\n",
    "    # FORMING TOKENIZED SEQUENCES FOR INPUT SENTENCE\n",
    "    seq = tk_inp.texts_to_sequences([ita_text])\n",
    "    # PADDING THE SEQUENCES\n",
    "    seq = pad_sequences(seq,maxlen = 20 , padding=\"post\")\n",
    "    # INITIALIZING THE STATES FOR INPUTING TO ENCODER\n",
    "    state = model.layers[0].initialize(1)\n",
    "    # GETTING THE ENCODED OUTPUT\n",
    "    enc_output,state_h,state_c= model.layers[0](seq,state)\n",
    "    # VARIABLE TO STORE PREDICTED SENTENCE\n",
    "    pred = []\n",
    "    # THIS VARIABLE STORES THE STATE TO BE INPUTED TO ONE STEP ENCODER\n",
    "    input_state_h = state_h\n",
    "    input_state_c = state_c\n",
    "    # THIS VARIABLE STORES THE VECTOR TO VE INPUTED TO ONE STEP ENCODER\n",
    "    current_vec = tf.ones((1,1))\n",
    "    # THIS VARIABLE WILL STORE ALL THE ALPHA VALUES OUTPUTS\n",
    "    alpha_values = []\n",
    "\n",
    "    for i in range(20):\n",
    "        # PASSING THE REQUIRED VARIABLE TO ONE STEP ENCODER LAYER\n",
    "        fc , dec_state_h ,dec_state_c, alphas = model.layers[1].layers[0](enc_output , current_vec ,input_state_h ,input_state_c)\n",
    "        #APPENDING THE ALPHA VALUES TO THE LIST \"alpha_values\"\n",
    "        alpha_values.append(alphas)\n",
    "         # UPDATING THE CURRENT VECTOR \n",
    "        current_vec = np.argmax(fc , axis = -1)\n",
    "         # UPDATING THE INPUT STATE\n",
    "        input_state_h = dec_state_h\n",
    "        input_state_c = dec_state_c\n",
    "        # GETTING THE ACTUAL WORDS FRO THE TOKENIZED INDEXES\n",
    "        pred.append(tk_out.index_word[current_vec[0][0]])\n",
    "        # IF THE WORD \"<end>\" COMES THE LOOP WILL BREAK\n",
    "        if tk_out.index_word[current_vec[0][0]]==\"<end>\":\n",
    "              break\n",
    "    # JOINING THE PREDICTED WORDS\n",
    "    pred_sent = \" \".join(pred)\n",
    "    # CONCATINATING ALL THE ALPHA VALUES\n",
    "    alpha_values = tf.squeeze(tf.concat(alpha_values,axis=-1),axis=0)\n",
    "    # RETURNING THE PREDICTED SENTENCE AND ALPHA VALUES\n",
    "    return  pred_sent , alpha_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b19c8d5b-2cc3-413a-aa46-68c5db1ce705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score as bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26953b01-d04e-4dee-9d03-afc091ccd837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [04:44,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BELU =  0.7554539743100016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BLEU = []\n",
    "index = []\n",
    "np.random.seed(1)\n",
    "test_data = df_val.loc[np.random.choice(df_val.index,size = 2000,replace=False)]\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        pred = predict(str(i.enc_input),model)[0].split()\n",
    "        act = [str(i.dec_output).split()]\n",
    "        b = bleu.sentence_bleu(act,pred)\n",
    "        BLEU.append(b)\n",
    "    except:\n",
    "        index.append(ind)\n",
    "        continue\n",
    "print(\"BELU = \", np.mean(BLEU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d17bad6f-cfb2-4192-829c-11a39ebeb10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  वह जमाना गया जब धन कमाने में काफी समय लगता थे और कड़ी मेहनत करनी पड़ती थी ।\n",
      "PREDICTED SENTENCE ===>  वह जमाना गया जब धन कमाने में काफी समय लगता था और कड़ी मेहनत करनी पड़ती थी । <end>\n",
      "ACTUAL SENTENCE ===>  वह जमाना गया जब धन कमाने में काफी समय लगता था और कड़ी मेहनत करनी पड़ती थी ।  <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[19])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[19],model)[0])\n",
    "print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adab3b53-0da9-4fa9-83da-3e5956229331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  नारों की आवाज मुखिया को ठीक उसी तरह खीच लाई जैसे बीन की धुन सांप को बाहर खीच लाती हैं ।\n",
      "PREDICTED SENTENCE ===>  की आवाज मुखिया को ठीक उसी तरह खीच लाई जैसे बीन की धुन सांप को बाहर खीच लाती है ।\n",
      "ACTUAL SENTENCE ===>  नारों की आवाज मुखिया को ठीक उसी तरह खीच लाई जैसे बीन की धुन सांप को बाहर खीच लाती है ।  <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[50])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[50],model)[0])\n",
    "print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33d17783-37e1-4767-8011-0ae432c9e466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  वह एक गिलास शर्बत\n",
      "PREDICTED SENTENCE ===>  ('उसने एक गिलास शर्बत <end>', <tf.Tensor: shape=(20, 5), dtype=float32, numpy=\n",
      "array([[9.99980330e-01, 3.38565878e-05, 3.34737149e-09, 1.54150968e-08,\n",
      "        3.54617447e-09],\n",
      "       [5.36854759e-06, 9.96379793e-01, 2.16728786e-05, 5.31993351e-07,\n",
      "        1.10517124e-06],\n",
      "       [2.66608311e-07, 3.56699037e-03, 9.99834538e-01, 1.25003382e-04,\n",
      "        1.36153421e-05],\n",
      "       [2.50011482e-07, 1.46757884e-05, 1.42085613e-04, 9.99401689e-01,\n",
      "        6.00945379e-04],\n",
      "       [2.13182346e-07, 2.19969661e-06, 1.65240044e-06, 4.68088809e-04,\n",
      "        9.97370362e-01],\n",
      "       [3.65056856e-08, 2.41503699e-06, 2.28207444e-08, 4.59519060e-06,\n",
      "        2.00175145e-03],\n",
      "       [1.10917696e-07, 1.69844967e-07, 7.72128528e-10, 5.44818306e-08,\n",
      "        1.08415861e-05],\n",
      "       [1.77963798e-07, 4.35481073e-09, 3.91599357e-11, 7.95768607e-09,\n",
      "        1.22674066e-06],\n",
      "       [3.57622048e-06, 7.50106366e-10, 6.77879541e-12, 6.59068577e-10,\n",
      "        4.17632116e-08],\n",
      "       [2.00795535e-06, 1.80866078e-09, 1.15391065e-11, 5.20947674e-10,\n",
      "        3.29417773e-08],\n",
      "       [2.23676170e-06, 6.70141498e-10, 4.22682896e-12, 2.54959165e-10,\n",
      "        3.91329635e-09],\n",
      "       [9.21124808e-07, 3.17244508e-10, 2.81212206e-12, 1.47252877e-10,\n",
      "        3.76009945e-10],\n",
      "       [3.00158007e-07, 3.93539035e-10, 5.48720661e-12, 3.12586956e-10,\n",
      "        9.51663609e-11],\n",
      "       [2.52828784e-07, 4.39914688e-10, 5.72436716e-12, 9.63973013e-10,\n",
      "        1.81678159e-10],\n",
      "       [3.68944427e-07, 6.57736532e-10, 6.53273356e-12, 1.26760213e-09,\n",
      "        2.19879240e-10],\n",
      "       [6.49572883e-07, 9.14942844e-10, 7.67353282e-12, 1.11745846e-09,\n",
      "        1.59868396e-10],\n",
      "       [7.50369679e-07, 1.02860565e-09, 9.20207660e-12, 1.03916487e-09,\n",
      "        1.30333785e-10],\n",
      "       [7.54217297e-07, 1.05366738e-09, 9.89199000e-12, 1.03737074e-09,\n",
      "        1.31768541e-10],\n",
      "       [7.24936399e-07, 1.03266118e-09, 9.99297086e-12, 1.06598141e-09,\n",
      "        1.48273741e-10],\n",
      "       [6.78232880e-07, 9.80950321e-10, 1.01657207e-11, 1.05957165e-09,\n",
      "        1.63427785e-10]], dtype=float32)>)\n",
      "ACTUAL SENTENCE ===>  उसने एक गिलास शर्बत <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[4])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[4],model))\n",
    "print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ce87336-f418-4775-ab4e-2626effd264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  गुजरात और यूपी भी में हुए ऐसे मामले\n",
      "PREDICTED SENTENCE ===>  ('गुजरात और यूपी में भी हुए ऐसे मामले <end>', <tf.Tensor: shape=(20, 9), dtype=float32, numpy=\n",
      "array([[9.98853922e-01, 3.24899411e-05, 1.24110766e-08, 6.57438237e-09,\n",
      "        1.43162726e-08, 4.03919076e-09, 4.15274526e-09, 3.89571087e-09,\n",
      "        3.35853144e-07],\n",
      "       [8.63616006e-05, 9.99728620e-01, 1.60602099e-06, 4.72136236e-07,\n",
      "        9.05473280e-06, 2.34476005e-07, 2.28071038e-08, 1.53203814e-07,\n",
      "        1.05292065e-05],\n",
      "       [3.40116203e-05, 1.93202126e-04, 9.99943852e-01, 1.02320351e-04,\n",
      "        4.15916962e-04, 1.57351235e-06, 5.02141631e-07, 1.36452240e-06,\n",
      "        2.14738952e-06],\n",
      "       [1.56725764e-05, 2.01702496e-05, 7.31064256e-06, 9.99598444e-01,\n",
      "        1.30946434e-03, 3.41392797e-06, 1.20849086e-06, 4.17459796e-06,\n",
      "        1.74643228e-05],\n",
      "       [2.84863418e-05, 4.97813380e-06, 3.47487439e-05, 1.88356353e-05,\n",
      "        9.92558360e-01, 1.51248113e-03, 1.81295163e-05, 9.45820866e-05,\n",
      "        9.75792955e-06],\n",
      "       [1.02311496e-05, 1.12751695e-05, 9.32543935e-06, 6.90869201e-05,\n",
      "        3.80385411e-03, 9.96370673e-01, 1.27386034e-03, 1.03409096e-04,\n",
      "        8.09186076e-06],\n",
      "       [5.09485108e-06, 1.35794608e-06, 2.18713762e-06, 6.75128877e-06,\n",
      "        5.79261978e-04, 1.87492836e-03, 9.97953057e-01, 6.78521115e-04,\n",
      "        9.33779156e-05],\n",
      "       [1.67537637e-05, 2.49897676e-06, 5.81720997e-07, 2.01258634e-04,\n",
      "        1.27332867e-03, 2.28578196e-04, 7.46095204e-04, 9.98525083e-01,\n",
      "        8.86911619e-03],\n",
      "       [4.28472376e-05, 4.20015249e-06, 3.37267522e-07, 2.74106674e-06,\n",
      "        2.87270195e-05, 7.56967484e-06, 5.53599602e-06, 5.81575499e-04,\n",
      "        9.85098958e-01],\n",
      "       [5.18388561e-06, 7.61353704e-07, 1.43679673e-08, 8.71283703e-08,\n",
      "        2.13573549e-05, 4.54792911e-07, 1.43648629e-06, 1.11324316e-05,\n",
      "        4.83001722e-03],\n",
      "       [2.48722590e-05, 1.67861145e-07, 8.17188983e-10, 1.31968356e-08,\n",
      "        3.63832697e-07, 4.25452882e-08, 4.35208349e-08, 5.18097316e-08,\n",
      "        9.12793446e-04],\n",
      "       [3.17827971e-06, 3.50663534e-08, 1.67722036e-09, 7.76019515e-09,\n",
      "        1.42946433e-07, 3.50125351e-09, 2.89276034e-08, 5.36141220e-09,\n",
      "        3.07772025e-05],\n",
      "       [3.71558854e-05, 7.99061084e-08, 1.14696308e-09, 1.66249965e-08,\n",
      "        8.20239947e-08, 3.90999277e-09, 1.96795238e-08, 1.22501067e-08,\n",
      "        3.44831060e-05],\n",
      "       [3.43480788e-05, 5.17885468e-08, 8.16534618e-10, 1.42384611e-08,\n",
      "        3.63687143e-08, 1.54977686e-09, 3.27922001e-09, 1.78439716e-08,\n",
      "        4.47801867e-05],\n",
      "       [2.39339643e-04, 5.95074567e-09, 4.11468186e-11, 2.41646231e-10,\n",
      "        7.64504682e-10, 1.70975872e-10, 2.84416407e-10, 3.50624840e-09,\n",
      "        3.42871062e-05],\n",
      "       [2.32285878e-04, 2.60089839e-09, 2.33668935e-11, 7.89009344e-11,\n",
      "        3.74602155e-10, 1.93114118e-11, 2.22739431e-11, 9.71016600e-10,\n",
      "        2.59616672e-06],\n",
      "       [1.19898570e-04, 3.08069481e-09, 8.96383315e-12, 5.16029650e-11,\n",
      "        1.60545022e-10, 1.17526639e-11, 5.81989968e-12, 9.12115522e-11,\n",
      "        1.88927316e-07],\n",
      "       [8.20796777e-05, 3.75401843e-09, 6.66546072e-12, 5.17308800e-11,\n",
      "        2.27602812e-10, 1.38771859e-11, 3.44688080e-12, 5.56847658e-11,\n",
      "        1.12703745e-07],\n",
      "       [6.58024001e-05, 3.99315336e-09, 6.40069074e-12, 5.30865837e-11,\n",
      "        2.33197683e-10, 1.47815735e-11, 2.38656924e-12, 4.46175805e-11,\n",
      "        8.16961006e-08],\n",
      "       [6.24687673e-05, 4.26277058e-09, 6.67933200e-12, 5.20908837e-11,\n",
      "        2.84262974e-10, 2.18745595e-11, 1.90926810e-12, 3.88860957e-11,\n",
      "        6.43134968e-08]], dtype=float32)>)\n",
      "ACTUAL SENTENCE ===>  गुजरात और यूपी में भी हुए ऐसे मामले <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[20])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[20],model))\n",
    "print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef8270cd-6983-48d2-a3cd-80381878000a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  अमेरिका भी इससे अलग नहीं हैं ।\n",
      "PREDICTED SENTENCE ===>  ('अमेरिका भी इससे अलग नहीं है । <end>', <tf.Tensor: shape=(20, 8), dtype=float32, numpy=\n",
      "array([[9.98489618e-01, 4.81580995e-04, 2.19598448e-07, 1.35816949e-08,\n",
      "        1.94864236e-09, 9.12403131e-09, 1.03830771e-05, 1.54007648e-04],\n",
      "       [2.72520178e-04, 9.98108029e-01, 1.19585848e-05, 1.40193407e-07,\n",
      "        6.12408769e-07, 2.49883129e-07, 8.15184649e-06, 1.19040417e-03],\n",
      "       [4.28637723e-05, 1.07149221e-03, 9.79689360e-01, 5.15208194e-05,\n",
      "        6.34142907e-06, 4.35848187e-06, 3.04026571e-06, 8.49192031e-04],\n",
      "       [1.60442651e-05, 1.49708620e-04, 2.02006809e-02, 9.98209715e-01,\n",
      "        4.32106387e-03, 9.83662358e-06, 6.99895918e-06, 3.86773696e-04],\n",
      "       [1.27743651e-05, 1.55754969e-05, 5.35609906e-05, 1.72620045e-03,\n",
      "        9.95338202e-01, 1.28763886e-02, 1.24779399e-04, 1.46938735e-04],\n",
      "       [2.33619521e-05, 1.30977842e-05, 3.63253312e-05, 1.16868978e-05,\n",
      "        3.04110232e-04, 9.87013876e-01, 6.52408646e-03, 6.50718866e-04],\n",
      "       [1.44470760e-05, 1.77668371e-05, 5.79396385e-07, 7.26022762e-08,\n",
      "        1.27702447e-06, 4.47445964e-05, 9.89584148e-01, 3.19831856e-02],\n",
      "       [5.32316449e-07, 1.23001533e-04, 5.17621811e-06, 2.72426313e-08,\n",
      "        3.39004743e-07, 6.17837031e-07, 2.08930811e-03, 9.25421357e-01],\n",
      "       [2.89726540e-06, 5.77508763e-06, 7.91700074e-07, 5.97478973e-08,\n",
      "        1.13633666e-06, 1.01363839e-05, 6.76582451e-04, 3.18494402e-02],\n",
      "       [6.52069066e-05, 9.39487927e-06, 1.01871751e-06, 4.69101906e-07,\n",
      "        1.43712268e-05, 1.46499988e-05, 7.57207570e-04, 4.41243593e-03],\n",
      "       [1.34236310e-04, 3.94724657e-06, 2.21832764e-07, 2.07848103e-07,\n",
      "        1.23374830e-05, 4.64536197e-06, 1.06667809e-04, 1.94129499e-03],\n",
      "       [4.34663816e-05, 2.76918286e-07, 5.16114476e-08, 3.64548636e-09,\n",
      "        1.04546004e-07, 1.86255002e-05, 4.14911337e-05, 2.76671548e-04],\n",
      "       [2.37433269e-04, 8.86749305e-08, 2.66711631e-09, 2.84282320e-10,\n",
      "        3.82305076e-09, 1.67514111e-06, 5.26199037e-05, 4.37492796e-04],\n",
      "       [1.95419270e-04, 5.95848810e-08, 9.20902965e-10, 2.35247950e-11,\n",
      "        5.36685252e-10, 1.40590359e-07, 1.02590675e-05, 1.99973743e-04],\n",
      "       [1.65478588e-04, 5.61857512e-08, 3.72304881e-10, 7.47446462e-12,\n",
      "        1.84375806e-10, 3.43803652e-09, 3.94715153e-06, 8.55326434e-05],\n",
      "       [8.69057039e-05, 3.53919347e-08, 1.01744065e-10, 4.93432031e-12,\n",
      "        3.91913862e-11, 1.48893384e-10, 1.80970986e-07, 3.18524758e-06],\n",
      "       [5.58880529e-05, 1.99354933e-08, 5.01651463e-11, 7.76459712e-12,\n",
      "        2.04928626e-11, 6.28220462e-11, 4.12770440e-08, 2.82081874e-06],\n",
      "       [5.46280389e-05, 2.27927686e-08, 4.34753900e-11, 8.84790158e-12,\n",
      "        1.71178887e-11, 5.68312411e-11, 3.27490071e-08, 2.89877698e-06],\n",
      "       [4.89095037e-05, 3.01883709e-08, 4.30647185e-11, 1.24832540e-11,\n",
      "        1.13364396e-11, 3.83073122e-11, 2.71605476e-08, 2.72760894e-06],\n",
      "       [3.73115399e-05, 3.31811236e-08, 4.58394434e-11, 2.20945692e-11,\n",
      "        8.30853095e-12, 2.68057295e-11, 2.84647186e-08, 3.07366076e-06]],\n",
      "      dtype=float32)>)\n",
      "ACTUAL SENTENCE ===>  अमेरिका भी इससे अलग नहीं है ।  <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[500])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[500],model))\n",
    "print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f904dd77-c409-4e49-b096-03a43dcb96ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  जनपथ के एक दुकानदार नितिन कहते है कि अबतक उन्होंने किसी को भी पॉलिथीन चेकिंग करते नहीं देखा\n",
      "PREDICTED SENTENCE ===>  ('जनपथ ने अबतक एक एक दुकानदार नितिन कहते हैं कि अबतक उन्होंने किसी को भी पॉलिथीन चेकिंग करते नहीं देखा', <tf.Tensor: shape=(20, 20), dtype=float32, numpy=\n",
      "array([[9.99808013e-01, 5.50184548e-01, 1.20955880e-03, 1.68752376e-04,\n",
      "        9.25014228e-06, 9.27644635e-07, 1.59587131e-07, 5.04325826e-08,\n",
      "        8.28998566e-07, 3.31519027e-06, 2.40469417e-05, 7.89095168e-07,\n",
      "        3.59310860e-07, 7.36679056e-07, 1.18249838e-04, 1.24103494e-07,\n",
      "        8.88263063e-09, 9.07024571e-07, 1.14412487e-05, 1.77769402e-06],\n",
      "       [9.93301455e-07, 3.91819447e-01, 1.12764470e-01, 2.20605335e-03,\n",
      "        2.05609304e-05, 6.29920905e-05, 2.19002268e-05, 4.12795771e-06,\n",
      "        7.08638254e-06, 5.15242718e-05, 1.85320932e-05, 2.46812601e-06,\n",
      "        1.52841380e-06, 9.12627627e-08, 1.00181273e-06, 9.44384038e-09,\n",
      "        3.41153950e-09, 2.69732308e-08, 2.77529875e-06, 1.08286059e-07],\n",
      "       [9.98683390e-07, 2.28810012e-02, 9.07379910e-02, 8.29028606e-01,\n",
      "        5.54732978e-01, 5.24168788e-03, 9.66466687e-05, 5.53027633e-07,\n",
      "        3.09662482e-07, 5.44235081e-06, 2.52752053e-03, 1.22600482e-04,\n",
      "        1.64576377e-05, 3.93065250e-07, 3.35359073e-06, 1.08182157e-07,\n",
      "        1.62494906e-08, 6.86908095e-08, 1.89685557e-06, 3.73669735e-07],\n",
      "       [1.02859610e-06, 7.45485479e-04, 2.60083145e-03, 4.14343961e-02,\n",
      "        4.40700382e-01, 9.86984134e-01, 7.32556276e-04, 4.10363373e-06,\n",
      "        1.88994613e-06, 2.56951716e-06, 1.78434420e-04, 1.30216111e-04,\n",
      "        4.49935178e-05, 4.57364467e-06, 2.37754302e-05, 2.27650503e-06,\n",
      "        8.99915506e-07, 4.95065137e-07, 1.27707199e-05, 9.13098802e-07],\n",
      "       [1.50489961e-06, 7.53082422e-05, 2.28630798e-03, 4.48069209e-03,\n",
      "        4.36355796e-04, 7.18832365e-04, 9.90894914e-01, 2.75097205e-04,\n",
      "        3.67976791e-05, 9.77418949e-06, 1.23514701e-06, 5.00667156e-05,\n",
      "        6.14443619e-04, 1.84748533e-05, 2.72574434e-05, 4.36286030e-07,\n",
      "        2.69354368e-05, 2.63316929e-06, 2.12579325e-05, 2.39446103e-06],\n",
      "       [2.87349235e-06, 9.23120024e-05, 4.80498141e-03, 9.73463699e-04,\n",
      "        1.85093581e-04, 1.29997919e-04, 1.62215860e-04, 9.93317246e-01,\n",
      "        2.88594951e-04, 1.99452206e-05, 8.80771859e-06, 3.29745053e-05,\n",
      "        4.85168493e-05, 4.58722352e-05, 1.35994435e-06, 1.48232871e-07,\n",
      "        7.27439442e-08, 1.76751957e-04, 2.74278664e-05, 1.42394811e-06],\n",
      "       [7.03590160e-07, 4.63571167e-04, 7.44830933e-04, 9.32693147e-05,\n",
      "        3.28272108e-05, 8.87588249e-05, 7.69719598e-04, 2.81256065e-03,\n",
      "        9.94184792e-01, 2.30403210e-04, 4.48995752e-06, 3.54949452e-06,\n",
      "        9.18204705e-07, 3.32290392e-06, 5.81573340e-06, 1.24819238e-07,\n",
      "        2.08471356e-08, 5.93991217e-06, 5.67051466e-04, 4.61327727e-05],\n",
      "       [9.83970267e-07, 5.53045422e-04, 3.64403278e-02, 1.59003932e-04,\n",
      "        6.01600696e-05, 9.93822778e-06, 1.43267811e-04, 3.02303757e-04,\n",
      "        5.81700413e-04, 9.97185171e-01, 4.27238439e-04, 3.21349908e-05,\n",
      "        3.62891501e-06, 3.43546344e-06, 6.38559413e-07, 6.88871111e-08,\n",
      "        3.64677497e-08, 1.16081594e-06, 5.12713741e-05, 1.88273225e-05],\n",
      "       [6.06746141e-07, 4.38593468e-03, 6.73350334e-01, 2.14087293e-02,\n",
      "        1.25953800e-03, 6.41967199e-05, 8.48000673e-06, 2.34629169e-05,\n",
      "        2.33352166e-05, 1.45490910e-03, 9.95837331e-01, 9.40017286e-04,\n",
      "        4.03766287e-04, 6.56166094e-06, 4.46365539e-06, 1.30359737e-07,\n",
      "        3.04694225e-08, 1.93243039e-07, 7.40344376e-06, 1.82438293e-06],\n",
      "       [8.60518298e-07, 2.40579364e-03, 2.64963172e-02, 7.89900273e-02,\n",
      "        9.24550404e-04, 2.28823512e-04, 1.20246157e-04, 4.03374288e-05,\n",
      "        1.61458374e-05, 1.26052455e-05, 2.39280867e-04, 9.93640006e-01,\n",
      "        6.89677941e-03, 6.21404688e-05, 6.44603688e-06, 2.20525271e-06,\n",
      "        9.25137655e-08, 1.46328216e-06, 1.05442341e-05, 2.45593537e-06],\n",
      "       [5.23566132e-06, 1.20527735e-02, 2.88467482e-02, 1.48138711e-02,\n",
      "        4.00799268e-04, 8.63503810e-05, 5.86975459e-03, 8.12643557e-04,\n",
      "        5.56462437e-05, 1.06537009e-04, 1.69776045e-04, 4.75167250e-03,\n",
      "        9.87297416e-01, 3.17737547e-04, 1.76476780e-04, 5.84875488e-06,\n",
      "        2.79130672e-05, 3.76770149e-05, 4.65799894e-05, 1.45710783e-05],\n",
      "       [1.85687786e-05, 6.29983726e-04, 8.96660611e-04, 4.26495862e-05,\n",
      "        6.94387563e-05, 8.73066965e-05, 4.33463079e-04, 1.87813479e-03,\n",
      "        4.15818722e-05, 7.43539977e-06, 6.42083978e-05, 3.78813202e-05,\n",
      "        6.14580116e-04, 9.94540155e-01, 3.23433866e-04, 1.55208932e-06,\n",
      "        2.88061665e-06, 1.83886397e-04, 4.53322609e-05, 1.62780343e-05],\n",
      "       [6.04700690e-05, 8.28357507e-03, 2.14899238e-03, 1.58677314e-04,\n",
      "        4.88208716e-05, 3.99771583e-04, 3.42599851e-05, 1.33783280e-04,\n",
      "        6.65235042e-04, 1.30097324e-05, 1.16994481e-04, 3.16090700e-05,\n",
      "        3.54446267e-04, 3.68123059e-03, 8.99552822e-01, 1.84064687e-04,\n",
      "        8.00933958e-06, 1.27184003e-05, 3.86923057e-04, 4.86078670e-06],\n",
      "       [3.47163295e-05, 2.20942427e-03, 1.45253260e-03, 4.95819841e-03,\n",
      "        1.08268345e-03, 5.81982546e-03, 1.03802500e-04, 9.33654519e-06,\n",
      "        5.14430212e-05, 2.98249943e-05, 3.52352945e-04, 1.38216608e-04,\n",
      "        8.79858562e-04, 5.90845535e-04, 9.85302180e-02, 9.97811139e-01,\n",
      "        6.01185660e-04, 3.51990748e-05, 2.47106800e-04, 6.68670255e-05],\n",
      "       [1.31586985e-05, 4.30642860e-04, 1.94138603e-03, 7.54619890e-04,\n",
      "        1.77100665e-05, 5.57091043e-05, 5.71581710e-04, 4.49621657e-05,\n",
      "        1.73507724e-05, 4.82478799e-06, 4.38873030e-06, 6.58650752e-05,\n",
      "        2.71252613e-03, 2.51304358e-04, 2.83558184e-04, 1.83567288e-03,\n",
      "        9.98960257e-01, 1.23949794e-04, 9.04861081e-05, 1.05030649e-05],\n",
      "       [1.00683485e-06, 1.61841745e-05, 1.98246507e-05, 1.50698243e-05,\n",
      "        2.75848583e-06, 4.79256732e-06, 2.09699319e-05, 2.70150165e-04,\n",
      "        1.22153506e-05, 1.77265656e-06, 1.29305965e-06, 4.99683301e-06,\n",
      "        5.14655731e-05, 2.27107113e-04, 6.77333155e-05, 3.37737816e-07,\n",
      "        1.01938887e-04, 9.96259212e-01, 1.45742355e-03, 4.64861296e-05],\n",
      "       [4.93309108e-06, 2.73918777e-05, 1.33330599e-04, 4.02390606e-06,\n",
      "        3.34246153e-07, 1.12088401e-06, 3.20857043e-06, 5.05094322e-05,\n",
      "        3.33679351e-03, 6.87100546e-05, 7.70111728e-06, 8.74645764e-07,\n",
      "        1.67100734e-06, 1.95472778e-04, 7.20614626e-05, 4.05413715e-07,\n",
      "        2.64008340e-05, 2.44365307e-03, 9.31628764e-01, 5.35476021e-04],\n",
      "       [4.27873347e-05, 2.71866471e-03, 1.30894622e-02, 3.04189976e-04,\n",
      "        1.53773399e-05, 1.48604013e-05, 1.00302987e-05, 9.21664559e-06,\n",
      "        6.62666222e-04, 7.58167531e-04, 1.24373273e-05, 1.32365085e-05,\n",
      "        5.52810452e-05, 4.97579567e-05, 7.98863883e-04, 1.54755777e-04,\n",
      "        2.10306913e-04, 3.81832302e-04, 5.68878986e-02, 9.95129943e-01],\n",
      "       [3.69803672e-07, 8.53035544e-06, 2.67637752e-05, 5.35177014e-06,\n",
      "        3.86677328e-07, 1.69186876e-07, 2.60778097e-06, 3.39676672e-06,\n",
      "        8.55907365e-06, 1.92146381e-05, 1.04992353e-06, 7.63010519e-07,\n",
      "        9.62075660e-07, 2.04850679e-07, 8.33625961e-07, 5.83754570e-07,\n",
      "        2.99797230e-05, 1.23456892e-04, 8.27597640e-03, 1.62594940e-03],\n",
      "       [1.93256611e-07, 1.63314216e-05, 8.22890252e-06, 3.09496187e-07,\n",
      "        8.74955575e-09, 1.00253041e-08, 2.03350083e-07, 8.03204875e-06,\n",
      "        7.12365636e-06, 1.47724404e-05, 2.94922052e-06, 1.59426232e-08,\n",
      "        3.36724355e-07, 5.94351377e-07, 1.60514367e-06, 3.51819445e-08,\n",
      "        2.86255317e-06, 2.08743746e-04, 2.19680340e-04, 2.47278530e-03]],\n",
      "      dtype=float32)>)\n",
      "ACTUAL SENTENCE ===>  जनपथ के एक दुकानदार नितिन कहते हैं कि अबतक उन्होंने किसी को भी पॉलिथीन चेकिंग करते नहीं देखा <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[700])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[700],model))\n",
    "print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "906cba19-a8be-4a7c-95c9-513fb0a960c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  इसलिए वैज्ञानिकों ने लाइकोपीन आधारिक एक दवाई बनाई हैं ।\n",
      "PREDICTED SENTENCE ===>  ('इसलिए वैज्ञानिकों ने लाइकोपीन आधारिक एक दवाई बनाई है । <end>', <tf.Tensor: shape=(20, 11), dtype=float32, numpy=\n",
      "array([[9.9877590e-01, 1.3056111e-04, 8.2218043e-07, 1.0026201e-09,\n",
      "        4.5531102e-08, 9.6673254e-08, 7.8770839e-08, 2.0109051e-07,\n",
      "        3.5331817e-07, 8.0025274e-07, 2.8712081e-04],\n",
      "       [2.4995950e-04, 9.9959677e-01, 5.1303894e-05, 3.2703622e-06,\n",
      "        1.8026620e-06, 6.9205424e-07, 3.3638524e-04, 1.1284838e-05,\n",
      "        9.7382963e-07, 2.4866944e-07, 1.0800510e-05],\n",
      "       [1.0347508e-05, 5.3292606e-05, 7.1994048e-01, 8.9224667e-04,\n",
      "        7.0513190e-05, 2.0528291e-06, 5.9535782e-06, 3.3524342e-05,\n",
      "        4.0889546e-07, 4.1862837e-08, 1.4747013e-05],\n",
      "       [2.4307187e-04, 5.0435851e-06, 2.7255586e-01, 9.9904162e-01,\n",
      "        4.1560413e-04, 1.2742775e-05, 1.8226605e-05, 3.0321648e-04,\n",
      "        1.4736061e-06, 9.3462899e-07, 1.2576739e-04],\n",
      "       [1.9582672e-05, 2.8245527e-06, 4.2136264e-04, 2.6888018e-05,\n",
      "        9.9690992e-01, 2.4796622e-03, 5.2528096e-05, 3.1732645e-05,\n",
      "        9.8245742e-05, 9.3962262e-06, 1.1299575e-05],\n",
      "       [5.5214739e-05, 1.5819979e-06, 7.8144985e-05, 2.7380735e-05,\n",
      "        2.3882498e-03, 9.9725038e-01, 5.5388478e-05, 1.9287540e-05,\n",
      "        9.7808144e-05, 5.9218124e-05, 5.3542204e-05],\n",
      "       [3.0938681e-05, 1.7447429e-04, 1.1972956e-05, 1.3193007e-06,\n",
      "        2.1283588e-05, 1.2572999e-04, 9.9915051e-01, 2.0372925e-05,\n",
      "        1.0903043e-05, 2.0854928e-05, 2.9161951e-04],\n",
      "       [5.1617291e-05, 2.8747998e-05, 6.9238562e-03, 7.2233042e-06,\n",
      "        1.3561024e-04, 1.0004137e-04, 3.6474908e-04, 9.9939191e-01,\n",
      "        1.1886972e-03, 5.7909619e-06, 2.7031822e-03],\n",
      "       [3.6980655e-05, 4.8457678e-06, 1.3217890e-05, 3.3714528e-08,\n",
      "        4.5199813e-05, 1.2431728e-05, 1.1177575e-05, 1.3358261e-04,\n",
      "        9.9826223e-01, 8.7563203e-05, 7.2221615e-04],\n",
      "       [1.1676749e-04, 8.2192582e-07, 3.6889602e-07, 8.6607708e-09,\n",
      "        1.5751613e-06, 8.1749367e-06, 4.1952144e-06, 5.7409284e-07,\n",
      "        2.0821905e-04, 9.9891484e-01, 5.8157521e-04],\n",
      "       [4.7690319e-05, 6.2593017e-07, 7.5844224e-07, 1.1123960e-08,\n",
      "        8.9618489e-08, 4.1185604e-07, 1.7012084e-07, 1.8308821e-05,\n",
      "        1.7203056e-05, 3.9416426e-04, 9.8913199e-01],\n",
      "       [7.4879540e-06, 5.2697573e-08, 4.9901735e-07, 3.8144203e-09,\n",
      "        7.7852792e-07, 6.5471971e-07, 1.1150925e-07, 9.3402150e-06,\n",
      "        5.6606201e-05, 2.3075091e-04, 2.4038977e-03],\n",
      "       [3.9709310e-05, 6.9429440e-08, 6.4615341e-07, 3.7588452e-09,\n",
      "        6.7696114e-06, 1.7871946e-06, 1.7115958e-07, 1.1566251e-05,\n",
      "        2.0729130e-05, 1.0898977e-04, 1.9959006e-03],\n",
      "       [8.7013148e-05, 7.0291136e-08, 4.7522477e-07, 1.2057131e-08,\n",
      "        2.1105282e-06, 2.5364243e-06, 1.6395394e-07, 8.5258653e-06,\n",
      "        2.1595255e-05, 4.9714188e-05, 7.1855239e-04],\n",
      "       [2.5136977e-05, 9.6504898e-08, 2.1969318e-07, 2.5648013e-09,\n",
      "        2.7514969e-07, 1.6305183e-06, 1.0826334e-07, 5.8249352e-06,\n",
      "        1.0418213e-05, 4.8373047e-05, 2.8527220e-04],\n",
      "       [1.3907505e-05, 1.0733626e-07, 8.2597040e-08, 1.0643340e-09,\n",
      "        1.0036602e-07, 5.6526500e-07, 7.2925083e-08, 7.0999391e-07,\n",
      "        2.7156052e-06, 5.1232837e-05, 3.7477302e-04],\n",
      "       [3.5042514e-05, 3.3420672e-08, 1.4351159e-08, 6.0531591e-10,\n",
      "        6.5091406e-08, 2.5036064e-07, 1.8969223e-08, 4.7872863e-08,\n",
      "        9.8302917e-07, 1.0970937e-05, 1.6291768e-04],\n",
      "       [4.8958391e-05, 9.8051505e-09, 3.6591004e-09, 2.1692159e-10,\n",
      "        2.2136357e-08, 1.6670096e-07, 5.7316325e-09, 8.2051379e-09,\n",
      "        3.7831188e-07, 3.9497554e-06, 8.1412130e-05],\n",
      "       [6.2624822e-05, 4.8782900e-10, 5.4926225e-10, 3.5267227e-11,\n",
      "        1.5418241e-09, 1.9503846e-08, 6.7552103e-10, 1.3917368e-09,\n",
      "        3.5582744e-08, 1.6298416e-06, 3.5778197e-05],\n",
      "       [4.2073920e-05, 1.9152767e-10, 4.3273973e-10, 9.6256111e-12,\n",
      "        3.5208644e-10, 1.2882816e-09, 3.0052777e-10, 2.6866870e-10,\n",
      "        6.0040266e-09, 6.1709136e-07, 7.7428786e-06]], dtype=float32)>)\n",
      "ACTUAL SENTENCE ===>  इसलिए वैज्ञानिकों ने लाइकोपीन आधारिक एक दवाई बनाई है ।  <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[900])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[900],model))\n",
    "print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc270dd9-a22d-4d1c-9bd7-257cbc2d8c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  पांचवें दिन फौज के लोगों को फकीरी दी जाती थे ।\n",
      "PREDICTED SENTENCE ===>  ('पांचवें दिन फौज के लोगों को फकीरी दी जाती थी । <end>', <tf.Tensor: shape=(20, 12), dtype=float32, numpy=\n",
      "array([[9.99896288e-01, 2.89322305e-02, 1.31963091e-07, 1.18955290e-08,\n",
      "        8.82760673e-08, 9.17517582e-08, 1.66982339e-08, 1.55641061e-07,\n",
      "        1.24816868e-08, 2.27507272e-07, 4.72933334e-06, 8.51445634e-08],\n",
      "       [2.44058742e-06, 9.68366563e-01, 1.40338307e-04, 1.21325320e-06,\n",
      "        1.14710092e-05, 8.89279590e-07, 7.31612658e-07, 1.72580951e-06,\n",
      "        1.30168814e-06, 2.31445188e-06, 3.03350339e-06, 5.63738604e-06],\n",
      "       [1.65161703e-06, 1.38122356e-03, 9.99459922e-01, 1.84743118e-03,\n",
      "        2.77654590e-06, 3.29397080e-05, 1.56429189e-06, 9.67641517e-06,\n",
      "        1.04639139e-05, 2.71209205e-06, 1.92263724e-06, 1.47236403e-06],\n",
      "       [3.25636847e-06, 8.44085025e-06, 1.33890018e-04, 9.97226298e-01,\n",
      "        8.09919220e-06, 1.75741866e-06, 4.27593841e-06, 1.86609923e-05,\n",
      "        3.86952415e-05, 9.73264378e-06, 8.51010839e-07, 5.30769285e-07],\n",
      "       [2.54628435e-06, 1.09936322e-04, 5.76938874e-07, 1.27192779e-05,\n",
      "        9.99812186e-01, 1.67767488e-04, 1.08460888e-04, 5.48717253e-05,\n",
      "        7.07441313e-06, 1.61258977e-05, 2.27752525e-06, 1.00404293e-06],\n",
      "       [1.42114709e-06, 5.14215324e-04, 2.49610457e-04, 3.70334819e-05,\n",
      "        1.05606334e-04, 9.98315930e-01, 1.65949599e-03, 6.05956558e-03,\n",
      "        9.28045665e-06, 4.02067781e-06, 4.14955502e-06, 9.69555367e-06],\n",
      "       [2.40378085e-06, 4.51531407e-04, 4.81376310e-06, 5.45162766e-05,\n",
      "        4.19077041e-05, 2.40818030e-04, 9.98153389e-01, 9.97193041e-04,\n",
      "        1.20187051e-05, 1.84613164e-05, 1.00161524e-05, 9.30487295e-06],\n",
      "       [9.92444257e-07, 3.56116107e-05, 8.19058096e-06, 1.01324324e-04,\n",
      "        1.24618919e-05, 1.22166809e-03, 2.59064400e-05, 9.91826415e-01,\n",
      "        9.16408317e-04, 4.39430820e-04, 1.37490097e-05, 6.69717338e-06],\n",
      "       [1.46962736e-06, 5.40607944e-05, 1.54244231e-06, 6.92018133e-04,\n",
      "        2.74514332e-06, 8.37417883e-06, 4.45792939e-05, 8.78674677e-04,\n",
      "        9.95396674e-01, 1.97605230e-03, 3.32866148e-05, 4.14725837e-05],\n",
      "       [4.55203553e-06, 9.55898304e-06, 2.70006893e-07, 2.75237176e-06,\n",
      "        1.02971137e-06, 3.19854894e-06, 1.11484644e-06, 1.29993452e-04,\n",
      "        3.53256520e-03, 9.93473232e-01, 1.10953180e-02, 4.63709061e-04],\n",
      "       [1.98171938e-06, 6.37838602e-05, 2.10149466e-07, 3.87797883e-07,\n",
      "        2.64373057e-07, 1.86519526e-06, 1.27679925e-07, 1.41824630e-05,\n",
      "        4.24257196e-05, 4.00904054e-03, 9.81157839e-01, 7.21841492e-03],\n",
      "       [5.01983493e-07, 6.52578456e-05, 1.36106351e-07, 5.17038416e-07,\n",
      "        3.70093289e-08, 1.59598119e-06, 2.41977887e-08, 1.11307475e-06,\n",
      "        3.68341347e-07, 1.50109345e-05, 7.48489099e-03, 9.91629541e-01],\n",
      "       [2.85142193e-07, 2.20074571e-06, 1.48657904e-07, 1.39924068e-05,\n",
      "        7.89066377e-08, 9.21862295e-07, 1.10958169e-07, 7.26897554e-07,\n",
      "        2.69678526e-06, 7.46449041e-06, 1.32932299e-04, 5.23325289e-04],\n",
      "       [1.08839595e-05, 2.54517431e-06, 2.51323105e-08, 4.55694953e-06,\n",
      "        9.34018203e-07, 1.03396610e-06, 1.03254486e-07, 1.90306002e-06,\n",
      "        1.59796616e-06, 1.03000539e-05, 3.38548161e-05, 5.42093840e-05],\n",
      "       [1.97995860e-06, 2.41492648e-06, 1.58572121e-07, 2.87264811e-06,\n",
      "        2.38743951e-07, 8.64858919e-07, 4.92255587e-08, 2.17018555e-06,\n",
      "        3.22392543e-06, 9.54818006e-07, 8.02837985e-06, 1.43206416e-05],\n",
      "       [1.90896958e-06, 2.14625103e-07, 5.09721261e-08, 1.22105814e-06,\n",
      "        7.83093057e-08, 6.56299619e-08, 1.11701741e-08, 1.34308925e-06,\n",
      "        1.01782698e-05, 3.07762002e-06, 4.80577501e-06, 6.14740111e-06],\n",
      "       [2.20587644e-05, 1.04784135e-07, 1.55284621e-08, 8.95245478e-07,\n",
      "        4.49758595e-08, 1.71378467e-08, 1.72341341e-09, 1.06187656e-06,\n",
      "        8.47324009e-06, 4.08190317e-06, 5.86085298e-06, 8.14273335e-06],\n",
      "       [2.68638905e-05, 9.51001766e-08, 6.57216770e-09, 2.19165358e-07,\n",
      "        1.09963789e-08, 2.32331470e-08, 2.80659357e-10, 2.53702837e-07,\n",
      "        3.39228905e-06, 3.68730161e-06, 1.36409665e-06, 2.01755051e-06],\n",
      "       [1.09530474e-05, 6.05429236e-08, 3.30788463e-09, 1.20228719e-07,\n",
      "        1.26591937e-08, 3.52923806e-08, 4.07691603e-10, 2.23815974e-07,\n",
      "        2.50099038e-06, 2.30070236e-06, 6.90863374e-07, 3.36970265e-06],\n",
      "       [5.60367243e-06, 2.08107611e-08, 3.34565597e-10, 1.86866895e-08,\n",
      "        9.75244063e-09, 9.44533785e-09, 1.01575609e-10, 8.00791824e-08,\n",
      "        7.62063848e-07, 1.73375031e-06, 4.02992441e-07, 7.67658605e-07]],\n",
      "      dtype=float32)>)\n",
      "ACTUAL SENTENCE ===>  पांचवें दिन फौज के लोगों को फकीरी दी जाती थी ।  <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[550])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(df_test.enc_input.values[550],model))\n",
    "print(\"ACTUAL SENTENCE ===> \",df_test.dec_output.values[550])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b212a30-b84b-4f58-85bb-568c1dbe2b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 199 ms, sys: 0 ns, total: 199 ms\n",
      "Wall time: 186 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'की आवाज मुखिया को ठीक उसी तरह खीच लाई जैसे बीन की धुन सांप को बाहर खीच लाती है ।'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predict(df_test.enc_input.values[50],model)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0b2466c-8376-436b-ac05-c27685aeee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(input,model,k):\n",
    "    seq = tk_inp.texts_to_sequences([input])\n",
    "    seq = pad_sequences(seq,maxlen = 51,padding=\"post\")\n",
    "\n",
    "    state = model.layers[0].initialize(1)\n",
    "    # GETTING THE ENCODED OUTPUT\n",
    "    enc_output,enc_state_h,enc_state_c = model.layers[0](seq,state)\n",
    "    \n",
    "\n",
    "    input_state_h = enc_state_h\n",
    "    input_state_c = enc_state_c \n",
    "    k_beams = [[tf.ones((1,1),dtype=tf.int32),0.0]]\n",
    "    for i in range(51):\n",
    "        candidates = []\n",
    "        for sent_pred , prob in k_beams :\n",
    "            if tk_out.word_index[\"<end>\"] in sent_pred.numpy() :\n",
    "                candidates += [[sent_pred , prob]]\n",
    "            else:\n",
    "               \n",
    "                dec_input = model.layers[1].layers[0].layers[0](sent_pred)\n",
    "                dec_output , dec_state_h , dec_state_c   =  model.layers[1].layers[0].layers[2](dec_input ,  initial_state =  [input_state_h , input_state_c])\n",
    "\n",
    "                context_vec , alphas =  model.layers[1].layers[0].layers[1](enc_output,dec_state_h)\n",
    "\n",
    "                # CONCATINATING THE CONTEXT VECTOR(BY EXPANDING DIMENSION) AND ENBEDDED VECTOR\n",
    "                dense_input =  tf.concat([tf.expand_dims(context_vec,1),tf.expand_dims(dec_state_h,1)],axis=-1)\n",
    "                \n",
    "                # PASSING THE DECODER OUTPUT THROUGH DENSE LAYER WITH UNITS EQUAL TO VOCAB SIZE\n",
    "                dense = model.layers[1].layers[0].layers[3](dense_input)\n",
    "\n",
    "                pred = tf.argsort(dense, direction= 'DESCENDING')[:,:,:k]\n",
    "                for w in range(k):\n",
    "                  candidates += [[tf.concat((sent_pred, pred[:,:,w]) , axis=-1) , (prob + tf.math.log(dense[:,:,pred[:,:,w][0][0]])[0][0])]  ]\n",
    "        k_beams = sorted(candidates,key=lambda tup:tup[1],reverse=True)[:k]\n",
    "\n",
    "    all_sent = []\n",
    "    for i,score in k_beams:\n",
    "        sent = \"\"\n",
    "        for j in range(1,51):\n",
    "            sent +=  tk_out.index_word[i.numpy()[:,j][0]] +  \" \" \n",
    "            if tk_out.index_word[i.numpy()[:,j][0]] ==\"<end>\":\n",
    "                break\n",
    "        all_sent.append((sent.strip(),score.numpy()))\n",
    "    return all_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cecb99d-4c3d-4654-98a7-375568a87d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [27:19,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BELU Score =  0.8733800699625884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION BELU SCORE\n",
    "BLEU_beam = []\n",
    "index = []\n",
    "np.random.seed(1)\n",
    "test_data = df_val.loc[np.random.choice(df_val.index,size = 2000,replace=False)]\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        pred = beam_search(str(i.enc_input),model,3)[0][0].split()\n",
    "        act = [str(i.dec_output).split()]\n",
    "        b =bleu.sentence_bleu(act,pred)\n",
    "        BLEU_beam.append(b)\n",
    "    except:\n",
    "        index.append(ind)\n",
    "        continue\n",
    "\n",
    "print(\"BELU Score = \",np.mean(BLEU_beam)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e819843-590b-49b8-bd21-7811934b4a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  वह जमाना गया जब धन कमाने में काफी समय लगता थे और कड़ी मेहनत करनी पड़ती थी ।\n",
      "==================================================\n",
      "ACTUAL OUTPUT ===>  वह जमाना गया जब धन कमाने में काफी समय लगता था और कड़ी मेहनत करनी पड़ती थी ।  <end>\n",
      "==================================================\n",
      "BEAM SEARCH OUTPUT ,  SCORE\n",
      "('वह जमाना गया जब धन कमाने में काफी समय लगता था और कड़ी मेहनत करनी पड़ती थी । <end>', -0.02508776)\n",
      "('वह जमाना था जब धन कमाने में काफी समय लगता था और कड़ी मेहनत करनी पड़ती थी । <end>', -5.4596124)\n",
      "('वह जमाना गया था धन कमाने में काफी समय लगता था और कड़ी मेहनत करनी पड़ती थी । <end>', -5.986592)\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[19])\n",
    "print(\"=\"*50)\n",
    "print(\"ACTUAL OUTPUT ===> \",df_test.dec_output.values[19])\n",
    "print(\"=\"*50)\n",
    "print(\"BEAM SEARCH OUTPUT ,  SCORE\")\n",
    "bm = (beam_search(df_test.enc_input.values[19],model,3))\n",
    "for i in bm:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40f7623c-e7fd-4d7b-92f5-08f033cf21ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  नारों की आवाज मुखिया को ठीक उसी तरह खीच लाई जैसे बीन की धुन सांप को बाहर खीच लाती हैं ।\n",
      "==================================================\n",
      "ACTUAL OUTPUT ===>  नारों की आवाज मुखिया को ठीक उसी तरह खीच लाई जैसे बीन की धुन सांप को बाहर खीच लाती है ।  <end>\n",
      "==================================================\n",
      "BEAM SEARCH OUTPUT ,  SCORE\n",
      "('नारों की आवाज मुखिया को ठीक उसी तरह खीच लाई जैसे बीन की धुन सांप को बाहर खीच लाती है । <end>', -0.090010874)\n",
      "('नारों की आवाज मुखिया को ठीक उसी तरह खीच लाई जैसे आप की धुन सांप को बाहर खीच लाती है । <end>', -5.01867)\n",
      "('नारों की आवाज मुखिया को ठीक उसी तरह खीच लाई जैसे सब बीन की धुन सांप को बाहर खीच लाती है । <end>', -5.509224)\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[50])\n",
    "print(\"=\"*50)\n",
    "print(\"ACTUAL OUTPUT ===> \",df_test.dec_output.values[50])\n",
    "print(\"=\"*50)\n",
    "print(\"BEAM SEARCH OUTPUT ,  SCORE\")\n",
    "bm = (beam_search(df_test.enc_input.values[50],model,3))\n",
    "for i in bm:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50766232-d01d-4c1c-995a-e73c0f96b112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  अमेरिका भी इससे अलग नहीं हैं ।\n",
      "==================================================\n",
      "ACTUAL OUTPUT ===>  अमेरिका भी इससे अलग नहीं है ।  <end>\n",
      "==================================================\n",
      "BEAM SEARCH OUTPUT ,  SCORE\n",
      "('अमेरिका भी इससे अलग नहीं है । <end>', -0.001691622)\n",
      "('अमेरिका को इससे अलग नहीं है । <end>', -7.1688113)\n",
      "('अमेरिका भी इससे अलग नहीं हैं । <end>', -7.655535)\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[500])\n",
    "print(\"=\"*50)\n",
    "print(\"ACTUAL OUTPUT ===> \",df_test.dec_output.values[500])\n",
    "print(\"=\"*50)\n",
    "print(\"BEAM SEARCH OUTPUT ,  SCORE\")\n",
    "bm = (beam_search(df_test.enc_input.values[500],model,3))\n",
    "for i in bm:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7344a35e-db59-4400-a7a2-ae5fa6be903c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  इसलिए वैज्ञानिकों ने लाइकोपीन आधारिक एक दवाई बनाई हैं ।\n",
      "==================================================\n",
      "ACTUAL OUTPUT ===>  इसलिए वैज्ञानिकों ने लाइकोपीन आधारिक एक दवाई बनाई है ।  <end>\n",
      "==================================================\n",
      "BEAM SEARCH OUTPUT ,  SCORE\n",
      "('इसलिए वैज्ञानिकों ने लाइकोपीन आधारिक एक दवाई बनाई है । <end>', -0.002123482)\n",
      "('इसलिए वैज्ञानिकों ने लाइकोपीन आधारिक एक खुल बनाई है । <end>', -9.93136)\n",
      "('इसलिए वैज्ञानिकों ने लाइकोपीन तबाही एक दवाई बनाई है । <end>', -10.147559)\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",df_test.enc_input.values[900])\n",
    "print(\"=\"*50)\n",
    "print(\"ACTUAL OUTPUT ===> \",df_test.dec_output.values[900])\n",
    "print(\"=\"*50)\n",
    "print(\"BEAM SEARCH OUTPUT ,  SCORE\")\n",
    "bm = (beam_search(df_test.enc_input.values[900],model,3))\n",
    "for i in bm:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e0224754-a841-48a8-8744-c098ef52abb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "      <th>dec_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरार...</td>\n",
       "      <td>इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरार...</td>\n",
       "      <td>इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरार...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>यह मन को काबू में करने वाली मुद्रा हैं इसीलिए ...</td>\n",
       "      <td>यह मन को काबू में करने वाली मुद्रा है इसीलिए इ...</td>\n",
       "      <td>यह मन को काबू में करने वाली मुद्रा है इसीलिए इ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा उसनेाँ...</td>\n",
       "      <td>आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा वहाँ प...</td>\n",
       "      <td>आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा वहाँ प...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>परिवार के मुताबिक धमाकों में हिस्सा लेने वाले ...</td>\n",
       "      <td>परिवार के मुताबिक धमाकों में हिस्सा लेने वाले ...</td>\n",
       "      <td>परिवार के मुताबिक धमाकों में हिस्सा लेने वाले ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>उनकी वो वाली बात भी अनिश्चित रहती हैं ।</td>\n",
       "      <td>उनकी वो वाली बात भी अनिश्चित रहती है ।</td>\n",
       "      <td>उनकी वो वाली बात भी अनिश्चित रहती है ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>पालिश घर्षक मशीनें करती हैंं ।</td>\n",
       "      <td>पालिश घर्षक मशीनें करती हैं ।</td>\n",
       "      <td>पालिश घर्षक मशीनें करती हैं ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>पहले पाकिस्तान ने हमले में भारत का हाथ बताया औ...</td>\n",
       "      <td>पहले पाकिस्तान ने हमले में भारत का हाथ बताया औ...</td>\n",
       "      <td>पहले पाकिस्तान ने हमले में भारत का हाथ बताया औ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>सारे का सारा दर्द उन्हों अप ह्रदय में ही समेट ...</td>\n",
       "      <td>सारे का सारा दर्द उन्होंने अपने ह्रदय में ही स...</td>\n",
       "      <td>सारे का सारा दर्द उन्होंने अपने ह्रदय में ही स...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>विल्सन ने लिखे पत्र में रिपब्लिकन सांसदों से भ...</td>\n",
       "      <td>विल्सन ने लिखे पत्र में रिपब्लिकन सांसदों से भ...</td>\n",
       "      <td>विल्सन ने लिखे पत्र में रिपब्लिकन सांसदों से भ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>और ये सिलसिला चलता रहता हैं ।</td>\n",
       "      <td>और ये सिलसिला चलता रहता है ।</td>\n",
       "      <td>और ये सिलसिला चलता रहता है ।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               enc_input  \\\n",
       "0      इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरार...   \n",
       "1      यह मन को काबू में करने वाली मुद्रा हैं इसीलिए ...   \n",
       "2      आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा उसनेाँ...   \n",
       "3      परिवार के मुताबिक धमाकों में हिस्सा लेने वाले ...   \n",
       "4                उनकी वो वाली बात भी अनिश्चित रहती हैं ।   \n",
       "...                                                  ...   \n",
       "29995                     पालिश घर्षक मशीनें करती हैंं ।   \n",
       "29996  पहले पाकिस्तान ने हमले में भारत का हाथ बताया औ...   \n",
       "29997  सारे का सारा दर्द उन्हों अप ह्रदय में ही समेट ...   \n",
       "29998  विल्सन ने लिखे पत्र में रिपब्लिकन सांसदों से भ...   \n",
       "29999                      और ये सिलसिला चलता रहता हैं ।   \n",
       "\n",
       "                                               dec_input  \\\n",
       "0      इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरार...   \n",
       "1      यह मन को काबू में करने वाली मुद्रा है इसीलिए इ...   \n",
       "2      आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा वहाँ प...   \n",
       "3      परिवार के मुताबिक धमाकों में हिस्सा लेने वाले ...   \n",
       "4                उनकी वो वाली बात भी अनिश्चित रहती है ।    \n",
       "...                                                  ...   \n",
       "29995                     पालिश घर्षक मशीनें करती हैं ।    \n",
       "29996  पहले पाकिस्तान ने हमले में भारत का हाथ बताया औ...   \n",
       "29997  सारे का सारा दर्द उन्होंने अपने ह्रदय में ही स...   \n",
       "29998  विल्सन ने लिखे पत्र में रिपब्लिकन सांसदों से भ...   \n",
       "29999                      और ये सिलसिला चलता रहता है ।    \n",
       "\n",
       "                                              dec_output  \n",
       "0      इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरार...  \n",
       "1      यह मन को काबू में करने वाली मुद्रा है इसीलिए इ...  \n",
       "2      आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा वहाँ प...  \n",
       "3      परिवार के मुताबिक धमाकों में हिस्सा लेने वाले ...  \n",
       "4                उनकी वो वाली बात भी अनिश्चित रहती है ।   \n",
       "...                                                  ...  \n",
       "29995                     पालिश घर्षक मशीनें करती हैं ।   \n",
       "29996  पहले पाकिस्तान ने हमले में भारत का हाथ बताया औ...  \n",
       "29997  सारे का सारा दर्द उन्होंने अपने ह्रदय में ही स...  \n",
       "29998  विल्सन ने लिखे पत्र में रिपब्लिकन सांसदों से भ...  \n",
       "29999                      और ये सिलसिला चलता रहता है ।   \n",
       "\n",
       "[30000 rows x 3 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On test dataset\n",
    "df_test1= pd.read_csv(\"DATA/etoori_test.csv\")\n",
    "df_test1.columns = [\"enc_input\",\"dec_input\"] \n",
    "df_test1[\"dec_output\"] = df_test1.dec_input\n",
    "print(df_test1.shape)\n",
    "df_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e6c7a6d8-c5a0-4039-b88f-a5ad1e37a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1 = df_test1.head(10000)\n",
    "df_test1[\"dec_input\"]= \"<start> \" + df_test1[\"dec_input\"]\n",
    "df_test1[\"dec_output\"] =  df_test1[\"dec_output\"] + \" <end>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1883eb-788d-48ba-944d-5037f95a46a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ = df_test1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3d7535b1-57fc-4975-973b-77870ffa138e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [23:54,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BELU Score =  0.7663164673918077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "BLEU_val_emb = []\n",
    "test_data = df_test1\n",
    "print(test_data.shape)\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = predict(str(i.enc_input),model)[0].split()\n",
    "        act = [str(i.dec_output).split()]\n",
    "        b =bleu.sentence_bleu(act,pred)\n",
    "        #print(\"BELU Score\",b)\n",
    "        BLEU_val_emb.append(b)\n",
    "    except:\n",
    "        continue\n",
    "print(\"BELU Score = \",np.mean(BLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5e0e0-1f8b-41a1-8282-7d3f23a33c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be0aba-a2e1-471d-a64b-92253101cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on 30k data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "44aa680b-7a92-4cb6-97a7-3c14d299625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 3)\n"
     ]
    }
   ],
   "source": [
    "# On test dataset\n",
    "df_test1= pd.read_csv(\"DATA/etoori_test.csv\")\n",
    "df_test1.columns = [\"enc_input\",\"dec_input\"] \n",
    "df_test1[\"dec_output\"] = df_test1.dec_input\n",
    "print(df_test1.shape)\n",
    "df_test1[\"dec_input\"]= \"<start> \" + df_test1[\"dec_input\"]\n",
    "df_test1[\"dec_output\"] =  df_test1[\"dec_output\"] + \" <end>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d1db6-d9e5-488e-b02f-3d9714bfce23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c81c49e-a55e-44b1-8bcc-1fc395f51618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8155it [18:58,  7.15it/s]"
     ]
    }
   ],
   "source": [
    "BLEU_val_emb = []\n",
    "test_data = df_test1\n",
    "print(test_data.shape)\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        #pred = predict(str(i.enc_input),model).split()\n",
    "        pred = predict(str(i.enc_input),model)[0].split()\n",
    "        act = [str(i.dec_output).split()]\n",
    "        b =bleu.sentence_bleu(act,pred)\n",
    "        BLEU_val_emb.append(b)\n",
    "    except:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22087ba5-87db-4786-8281-fe6bd5ccb4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BELU Score for 30k data points= \",np.mean(BLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d178c8c-0e1a-4c6a-89be-3d83113db557",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BELU Score= \",np.mean(BLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb1f6e-ea55-4ac9-a7c5-0f8c55171c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4712aa5c-6303-4abb-bd5b-7c1d5a395b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  राजनैतिक गलियारों में इन दिनों यह चर्चा गर्म है कि सांगार क्षेत्र से पिछले पांच सालों से विधायक की दावेदारी का खुद का मजबूत दावेदार समझ वाले एक ता की दावेदारी बेहद फीकी है ।\n",
      "PREDICTED SENTENCE ===>  सालों की दावेदारी का खुद का मजबूत दावेदार समझ वाले एक नेता की दावेदारी बेहद फीकी हैं । । <end>\n",
      "ACTUAL SENTENCE ===>  राजनैतिक गलियारों में इन दिनों यह चर्चा गर्म है कि सांगानेर क्षेत्र से पिछले पांच सालों से विधायक की दावेदारी का खुद का मजबूत दावेदार समझने वाले एक नेता की दावेदारी बेहद फीकी है ।  <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",test_data.enc_input.values[10])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(test_data.enc_input.values[10],model)[0])\n",
    "print(\"ACTUAL SENTENCE ===> \",test_data.dec_output.values[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9bcb5c18-97ba-4358-92ab-ba627841646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  यह मन को काबू में करने वाली मुद्रा हैं इसीलिए इसे चित्त हस्त मुद्रा योग कहते हैंं ।\n",
      "PREDICTED SENTENCE ===>  यह मन को काबू में करने वाली मुद्रा है इसीलिए इसे चित्त हस्त मुद्रा योग कहते हैं । <end>\n",
      "ACTUAL SENTENCE ===>  यह मन को काबू में करने वाली मुद्रा है इसीलिए इसे चित्त हस्त मुद्रा योग कहते हैं ।  <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",test_data.enc_input.values[1])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(test_data.enc_input.values[1],model)[0])\n",
    "print(\"ACTUAL SENTENCE ===> \",test_data.dec_output.values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "566429c6-8001-4188-82e0-c0fcfdba5e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  शोधकर्ताओं परिकल्पना हैं कि इस\n",
      "PREDICTED SENTENCE ===>  शोधकर्ताओं ने परिकल्पना है कि इस <end>\n",
      "ACTUAL SENTENCE ===>  शोधकर्ताओं परिकल्पना है कि इस <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",test_data.enc_input.values[100])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(test_data.enc_input.values[100],model)[0])\n",
    "print(\"ACTUAL SENTENCE ===> \",test_data.dec_output.values[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "642704f1-7472-42c6-83d1-e02a851c58f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  दूसरे एफडीआया संसद में भी पारित हो गया है ।\n",
      "PREDICTED SENTENCE ===>  दूसरे एफडीआई संसद में भी पारित हो गया है । <end>\n",
      "ACTUAL SENTENCE ===>  दूसरे एफडीआई संसद में भी पारित हो गया है ।  <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",test_data.enc_input.values[200])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(test_data.enc_input.values[200],model)[0])\n",
    "print(\"ACTUAL SENTENCE ===> \",test_data.dec_output.values[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "892e3a63-3bb6-49d7-9758-6654ef738997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  क्योंकि एडवीज एंटोनिया अलवीना माइनो उर्फ सोनिया गांधी का धन स्विस बैंक में भरा पड़ा है इसीलिए सोनिया गांधी के गुलाम प्रधानमन्त्री मनमोहन सिंह जर्मनी द्वारा सविस बैंक में काला धन रख वालों की सूची आफर कर के बाबजूद सूची ले से मना कर दिया था ।\n",
      "PREDICTED SENTENCE ===>  बैंक ने धन रखने वालों की सूची आफर करने के बाबजूद सूची लेने के बाबजूद सूची ले से मना कर\n",
      "ACTUAL SENTENCE ===>  क्योंकि एडवीज एंटोनिया अलवीना माइनो उर्फ सोनिया गांधी का धन स्विस बैंक में भरा पड़ा है इसीलिए सोनिया गांधी के गुलाम प्रधानमन्त्री मनमोहन सिंह ने जर्मनी द्वारा सविस बैंक में काला धन रखने वालों की सूची आफर करने के बाबजूद सूची लेने से मना कर दिया था ।  <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",test_data.enc_input.values[500])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(test_data.enc_input.values[500],model)[0])\n",
    "print(\"ACTUAL SENTENCE ===> \",test_data.dec_output.values[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "991f8490-41d9-4ce9-9d34-39a21b68fd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  इस बात का खुलासा होने के बाद नक्सलियों के स्याह चेहरे का एक और बदनुमा रंग सामने आ गया हैं ।\n",
      "PREDICTED SENTENCE ===>  बात का खुलासा होने के बाद नक्सलियों के स्याह चेहरे का एक और बदनुमा रंग सामने आ गया है ।\n",
      "ACTUAL SENTENCE ===>  इस बात का खुलासा होने के बाद नक्सलियों के स्याह चेहरे का एक और बदनुमा रंग सामने आ गया है ।  <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",test_data.enc_input.values[950])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(test_data.enc_input.values[950],model)[0])\n",
    "print(\"ACTUAL SENTENCE ===> \",test_data.dec_output.values[950])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be7df953-0715-41d9-8726-d83c52aaede0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SENTENCE ===>  हां कहीं यह बहुत खुलकर साम आता है और कहीं अंदर ही अंदर पलता रहता है ।\n",
      "PREDICTED SENTENCE ===>  हां कहीं यह बहुत खुलकर सामने आता है और कहीं अंदर ही अंदर पलता रहता है । <end>\n",
      "ACTUAL SENTENCE ===>  हां कहीं यह बहुत खुलकर सामने आता है और कहीं अंदर ही अंदर पलता रहता है ।  <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT SENTENCE ===> \",test_data.enc_input.values[800])\n",
    "print(\"PREDICTED SENTENCE ===> \",predict(test_data.enc_input.values[800],model)[0])\n",
    "print(\"ACTUAL SENTENCE ===> \",test_data.dec_output.values[800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4750b75-2dd4-4f4e-947c-f8c4c713c5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb1123-895b-418c-b5ee-c550df76d526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
