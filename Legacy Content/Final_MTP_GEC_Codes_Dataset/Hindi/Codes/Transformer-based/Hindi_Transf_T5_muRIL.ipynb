{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9c7780-28f4-4676-8620-cfeffba88c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar  1 14:20:23 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.84       Driver Version: 460.84       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 20%   21C    P8     8W / 250W |    447MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 20%   21C    P8     6W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:07:00.0 Off |                  N/A |\n",
      "| 20%   43C    P2    56W / 250W |  10902MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "| 20%   24C    P8     8W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce GTX 108...  Off  | 00000000:0C:00.0 Off |                  N/A |\n",
      "| 20%   38C    P8     8W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce GTX 108...  Off  | 00000000:0D:00.0 Off |                  N/A |\n",
      "| 20%   22C    P8     7W / 250W |    795MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce GTX 108...  Off  | 00000000:0E:00.0 Off |                  N/A |\n",
      "| 53%   82C    P2   208W / 250W |   6609MiB / 11178MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce GTX 108...  Off  | 00000000:0F:00.0 Off |                  N/A |\n",
      "| 32%   64C    P2    79W / 250W |   3183MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     38504      C   /opt/conda/bin/python             445MiB |\n",
      "|    2   N/A  N/A     14515      C   python3                          2919MiB |\n",
      "|    2   N/A  N/A     30017      C   /usr/bin/python3                 7981MiB |\n",
      "|    5   N/A  N/A     32689      C   /usr/bin/python3                  793MiB |\n",
      "|    6   N/A  N/A     40050      C   python3                          6607MiB |\n",
      "|    7   N/A  N/A      9775      C   python3                          3181MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1472674-e209-489e-93b9-3d3c8da03a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, \"0\" to  \"7\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "#torch.cuda.set_device(0)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(n_gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba40e0c-5fcc-43a9-8d28-47ff9e9870f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "340acc79-3319-47aa-8140-07059948a04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /DATA/gupta92/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "#Set a seed\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7712b672-2f06-47dd-906a-42ab51326a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc22694-eff1-4618-ad9c-8b6110972cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4aaf87c-52e6-4e3d-870c-1955b9afd376",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e4420cd-d086-467d-8384-bc29bb4d91ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('DATA/etoori_train.csv')\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc84c40-1152-4f97-b749-186365fff1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'enc_input':'input'}, inplace = True)\n",
    "df.rename(columns = {'dec_input':'output'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a0a74f8-b795-47a1-aa0a-e3a293a0165b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡§™‡§∞‡§®‡•ç‡§§‡•Å ‡§µ‡•á ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§â‡§® ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§ú‡§º‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§∏‡§Æ‡§Ø ‡§§‡§ï ‡§Ö‡§™ ‡§π‡•É‡§¶‡§Ø ‡§Æ‡•á‡§Ç ‡§ó‡§æ‡§Å‡§† ‡§¨‡§®‡§æ‡§ï‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§ñ‡§§‡•á ‡§•‡•á ‡•§</td>\n",
       "      <td>‡§™‡§∞‡§®‡•ç‡§§‡•Å ‡§µ‡•á ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§â‡§® ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§ú‡§º‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§∏‡§Æ‡§Ø ‡§§‡§ï ‡§Ö‡§™‡§®‡•á ‡§π‡•É‡§¶‡§Ø ‡§Æ‡•á‡§Ç ‡§ó‡§æ‡§Å‡§† ‡§¨‡§®‡§æ‡§ï‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§ñ‡§§‡•á ‡§•‡•á ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ï‡•ã ‡§µ‡§ø‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞ ‡§ï‡§æ ‡§∑‡§°‡§º‡§Ø‡§Ç‡§§‡•ç‡§∞ ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à ‡§ú‡•ã ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§ï‡§æ ‡§µ‡§ø‡§∑‡§Ø ‡§π‡•à ‡•§</td>\n",
       "      <td>‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ï‡•ã ‡§µ‡§ø‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ ‡§∑‡§°‡§º‡§Ø‡§Ç‡§§‡•ç‡§∞ ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à ‡§ú‡•ã ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§ï‡§æ ‡§µ‡§ø‡§∑‡§Ø ‡§π‡•à ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡§§‡•Ä‡§® ‡§∏‡§æ‡§≤ ‡§™‡§π‡§≤‡•á ‡§ï‡§æ‡§§‡§ø‡§≤‡§æ‡§®‡§æ ‡§π‡§Æ‡§≤‡•á ‡§ï‡•á ‡§™‡•ç‡§∞‡§ï‡§∞‡§£ ‡§Æ‡•á‡§Ç ‡§è‡§´‡§Ü‡§∞ ‡§≤‡§ó ‡§ú‡§æ ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§°‡•á‡§∞‡§æ ‡§™‡•ç‡§∞‡•á‡§Æ‡•Ä ‡§á‡§∏ ‡§¨‡§æ‡§∞ ‡§¶‡§∞‡•ç‡§ú ‡§π‡•Å‡§è ‡§Æ‡•Å‡§ï‡§¶‡§Æ‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§≠‡•Ä ‡§Ö‡§≠‡§ø‡§Ø‡•Å‡§ï‡•ç‡§§‡•ã‡§Ç ‡§ï‡•Ä ‡§ó‡§ø‡§∞‡§´‡•ç‡§§‡§æ‡§∞‡•Ä ‡§ï‡•Ä ‡§Æ‡§æ‡§Ç‡§ó ‡§ï‡•ã ‡§≤‡•á‡§ï‡§∞ ‡§Ü‡§Ç‡§¶‡•ã‡§≤‡§®‡§∞‡§§ ‡§π‡•à‡§Ç ‡•§</td>\n",
       "      <td>‡§§‡•Ä‡§® ‡§∏‡§æ‡§≤ ‡§™‡§π‡§≤‡•á ‡§ï‡§æ‡§§‡§ø‡§≤‡§æ‡§®‡§æ ‡§π‡§Æ‡§≤‡•á ‡§ï‡•á ‡§™‡•ç‡§∞‡§ï‡§∞‡§£ ‡§Æ‡•á‡§Ç ‡§è‡§´‡§Ü‡§∞ ‡§≤‡§ó ‡§ú‡§æ‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§°‡•á‡§∞‡§æ ‡§™‡•ç‡§∞‡•á‡§Æ‡•Ä ‡§á‡§∏ ‡§¨‡§æ‡§∞ ‡§¶‡§∞‡•ç‡§ú ‡§π‡•Å‡§è ‡§Æ‡•Å‡§ï‡§¶‡§Æ‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§≠‡•Ä ‡§Ö‡§≠‡§ø‡§Ø‡•Å‡§ï‡•ç‡§§‡•ã‡§Ç ‡§ï‡•Ä ‡§ó‡§ø‡§∞‡§´‡•ç‡§§‡§æ‡§∞‡•Ä ‡§ï‡•Ä ‡§Æ‡§æ‡§Ç‡§ó ‡§ï‡•ã ‡§≤‡•á‡§ï‡§∞ ‡§Ü‡§Ç‡§¶‡•ã‡§≤‡§®‡§∞‡§§ ‡§π‡•à‡§Ç ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡§∞‡§æ‡§Æ‡§æ‡§Ø‡§£ ‡§∞‡§ø‡§µ‡§ø‡§ú‡§ø‡§ü‡•á‡§° ‡§Ö ‡§ü‡•á‡§≤ ‡§ë‡§´ ‡§≤‡§µ ‡§è‡§Ç‡§° ‡§è‡§°‡§µ‡•á‡§Ç‡§ö‡§∞ ‡§®‡§æ‡§Æ ‡§∏‡•á ‡§∏‡§ø‡§Ç‡§ó‡§æ‡§™‡•Å‡§∞ ‡§ï‡•á ‡§™‡•á‡§∞‡§æ‡§®‡§æ‡§Ç‡§ï‡§æ ‡§Æ‡•ç‡§Ø‡•Ç‡§ú‡§ø‡§Ø‡§Æ ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä ‡§≤‡§ó‡§æ‡§à ‡§ó‡§à ‡§π‡•à‡§Ç ‡•§</td>\n",
       "      <td>‡§∞‡§æ‡§Æ‡§æ‡§Ø‡§£ ‡§∞‡§ø‡§µ‡§ø‡§ú‡§ø‡§ü‡•á‡§° ‡§Ö ‡§ü‡•á‡§≤ ‡§ë‡§´ ‡§≤‡§µ ‡§è‡§Ç‡§° ‡§è‡§°‡§µ‡•á‡§Ç‡§ö‡§∞ ‡§®‡§æ‡§Æ ‡§∏‡•á ‡§∏‡§ø‡§Ç‡§ó‡§æ‡§™‡•Å‡§∞ ‡§ï‡•á ‡§™‡•á‡§∞‡§æ‡§®‡§æ‡§Ç‡§ï‡§æ ‡§Æ‡•ç‡§Ø‡•Ç‡§ú‡§ø‡§Ø‡§Æ ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä ‡§≤‡§ó‡§æ‡§à ‡§ó‡§à ‡§π‡•à ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡§§‡§¨ ‡§§‡§ï ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§∞‡§æ‡§Æ ‡§≤‡•á ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§¶‡•Ä‡§ú‡§ø‡§è ‡•§</td>\n",
       "      <td>‡§§‡§¨ ‡§§‡§ï ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§∞‡§æ‡§Æ ‡§≤‡•á‡§®‡•á ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§¶‡•Ä‡§ú‡§ø‡§è ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>‡§¶‡•á‡§µ ‡§∏‡•ç‡§•‡•á‡§® ‡§§‡§ï ‡§∏‡§°‡§º‡§ï ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§ø‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ö‡§≤ ‡§∞‡§π‡•Ä ‡§π‡•à ‡•§</td>\n",
       "      <td>‡§¶‡•á‡§µ ‡§∏‡•ç‡§•‡§æ‡§® ‡§§‡§ï ‡§∏‡§°‡§º‡§ï ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§ø‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ö‡§≤ ‡§∞‡§π‡•Ä ‡§π‡•à ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>‡§µ‡§∞‡•ç‡§° ‡§µ‡§æ‡§ö‡§∞ ‡§µ ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§ö‡§ø‡§§‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§Ö‡§®‡•Ç‡§™ ‡§∏‡§æ‡§π ‡§Ü‡§∞‡§è‡§®‡§è‡§∏ ‡§ï‡•ã ‡§¨‡§§‡§æ‡§Ø‡§æ ‡§ï‡§ø ‡§∂‡•ç‡§µ‡•á‡§§‡§ï‡§Ç‡§† ‡§ö‡§ø‡§≤‡§ö‡§ø‡§≤ ‡§™‡§ï‡•ç‡§∑‡•Ä ‡§ï‡§æ ‡§µ‡•à‡§ú‡•ç‡§û‡§æ‡§®‡§ø‡§ï ‡§®‡§æ‡§Æ ‡§π‡•ç‡§µ‡§æ‡§á‡§ü ‡§•‡•ç‡§∞‡•ã‡§ü‡•á‡§ü ‡§≤‡§æ‡§´‡§ø‡§Ç‡§ó ‡§•‡•ç‡§∞‡§∏ ‡§π‡•à ‡•§</td>\n",
       "      <td>‡§µ‡§∞‡•ç‡§° ‡§µ‡§æ‡§ö‡§∞ ‡§µ ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§ö‡§ø‡§§‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§Ö‡§®‡•Ç‡§™ ‡§∏‡§æ‡§π ‡§®‡•á ‡§Ü‡§∞‡§è‡§®‡§è‡§∏ ‡§ï‡•ã ‡§¨‡§§‡§æ‡§Ø‡§æ ‡§ï‡§ø ‡§∂‡•ç‡§µ‡•á‡§§‡§ï‡§Ç‡§† ‡§ö‡§ø‡§≤‡§ö‡§ø‡§≤ ‡§™‡§ï‡•ç‡§∑‡•Ä ‡§ï‡§æ ‡§µ‡•à‡§ú‡•ç‡§û‡§æ‡§®‡§ø‡§ï ‡§®‡§æ‡§Æ ‡§π‡•ç‡§µ‡§æ‡§á‡§ü ‡§•‡•ç‡§∞‡•ã‡§ü‡•á‡§ü ‡§≤‡§æ‡§´‡§ø‡§Ç‡§ó ‡§•‡•ç‡§∞‡§∏ ‡§π‡•à ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‡§¶‡•ã ‡§¶‡§ø‡§® ‡§™‡§π‡§≤‡•á ‡§ú‡•ç‡§û‡§æ‡§®‡§ú‡•Ä ‡§™‡•ã‡§∏‡•ç‡§ü ‡§†‡•á‡§≤‡•Ä ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç</td>\n",
       "      <td>‡§¶‡•ã ‡§¶‡§ø‡§® ‡§™‡§π‡§≤‡•á ‡§ú‡•ç‡§û‡§æ‡§®‡§ú‡•Ä ‡§®‡•á ‡§™‡•ã‡§∏‡•ç‡§ü ‡§†‡•á‡§≤‡•Ä ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ ‡§ï‡•Ä ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§Æ‡•á‡§Ç ‡§¨‡•Ä‡§Æ‡§æ ‡§ï‡§Æ‡•ç‡§™‡§®‡•Ä ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§¶‡•Ä ‡§ú‡§æ ‡§µ‡§æ‡§≤‡•Ä ‡§∞‡§æ‡§∂‡§ø ‡§®‡§ø‡§∞‡•ç‡§ß‡§æ‡§∞‡§ø‡§§ ‡§ï‡§∞ ‡§¶‡•Ä ‡§ó‡§à ‡§π‡•à ‡•§</td>\n",
       "      <td>‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ ‡§ï‡•Ä ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§Æ‡•á‡§Ç ‡§¨‡•Ä‡§Æ‡§æ ‡§ï‡§Æ‡•ç‡§™‡§®‡•Ä ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§¶‡•Ä ‡§ú‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§∞‡§æ‡§∂‡§ø ‡§®‡§ø‡§∞‡•ç‡§ß‡§æ‡§∞‡§ø‡§§ ‡§ï‡§∞ ‡§¶‡•Ä ‡§ó‡§à ‡§π‡•à ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>‡§¶‡•á‡§∞ ‡§∏‡•á ‡§ú‡§æ‡§ä‡§Ç‡§ó‡§æ ‡§§‡•ã ‡§¶‡•Å‡§ï‡§æ‡§Ç ‡§¨‡§Ç‡§¶ ‡§π‡•ã ‡§ú‡§æ‡§è‡§Ç‡§ó‡•Ä ‡•§</td>\n",
       "      <td>‡§¶‡•á‡§∞ ‡§∏‡•á ‡§ú‡§æ‡§ä‡§Ç‡§ó‡§æ ‡§§‡•ã ‡§¶‡•Å‡§ï‡§æ‡§®‡•á‡§Ç ‡§¨‡§Ç‡§¶ ‡§π‡•ã ‡§ú‡§æ‡§è‡§Ç‡§ó‡•Ä ‡•§</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                          input  \\\n",
       "0                                                                              ‡§™‡§∞‡§®‡•ç‡§§‡•Å ‡§µ‡•á ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§â‡§® ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§ú‡§º‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§∏‡§Æ‡§Ø ‡§§‡§ï ‡§Ö‡§™ ‡§π‡•É‡§¶‡§Ø ‡§Æ‡•á‡§Ç ‡§ó‡§æ‡§Å‡§† ‡§¨‡§®‡§æ‡§ï‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§ñ‡§§‡•á ‡§•‡•á ‡•§   \n",
       "1                                                                                    ‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ï‡•ã ‡§µ‡§ø‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞ ‡§ï‡§æ ‡§∑‡§°‡§º‡§Ø‡§Ç‡§§‡•ç‡§∞ ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à ‡§ú‡•ã ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§ï‡§æ ‡§µ‡§ø‡§∑‡§Ø ‡§π‡•à ‡•§   \n",
       "2  ‡§§‡•Ä‡§® ‡§∏‡§æ‡§≤ ‡§™‡§π‡§≤‡•á ‡§ï‡§æ‡§§‡§ø‡§≤‡§æ‡§®‡§æ ‡§π‡§Æ‡§≤‡•á ‡§ï‡•á ‡§™‡•ç‡§∞‡§ï‡§∞‡§£ ‡§Æ‡•á‡§Ç ‡§è‡§´‡§Ü‡§∞ ‡§≤‡§ó ‡§ú‡§æ ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§°‡•á‡§∞‡§æ ‡§™‡•ç‡§∞‡•á‡§Æ‡•Ä ‡§á‡§∏ ‡§¨‡§æ‡§∞ ‡§¶‡§∞‡•ç‡§ú ‡§π‡•Å‡§è ‡§Æ‡•Å‡§ï‡§¶‡§Æ‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§≠‡•Ä ‡§Ö‡§≠‡§ø‡§Ø‡•Å‡§ï‡•ç‡§§‡•ã‡§Ç ‡§ï‡•Ä ‡§ó‡§ø‡§∞‡§´‡•ç‡§§‡§æ‡§∞‡•Ä ‡§ï‡•Ä ‡§Æ‡§æ‡§Ç‡§ó ‡§ï‡•ã ‡§≤‡•á‡§ï‡§∞ ‡§Ü‡§Ç‡§¶‡•ã‡§≤‡§®‡§∞‡§§ ‡§π‡•à‡§Ç ‡•§   \n",
       "3                                                    ‡§∞‡§æ‡§Æ‡§æ‡§Ø‡§£ ‡§∞‡§ø‡§µ‡§ø‡§ú‡§ø‡§ü‡•á‡§° ‡§Ö ‡§ü‡•á‡§≤ ‡§ë‡§´ ‡§≤‡§µ ‡§è‡§Ç‡§° ‡§è‡§°‡§µ‡•á‡§Ç‡§ö‡§∞ ‡§®‡§æ‡§Æ ‡§∏‡•á ‡§∏‡§ø‡§Ç‡§ó‡§æ‡§™‡•Å‡§∞ ‡§ï‡•á ‡§™‡•á‡§∞‡§æ‡§®‡§æ‡§Ç‡§ï‡§æ ‡§Æ‡•ç‡§Ø‡•Ç‡§ú‡§ø‡§Ø‡§Æ ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä ‡§≤‡§ó‡§æ‡§à ‡§ó‡§à ‡§π‡•à‡§Ç ‡•§   \n",
       "4                                                                                                                  ‡§§‡§¨ ‡§§‡§ï ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§∞‡§æ‡§Æ ‡§≤‡•á ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§¶‡•Ä‡§ú‡§ø‡§è ‡•§   \n",
       "5                                                                                                               ‡§¶‡•á‡§µ ‡§∏‡•ç‡§•‡•á‡§® ‡§§‡§ï ‡§∏‡§°‡§º‡§ï ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§ø‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ö‡§≤ ‡§∞‡§π‡•Ä ‡§π‡•à ‡•§   \n",
       "6                              ‡§µ‡§∞‡•ç‡§° ‡§µ‡§æ‡§ö‡§∞ ‡§µ ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§ö‡§ø‡§§‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§Ö‡§®‡•Ç‡§™ ‡§∏‡§æ‡§π ‡§Ü‡§∞‡§è‡§®‡§è‡§∏ ‡§ï‡•ã ‡§¨‡§§‡§æ‡§Ø‡§æ ‡§ï‡§ø ‡§∂‡•ç‡§µ‡•á‡§§‡§ï‡§Ç‡§† ‡§ö‡§ø‡§≤‡§ö‡§ø‡§≤ ‡§™‡§ï‡•ç‡§∑‡•Ä ‡§ï‡§æ ‡§µ‡•à‡§ú‡•ç‡§û‡§æ‡§®‡§ø‡§ï ‡§®‡§æ‡§Æ ‡§π‡•ç‡§µ‡§æ‡§á‡§ü ‡§•‡•ç‡§∞‡•ã‡§ü‡•á‡§ü ‡§≤‡§æ‡§´‡§ø‡§Ç‡§ó ‡§•‡•ç‡§∞‡§∏ ‡§π‡•à ‡•§   \n",
       "7                                                                                                                         ‡§¶‡•ã ‡§¶‡§ø‡§® ‡§™‡§π‡§≤‡•á ‡§ú‡•ç‡§û‡§æ‡§®‡§ú‡•Ä ‡§™‡•ã‡§∏‡•ç‡§ü ‡§†‡•á‡§≤‡•Ä ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç   \n",
       "8                                                                             ‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ ‡§ï‡•Ä ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§Æ‡•á‡§Ç ‡§¨‡•Ä‡§Æ‡§æ ‡§ï‡§Æ‡•ç‡§™‡§®‡•Ä ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§¶‡•Ä ‡§ú‡§æ ‡§µ‡§æ‡§≤‡•Ä ‡§∞‡§æ‡§∂‡§ø ‡§®‡§ø‡§∞‡•ç‡§ß‡§æ‡§∞‡§ø‡§§ ‡§ï‡§∞ ‡§¶‡•Ä ‡§ó‡§à ‡§π‡•à ‡•§   \n",
       "9                                                                                                                        ‡§¶‡•á‡§∞ ‡§∏‡•á ‡§ú‡§æ‡§ä‡§Ç‡§ó‡§æ ‡§§‡•ã ‡§¶‡•Å‡§ï‡§æ‡§Ç ‡§¨‡§Ç‡§¶ ‡§π‡•ã ‡§ú‡§æ‡§è‡§Ç‡§ó‡•Ä ‡•§   \n",
       "\n",
       "                                                                                                                                                            output  \n",
       "0                                                                              ‡§™‡§∞‡§®‡•ç‡§§‡•Å ‡§µ‡•á ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§â‡§® ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§ú‡§º‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§∏‡§Æ‡§Ø ‡§§‡§ï ‡§Ö‡§™‡§®‡•á ‡§π‡•É‡§¶‡§Ø ‡§Æ‡•á‡§Ç ‡§ó‡§æ‡§Å‡§† ‡§¨‡§®‡§æ‡§ï‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§ñ‡§§‡•á ‡§•‡•á ‡•§   \n",
       "1                                                                                    ‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ï‡•ã ‡§µ‡§ø‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ ‡§∑‡§°‡§º‡§Ø‡§Ç‡§§‡•ç‡§∞ ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à ‡§ú‡•ã ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§ï‡§æ ‡§µ‡§ø‡§∑‡§Ø ‡§π‡•à ‡•§   \n",
       "2  ‡§§‡•Ä‡§® ‡§∏‡§æ‡§≤ ‡§™‡§π‡§≤‡•á ‡§ï‡§æ‡§§‡§ø‡§≤‡§æ‡§®‡§æ ‡§π‡§Æ‡§≤‡•á ‡§ï‡•á ‡§™‡•ç‡§∞‡§ï‡§∞‡§£ ‡§Æ‡•á‡§Ç ‡§è‡§´‡§Ü‡§∞ ‡§≤‡§ó ‡§ú‡§æ‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§°‡•á‡§∞‡§æ ‡§™‡•ç‡§∞‡•á‡§Æ‡•Ä ‡§á‡§∏ ‡§¨‡§æ‡§∞ ‡§¶‡§∞‡•ç‡§ú ‡§π‡•Å‡§è ‡§Æ‡•Å‡§ï‡§¶‡§Æ‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§≠‡•Ä ‡§Ö‡§≠‡§ø‡§Ø‡•Å‡§ï‡•ç‡§§‡•ã‡§Ç ‡§ï‡•Ä ‡§ó‡§ø‡§∞‡§´‡•ç‡§§‡§æ‡§∞‡•Ä ‡§ï‡•Ä ‡§Æ‡§æ‡§Ç‡§ó ‡§ï‡•ã ‡§≤‡•á‡§ï‡§∞ ‡§Ü‡§Ç‡§¶‡•ã‡§≤‡§®‡§∞‡§§ ‡§π‡•à‡§Ç ‡•§   \n",
       "3                                                       ‡§∞‡§æ‡§Æ‡§æ‡§Ø‡§£ ‡§∞‡§ø‡§µ‡§ø‡§ú‡§ø‡§ü‡•á‡§° ‡§Ö ‡§ü‡•á‡§≤ ‡§ë‡§´ ‡§≤‡§µ ‡§è‡§Ç‡§° ‡§è‡§°‡§µ‡•á‡§Ç‡§ö‡§∞ ‡§®‡§æ‡§Æ ‡§∏‡•á ‡§∏‡§ø‡§Ç‡§ó‡§æ‡§™‡•Å‡§∞ ‡§ï‡•á ‡§™‡•á‡§∞‡§æ‡§®‡§æ‡§Ç‡§ï‡§æ ‡§Æ‡•ç‡§Ø‡•Ç‡§ú‡§ø‡§Ø‡§Æ ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä ‡§≤‡§ó‡§æ‡§à ‡§ó‡§à ‡§π‡•à ‡•§   \n",
       "4                                                                                                                  ‡§§‡§¨ ‡§§‡§ï ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§∞‡§æ‡§Æ ‡§≤‡•á‡§®‡•á ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§¶‡•Ä‡§ú‡§ø‡§è ‡•§   \n",
       "5                                                                                                                 ‡§¶‡•á‡§µ ‡§∏‡•ç‡§•‡§æ‡§® ‡§§‡§ï ‡§∏‡§°‡§º‡§ï ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§ø‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ö‡§≤ ‡§∞‡§π‡•Ä ‡§π‡•à ‡•§   \n",
       "6                             ‡§µ‡§∞‡•ç‡§° ‡§µ‡§æ‡§ö‡§∞ ‡§µ ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§ö‡§ø‡§§‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§Ö‡§®‡•Ç‡§™ ‡§∏‡§æ‡§π ‡§®‡•á ‡§Ü‡§∞‡§è‡§®‡§è‡§∏ ‡§ï‡•ã ‡§¨‡§§‡§æ‡§Ø‡§æ ‡§ï‡§ø ‡§∂‡•ç‡§µ‡•á‡§§‡§ï‡§Ç‡§† ‡§ö‡§ø‡§≤‡§ö‡§ø‡§≤ ‡§™‡§ï‡•ç‡§∑‡•Ä ‡§ï‡§æ ‡§µ‡•à‡§ú‡•ç‡§û‡§æ‡§®‡§ø‡§ï ‡§®‡§æ‡§Æ ‡§π‡•ç‡§µ‡§æ‡§á‡§ü ‡§•‡•ç‡§∞‡•ã‡§ü‡•á‡§ü ‡§≤‡§æ‡§´‡§ø‡§Ç‡§ó ‡§•‡•ç‡§∞‡§∏ ‡§π‡•à ‡•§   \n",
       "7                                                                                                                         ‡§¶‡•ã ‡§¶‡§ø‡§® ‡§™‡§π‡§≤‡•á ‡§ú‡•ç‡§û‡§æ‡§®‡§ú‡•Ä ‡§®‡•á ‡§™‡•ã‡§∏‡•ç‡§ü ‡§†‡•á‡§≤‡•Ä ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç  \n",
       "8                                                                             ‡§¶‡•Å‡§∞‡•ç‡§ò‡§ü‡§®‡§æ ‡§ï‡•Ä ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§Æ‡•á‡§Ç ‡§¨‡•Ä‡§Æ‡§æ ‡§ï‡§Æ‡•ç‡§™‡§®‡•Ä ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§¶‡•Ä ‡§ú‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§∞‡§æ‡§∂‡§ø ‡§®‡§ø‡§∞‡•ç‡§ß‡§æ‡§∞‡§ø‡§§ ‡§ï‡§∞ ‡§¶‡•Ä ‡§ó‡§à ‡§π‡•à ‡•§   \n",
       "9                                                                                                                        ‡§¶‡•á‡§∞ ‡§∏‡•á ‡§ú‡§æ‡§ä‡§Ç‡§ó‡§æ ‡§§‡•ã ‡§¶‡•Å‡§ï‡§æ‡§®‡•á‡§Ç ‡§¨‡§Ç‡§¶ ‡§π‡•ã ‡§ú‡§æ‡§è‡§Ç‡§ó‡•Ä ‡•§   "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81e7ec2-d0fa-45d6-be30-7c2e2877dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration, T5Tokenizer, \n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "  )\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c509b356-7320-4dbb-9184-9750dd208a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fc016f6-2311-4978-a799-f40e96abaef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='google/muril-base-cased', vocab_size=197285, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbc19d2f-5c42-46ea-8c21-decc47553c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a081dd-fbb1-4b1d-a762-4d99ecc0ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Initialize with t5-base, then have trained and saved t5_gec_hindi_muRIL_1, t5_gec_hindi_muRIL_2, t5_gec_hindi_muRIL_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b199d7a2-2b5d-4904-b204-a379c34aefca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "#model_name = 't5-base'\n",
    "#tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "#model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "model_name = 't5_gec_hindi_muRIL_3'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca44d0-39fd-48a8-a0a2-466fd4ce1d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aca04df4-d7de-4f44-9800-94eb1c5296ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_token_len(example):\n",
    "    return len(tokenizer(example).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9821b86-5a8b-4956-856c-d4c88d482f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((126000, 2), (14000, 2))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train - Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.10, shuffle=True)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c514352d-1338-4a42-95a6-36151e6b0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['input_token_len'] = test_df['input'].apply(calc_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9df2945-fb4d-4f63-b24b-2571dceba51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>input_token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40665</th>\n",
       "      <td>‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡•Ä ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§</td>\n",
       "      <td>‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡§æ ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48520</th>\n",
       "      <td>‡§§‡§•‡•á ‡§ó‡•å‡§∂‡§æ‡§≤‡§æ ‡§ï‡§æ</td>\n",
       "      <td>‡§§‡§•‡§æ ‡§ó‡•å‡§∂‡§æ‡§≤‡§æ ‡§ï‡§æ</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138403</th>\n",
       "      <td>‡§¨‡§ö‡•ç‡§ö‡•á ‡§ñ‡•á‡§≤ ‡§∞‡§π‡•á ‡§π‡•à ‡§ï‡§ø‡§≤‡§ï‡§æ‡§∞‡§ø‡§Ø‡§æ‡§Å ‡§ó‡•Ç‡§Ç‡§ú ‡§∞‡§π‡•Ä ‡§π‡•à ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§æ‡§£‡•ç‡§° ‡§Æ‡•á‡§Ç</td>\n",
       "      <td>‡§¨‡§ö‡•ç‡§ö‡•á ‡§ñ‡•á‡§≤ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§ï‡§ø‡§≤‡§ï‡§æ‡§∞‡§ø‡§Ø‡§æ‡§Å ‡§ó‡•Ç‡§Ç‡§ú ‡§∞‡§π‡•Ä ‡§π‡•à‡§Ç ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§æ‡§£‡•ç‡§° ‡§Æ‡•á‡§Ç</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130079</th>\n",
       "      <td>‡§ß‡§∞‡§®‡•á ‡§™‡§∞ ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø ‡§Ö‡§¨ ‡§∏‡§Æ‡§Ø ‡§Ü ‡§ó‡§Ø‡§æ ‡§π‡•à‡§Ç ‡§ï‡§ø ‡§ó‡§®‡•ç‡§®‡•á ‡§ï‡•Ä ‡§ñ‡•á‡§§‡•Ä ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§ï‡•á ‡§≤‡§ø‡§Ø‡•á ‡§®‡•Å‡§ï‡§∏‡§æ‡§® ‡§ï‡§æ ‡§∏‡•å‡§¶‡§æ ‡§π‡•ã ‡§ó‡§Ø‡•Ä ‡§π‡•à‡§Ç ‡•§</td>\n",
       "      <td>‡§ß‡§∞‡§®‡•á ‡§™‡§∞ ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø ‡§Ö‡§¨ ‡§∏‡§Æ‡§Ø ‡§Ü ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ó‡§®‡•ç‡§®‡•á ‡§ï‡•Ä ‡§ñ‡•á‡§§‡•Ä ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§ï‡•á ‡§≤‡§ø‡§Ø‡•á ‡§®‡•Å‡§ï‡§∏‡§æ‡§® ‡§ï‡§æ ‡§∏‡•å‡§¶‡§æ ‡§π‡•ã ‡§ó‡§Ø‡•Ä ‡§π‡•à ‡•§</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50146</th>\n",
       "      <td>‡§∏‡§°‡§º‡§ï ‡§Æ‡§≤‡§¨‡•á ‡§ï‡§æ ‡§¢‡•á‡§∞ ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§¶‡•Å‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ ‡§ö‡§æ‡§∞ ‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ‡§æ‡§π‡§® ‡§ö‡§æ‡§≤‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§∞‡•á‡§∂‡§æ‡§® ‡§π‡•ã‡§®‡§æ ‡§™‡§°‡§º ‡§∞‡§π‡§æ ‡§π‡•à‡§Ç ‡•§</td>\n",
       "      <td>‡§∏‡§°‡§º‡§ï ‡§Æ‡§≤‡§¨‡•á ‡§ï‡§æ ‡§¢‡•á‡§∞ ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§¶‡•Å‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ ‡§ö‡§æ‡§∞ ‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ‡§æ‡§π‡§® ‡§ö‡§æ‡§≤‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§∞‡•á‡§∂‡§æ‡§® ‡§π‡•ã‡§®‡§æ ‡§™‡§°‡§º ‡§∞‡§π‡§æ ‡§π‡•à ‡•§</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        input  \\\n",
       "40665         ‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡•Ä ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§   \n",
       "48520                                                                                           ‡§§‡§•‡•á ‡§ó‡•å‡§∂‡§æ‡§≤‡§æ ‡§ï‡§æ   \n",
       "138403                                                 ‡§¨‡§ö‡•ç‡§ö‡•á ‡§ñ‡•á‡§≤ ‡§∞‡§π‡•á ‡§π‡•à ‡§ï‡§ø‡§≤‡§ï‡§æ‡§∞‡§ø‡§Ø‡§æ‡§Å ‡§ó‡•Ç‡§Ç‡§ú ‡§∞‡§π‡•Ä ‡§π‡•à ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§æ‡§£‡•ç‡§° ‡§Æ‡•á‡§Ç   \n",
       "130079  ‡§ß‡§∞‡§®‡•á ‡§™‡§∞ ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø ‡§Ö‡§¨ ‡§∏‡§Æ‡§Ø ‡§Ü ‡§ó‡§Ø‡§æ ‡§π‡•à‡§Ç ‡§ï‡§ø ‡§ó‡§®‡•ç‡§®‡•á ‡§ï‡•Ä ‡§ñ‡•á‡§§‡•Ä ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§ï‡•á ‡§≤‡§ø‡§Ø‡•á ‡§®‡•Å‡§ï‡§∏‡§æ‡§® ‡§ï‡§æ ‡§∏‡•å‡§¶‡§æ ‡§π‡•ã ‡§ó‡§Ø‡•Ä ‡§π‡•à‡§Ç ‡•§   \n",
       "50146                   ‡§∏‡§°‡§º‡§ï ‡§Æ‡§≤‡§¨‡•á ‡§ï‡§æ ‡§¢‡•á‡§∞ ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§¶‡•Å‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ ‡§ö‡§æ‡§∞ ‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ‡§æ‡§π‡§® ‡§ö‡§æ‡§≤‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§∞‡•á‡§∂‡§æ‡§® ‡§π‡•ã‡§®‡§æ ‡§™‡§°‡§º ‡§∞‡§π‡§æ ‡§π‡•à‡§Ç ‡•§   \n",
       "\n",
       "                                                                                                      output  \\\n",
       "40665       ‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡§æ ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§    \n",
       "48520                                                                                          ‡§§‡§•‡§æ ‡§ó‡•å‡§∂‡§æ‡§≤‡§æ ‡§ï‡§æ   \n",
       "138403                                              ‡§¨‡§ö‡•ç‡§ö‡•á ‡§ñ‡•á‡§≤ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§ï‡§ø‡§≤‡§ï‡§æ‡§∞‡§ø‡§Ø‡§æ‡§Å ‡§ó‡•Ç‡§Ç‡§ú ‡§∞‡§π‡•Ä ‡§π‡•à‡§Ç ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§æ‡§£‡•ç‡§° ‡§Æ‡•á‡§Ç   \n",
       "130079  ‡§ß‡§∞‡§®‡•á ‡§™‡§∞ ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø ‡§Ö‡§¨ ‡§∏‡§Æ‡§Ø ‡§Ü ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ó‡§®‡•ç‡§®‡•á ‡§ï‡•Ä ‡§ñ‡•á‡§§‡•Ä ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§ï‡•á ‡§≤‡§ø‡§Ø‡•á ‡§®‡•Å‡§ï‡§∏‡§æ‡§® ‡§ï‡§æ ‡§∏‡•å‡§¶‡§æ ‡§π‡•ã ‡§ó‡§Ø‡•Ä ‡§π‡•à ‡•§    \n",
       "50146                  ‡§∏‡§°‡§º‡§ï ‡§Æ‡§≤‡§¨‡•á ‡§ï‡§æ ‡§¢‡•á‡§∞ ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§¶‡•Å‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ ‡§ö‡§æ‡§∞ ‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ‡§æ‡§π‡§® ‡§ö‡§æ‡§≤‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§∞‡•á‡§∂‡§æ‡§® ‡§π‡•ã‡§®‡§æ ‡§™‡§°‡§º ‡§∞‡§π‡§æ ‡§π‡•à ‡•§    \n",
       "\n",
       "        input_token_len  \n",
       "40665                28  \n",
       "48520                 6  \n",
       "138403               14  \n",
       "130079               29  \n",
       "50146                23  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd321d92-6f3a-40ee-ba4a-907be30ea244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14000.000000\n",
       "mean        21.935000\n",
       "std         12.422581\n",
       "min          4.000000\n",
       "25%         14.000000\n",
       "50%         19.000000\n",
       "75%         27.000000\n",
       "max        308.000000\n",
       "Name: input_token_len, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['input_token_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5e933a2-ae5f-4c20-87b4-4dbb99b2c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use a token length of 64 since it will cover the vast majority of examples\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ca6fab2-ae06-441f-807e-da87e843249b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'input_token_len', '__index_level_0__'],\n",
       "    num_rows: 14000\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d75fca39-9ccb-4bce-8a14-ca5d573cb60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GrammarDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer,print_text=False):         \n",
    "        self.dataset = dataset\n",
    "        self.pad_to_max_length = False\n",
    "        self.tokenizer = tokenizer\n",
    "        self.print_text = print_text\n",
    "        self.max_len = 64\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "    def tokenize_data(self, example):\n",
    "        input_, target_ = example['input'], example['output']\n",
    "\n",
    "        # tokenize inputs\n",
    "        tokenized_inputs = tokenizer(input_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "    \n",
    "        tokenized_targets = tokenizer(target_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "\n",
    "        inputs={\"input_ids\": tokenized_inputs['input_ids'],\n",
    "            \"attention_mask\": tokenized_inputs['attention_mask'],\n",
    "            \"labels\": tokenized_targets['input_ids']\n",
    "        }\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenize_data(self.dataset[index])\n",
    "        \n",
    "        if self.print_text:\n",
    "            for k in inputs.keys():\n",
    "                print(k, len(inputs[k]))\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8aa2940-f691-4bc5-9c05-d70ff42e05ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids 27\n",
      "attention_mask 27\n",
      "labels 27\n",
      "{'input_ids': [104, 75159, 1154, 7899, 106524, 3768, 1169, 16015, 1250, 2316, 1419, 1123, 7132, 64219, 2115, 1125, 30361, 1438, 1228, 106524, 1117, 10171, 1194, 1254, 7544, 492, 105], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [104, 75159, 1154, 7899, 106524, 3768, 1169, 16015, 1250, 2316, 1243, 1123, 7132, 64219, 2115, 1125, 30361, 1438, 1228, 106524, 1117, 10171, 1194, 1254, 7544, 492, 105]}\n"
     ]
    }
   ],
   "source": [
    "dataset = GrammarDataset(test_dataset, tokenizer, True)\n",
    "print(dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc0564-2706-4d95-8dfb-e9970a0e2a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e6bd3fc-b11e-47a1-b473-10705ebe1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Evaluator\n",
    "#!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f50438a6-1ed2-460a-8bb0-307fa7b23806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf186c82-6bea-4bb3-b9f3-1b00df72ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model\n",
    "#data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest', return_tensors='pt')\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest', return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b108ae3-2088-44fa-af0f-8b3583bd157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining training related arguments\n",
    "batch_size = 8\n",
    "'''\n",
    "args = Seq2SeqTrainingArguments(output_dir=\"models/hindi/T5_muRIL_3\",\n",
    "                        evaluation_strategy=\"epoch\",\n",
    "                        per_device_train_batch_size=batch_size,\n",
    "                        per_device_eval_batch_size=batch_size,\n",
    "                        learning_rate=2e-5,\n",
    "                        num_train_epochs=2,\n",
    "                        weight_decay=0.1,\n",
    "                        save_total_limit=2,\n",
    "                        predict_with_generate=True,\n",
    "                        fp16 = True,\n",
    "                        gradient_accumulation_steps = 6,\n",
    "                        #eval_steps = 500,\n",
    "                        #save_steps = 2000,\n",
    "                        #dataloader_num_workers= 4,\n",
    "                        load_best_model_at_end=True,\n",
    "                        logging_dir=\"logs/hindi/T5_muRIL_3\")\n",
    "'''\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"models/hindi/T5_muRIL\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.1,\n",
    "    save_steps=10000,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    #logging_dir=\"logs/hindi/T5_muRIL_3\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2a430e6-c635-43bb-a305-357ac0773191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfbcd6b1-bdcb-4f9d-a7ca-cd6924d7e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d9743c7-8b58-4bee-9ab1-2fcd8032b17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# defining trainer using ü§ó\n",
    "trainer = Seq2SeqTrainer(model=model, \n",
    "                args=args, \n",
    "                train_dataset= GrammarDataset(train_dataset, tokenizer),\n",
    "                eval_dataset=GrammarDataset(test_dataset, tokenizer),\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator)\n",
    "                #compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50e692da-5dee-4325-b082-b660f82d1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aaaea036-afff-4988-b9c3-605dace1b50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 126000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 78750\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78750' max='78750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78750/78750 6:38:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.524600</td>\n",
       "      <td>0.324090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.444900</td>\n",
       "      <td>0.240954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.391500</td>\n",
       "      <td>0.194559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.356900</td>\n",
       "      <td>0.176006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.348300</td>\n",
       "      <td>0.169690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to models/hindi/T5_muRIL/checkpoint-10000\n",
      "Configuration saved in models/hindi/T5_muRIL/checkpoint-10000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL/checkpoint-10000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hindi/T5_muRIL/checkpoint-20000\n",
      "Configuration saved in models/hindi/T5_muRIL/checkpoint-20000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL/checkpoint-20000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL/checkpoint-30000\n",
      "Configuration saved in models/hindi/T5_muRIL/checkpoint-30000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL/checkpoint-30000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hindi/T5_muRIL/checkpoint-40000\n",
      "Configuration saved in models/hindi/T5_muRIL/checkpoint-40000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL/checkpoint-40000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL/checkpoint-40000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hindi/T5_muRIL/checkpoint-50000\n",
      "Configuration saved in models/hindi/T5_muRIL/checkpoint-50000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL/checkpoint-50000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL/checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL/checkpoint-50000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL/checkpoint-60000\n",
      "Configuration saved in models/hindi/T5_muRIL/checkpoint-60000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL/checkpoint-60000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL/checkpoint-60000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL/checkpoint-60000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hindi/T5_muRIL/checkpoint-70000\n",
      "Configuration saved in models/hindi/T5_muRIL/checkpoint-70000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL/checkpoint-70000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL/checkpoint-70000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL/checkpoint-70000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=78750, training_loss=0.4303435190352183, metrics={'train_runtime': 23896.2568, 'train_samples_per_second': 26.364, 'train_steps_per_second': 3.295, 'total_flos': 2.96756122281984e+16, 'train_loss': 0.4303435190352183, 'epoch': 5.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Wandb API Key: bcdbdd5ee9d76a20c90b5f2a246eb45f14b341a9\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"     #Disabling Wandb\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4313cc0-2a52-490a-bdbd-097c6832a0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065a0b4-9437-4647-a581-407445c7bf42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e746eaf-0ec9-4884-b86f-605a3e569776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 126000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15750\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15750' max='15750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15750/15750 1:19:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.669900</td>\n",
       "      <td>0.476513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-10000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-10000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-10000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15750, training_loss=0.7122194562639509, metrics={'train_runtime': 4755.8781, 'train_samples_per_second': 26.494, 'train_steps_per_second': 3.312, 'total_flos': 5930269812449280.0, 'train_loss': 0.7122194562639509, 'epoch': 1.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Wandb API Key: bcdbdd5ee9d76a20c90b5f2a246eb45f14b341a9\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"     #Disabling Wandb\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2e3dab-c263-41e6-81e6-f241db21be94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f5246-3f39-4d82-b4b6-a31f62d60a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3432ba00-8cd4-4656-aae6-8b886eb97d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 126000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 31500\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31500' max='31500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31500/31500 3:10:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.246400</td>\n",
       "      <td>1.125347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-2000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-2000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-4000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-4000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-6000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-6000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-8000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-8000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-8000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-10000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-10000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-10000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-12000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-12000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-12000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-14000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-14000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-14000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-16000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-16000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-16000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-18000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-18000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-18000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-20000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-20000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-20000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-22000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-22000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-22000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-24000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-24000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-24000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-26000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-26000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-26000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-28000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-28000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-28000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_muRIL_3/checkpoint-30000\n",
      "Configuration saved in models/hindi/T5_muRIL_3/checkpoint-30000/config.json\n",
      "Model weights saved in models/hindi/T5_muRIL_3/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_muRIL_3/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_muRIL_3/checkpoint-30000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31500, training_loss=1.3424053199404762, metrics={'train_runtime': 11415.222, 'train_samples_per_second': 11.038, 'train_steps_per_second': 2.759, 'total_flos': 5013459920793600.0, 'train_loss': 1.3424053199404762, 'epoch': 1.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Wandb API Key: bcdbdd5ee9d76a20c90b5f2a246eb45f14b341a9\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"     #Disabling Wandb\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4ed8776-52ca-4d06-9e15-158994267915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch_device = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b68254c3-619e-464f-bfa6-01c21279c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to t5_gec_hindi_muRIL\n",
      "Configuration saved in t5_gec_hindi_muRIL/config.json\n",
      "Model weights saved in t5_gec_hindi_muRIL/pytorch_model.bin\n",
      "tokenizer config file saved in t5_gec_hindi_muRIL/tokenizer_config.json\n",
      "Special tokens file saved in t5_gec_hindi_muRIL/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "#Save the trained Model\n",
    "trainer.save_model('t5_gec_hindi_muRIL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b9bd5-acdd-4728-8bb5-8ac6bb809b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6e472-f983-4018-97ea-fd020ae3ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!zip -r 't5_gec_hindi_muRIL_2.zip' 't5_gec_hindi_muRIL_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895631b-8f35-4ea2-930f-ef9c56e17072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da45d37b-cd1f-43e7-a4e2-bc18ef984b14",
   "metadata": {},
   "source": [
    "# Testing Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea2de44-6fbe-40a6-ba4f-f074a82b6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration,AutoTokenizer\n",
    "model_name = 't5_gec_hindi_muRIL'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98143e4e-6280-4ab1-96a5-2be73cad8d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"surajp/RoBERTa-hindi-guj-san\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n",
    "trained_model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dee6d65-83bc-4bb6-bdbc-8511c6becea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(197285, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(197285, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(197285, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=197285, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde5ea1c-2854-458b-bd0e-288c372d6768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+------------+\n",
      "|                               Modules                                | Parameters |\n",
      "+----------------------------------------------------------------------+------------+\n",
      "|                            shared.weight                             | 151514880  |\n",
      "|            encoder.block.0.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.0.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.0.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.0.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "| encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight |    384     |\n",
      "|              encoder.block.0.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.0.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.0.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.0.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.1.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.1.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.1.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.1.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.1.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.1.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.1.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.1.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.2.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.2.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.2.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.2.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.2.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.2.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.2.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.2.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.3.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.3.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.3.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.3.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.3.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.3.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.3.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.3.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.4.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.4.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.4.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.4.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.4.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.4.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.4.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.4.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.5.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.5.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.5.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.5.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.5.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.5.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.5.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.5.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.6.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.6.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.6.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.6.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.6.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.6.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.6.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.6.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.7.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.7.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.7.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.7.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.7.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.7.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.7.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.7.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.8.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.8.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.8.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.8.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.8.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.8.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.8.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.8.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.9.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.9.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.9.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.9.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.9.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.9.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.9.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.9.layer.1.layer_norm.weight               |    768     |\n",
      "|           encoder.block.10.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|           encoder.block.10.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|           encoder.block.10.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|           encoder.block.10.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.10.layer.0.layer_norm.weight              |    768     |\n",
      "|          encoder.block.10.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|          encoder.block.10.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.10.layer.1.layer_norm.weight              |    768     |\n",
      "|           encoder.block.11.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|           encoder.block.11.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|           encoder.block.11.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|           encoder.block.11.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.11.layer.0.layer_norm.weight              |    768     |\n",
      "|          encoder.block.11.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|          encoder.block.11.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.11.layer.1.layer_norm.weight              |    768     |\n",
      "|                   encoder.final_layer_norm.weight                    |    768     |\n",
      "|            decoder.block.0.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.0.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.0.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.0.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "| decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight |    384     |\n",
      "|              decoder.block.0.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.0.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.0.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.0.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.0.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.0.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.0.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.0.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.0.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.1.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.1.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.1.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.1.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.1.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.1.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.1.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.1.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.1.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.1.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.1.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.1.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.1.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.2.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.2.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.2.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.2.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.2.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.2.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.2.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.2.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.2.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.2.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.2.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.2.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.2.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.3.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.3.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.3.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.3.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.3.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.3.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.3.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.3.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.3.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.3.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.3.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.3.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.3.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.4.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.4.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.4.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.4.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.4.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.4.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.4.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.4.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.4.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.4.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.4.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.4.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.4.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.5.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.5.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.5.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.5.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.5.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.5.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.5.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.5.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.5.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.5.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.5.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.5.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.5.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.6.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.6.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.6.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.6.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.6.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.6.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.6.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.6.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.6.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.6.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.6.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.6.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.6.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.7.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.7.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.7.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.7.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.7.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.7.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.7.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.7.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.7.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.7.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.7.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.7.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.7.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.8.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.8.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.8.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.8.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.8.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.8.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.8.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.8.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.8.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.8.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.8.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.8.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.8.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.9.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.9.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.9.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.9.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.9.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.9.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.9.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.9.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.9.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.9.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.9.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.9.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.9.layer.2.layer_norm.weight               |    768     |\n",
      "|           decoder.block.10.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|           decoder.block.10.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|           decoder.block.10.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|           decoder.block.10.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.10.layer.0.layer_norm.weight              |    768     |\n",
      "|          decoder.block.10.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|          decoder.block.10.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|          decoder.block.10.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|          decoder.block.10.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.10.layer.1.layer_norm.weight              |    768     |\n",
      "|          decoder.block.10.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|          decoder.block.10.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.10.layer.2.layer_norm.weight              |    768     |\n",
      "|           decoder.block.11.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|           decoder.block.11.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|           decoder.block.11.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|           decoder.block.11.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.11.layer.0.layer_norm.weight              |    768     |\n",
      "|          decoder.block.11.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|          decoder.block.11.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|          decoder.block.11.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|          decoder.block.11.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.11.layer.1.layer_norm.weight              |    768     |\n",
      "|          decoder.block.11.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|          decoder.block.11.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.11.layer.2.layer_norm.weight              |    768     |\n",
      "|                   decoder.final_layer_norm.weight                    |    768     |\n",
      "+----------------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 349744128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "349744128"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "count_parameters(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e20830-2f49-4050-b1cd-67ebdb5a4534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 1334.168MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in trained_model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in trained_model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9669013c-f34f-44b1-a35c-f265f106a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grammar(input_text,num_return_sequences):\n",
    "    #batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\").to(torch_device)\n",
    "    batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\",return_token_type_ids=False).to(device)\n",
    "    #print(batch)\n",
    "    #translated = model1.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    translated= trained_model.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    #print(\"here\")\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b786ccc2-8dea-4c4d-b60c-c242ef187a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(T5ForConditionalGeneration.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2dc10f7d-909a-4d2b-917a-475e43619610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡•Ä ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§\n",
      "Correct Seentence: ‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡§æ ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§ \n",
      "Predicted Sentence: ‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§æ‡§∞‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡•Ä ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§\n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[0]\n",
    "correct = test_df['output'].iat[0]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "l = len(correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=4)[0]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33497c92-ccc5-4eba-89d9-004161a5e100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§ú‡§æ‡§®‡§§‡§æ ‡§π‡•à‡§Ç ‡§ï‡§ø ‡§â‡§∏‡§ï‡•á ‡§Æ‡§æ‡§∏‡§ø‡§ï ‡§µ‡•á‡§§‡§® ‡§ï‡§æ ‡§ï‡•Å‡§õ ‡§Ö‡§Ç‡§∂ ‡§ò‡•Ç‡§∏ ‡§Æ‡•á‡§Ç ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à‡§Ç ‡•§\n",
      "Correct Seentence: ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§ú‡§æ‡§®‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§â‡§∏‡§ï‡•á ‡§Æ‡§æ‡§∏‡§ø‡§ï ‡§µ‡•á‡§§‡§® ‡§ï‡§æ ‡§ï‡•Å‡§õ ‡§Ö‡§Ç‡§∂ ‡§ò‡•Ç‡§∏ ‡§Æ‡•á‡§Ç ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§ \n",
      "Predicted Sentence: ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§ú‡§æ‡§®‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§â‡§∏‡§ï‡•á ‡§Æ‡§æ‡§∏‡§ø‡§ï ‡§µ‡•á‡§§‡§® ‡§ï‡§æ ‡§ï‡•Å‡§õ ‡§Ö‡§Ç‡§∂ ‡§ò‡•Ç‡§∏ ‡§Æ‡•á‡§Ç ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§\n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[13]\n",
    "correct = test_df['output'].iat[13]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "l = len(correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=2)[0]\n",
    "p = len(predicted_s)\n",
    "'''\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "elif (p<l-4):\n",
    "    predicted_s = text\n",
    "'''\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f8b0823-1f45-41f2-a239-c4b724f58309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§á‡§∏ ‡§∏‡§¨‡§Æ‡•á‡§Ç ‡§Ö‡§π‡§Æ ‡§¨‡§æ‡§§ ‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ ‡§Æ‡§§‡§¶‡§æ‡§§‡§æ‡§ì‡§Ç ‡§ï‡§æ ‡§∞‡•Å‡§ù‡§æ‡§® ‡§π‡•à‡§Ç ‡•§\n",
      "Correct Seentence: ‡§á‡§∏ ‡§∏‡§¨‡§Æ‡•á‡§Ç ‡§Ö‡§π‡§Æ ‡§¨‡§æ‡§§ ‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ ‡§Æ‡§§‡§¶‡§æ‡§§‡§æ‡§ì‡§Ç ‡§ï‡§æ ‡§∞‡•Å‡§ù‡§æ‡§® ‡§π‡•à ‡•§ \n",
      "Predicted Sentence: ‡§á‡§∏ ‡§∏‡§¨‡§Æ‡•á‡§Ç ‡§Ö‡§π‡§Æ ‡§¨‡§æ‡§§ ‡§Æ‡•Å‡§∏‡•ç‡§≤‡§ø‡§Æ ‡§Æ‡§§‡§¶‡§æ‡§§‡§æ‡§ì‡§Ç ‡§ï‡§æ ‡§∞‡•Å‡§ù‡§æ‡§® ‡§π‡•à ‡•§\n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[43]\n",
    "correct = test_df['output'].iat[43]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "l = len(correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=2)[0]\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79771c80-b79f-4ac2-8086-4eb0a0d82051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§§‡§•‡•á ‡§ó‡•å‡§∂‡§æ‡§≤‡§æ ‡§ï‡§æ\n",
      "Correct Seentence: ‡§§‡§•‡§æ ‡§ó‡•å‡§∂‡§æ‡§≤‡§æ ‡§ï‡§æ\n",
      "Predicted Sentence: ‡§§‡§•‡§æ ‡§ó‡•å‡§∂‡§æ‡§≤‡§æ ‡§ï‡§æ\n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[1]\n",
    "correct = test_df['output'].iat[1]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "l = len(correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=4)[0][:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed1ae34f-0274-43ec-afed-f0ce7e723612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡•Ä ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§\n",
      "Correct Seentence: ‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡§æ ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§ \n",
      "Predicted Sentence: ‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§æ‡§∞‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡•Ä ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§\n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[0]\n",
    "correct = test_df['output'].iat[0]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "l = len(correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=2)[0]\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3c96d0d-6d65-42bc-8faa-30a3b931c57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Ø ‡§ú‡§®‡§§‡§æ ‡§¶‡§≤ ‡§π‡§∞‡§ø‡§Ø‡§æ‡§£‡§æ ‡§Ü‡§ú ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§Ü‡§∞‡§ï‡•ç‡§∑‡§£ ‡§µ‡§ø‡§ß‡•á‡§Ø‡§ï ‡§ï‡•á ‡§µ‡§ø‡§∞‡•ã‡§ß ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§¶‡•á‡§∂ ‡§∏‡§ö‡§ø‡§µ ‡§µ‡§ø‡§ú‡§Ø ‡§∏‡§ø‡§Ç‡§π ‡§ï‡§æ‡§∞‡§ó‡§µ‡§æ‡§≤ ‡§ï‡•á ‡§§‡•É‡§§‡•ç‡§µ ‡§Æ‡•á‡§Ç ‡§Ö‡§Ç‡§¨‡•á‡§°‡§ï‡§∞ ‡§™‡§æ‡§∞‡•ç‡§ï ‡§ï‡•á ‡§∏‡§Æ‡•Ä‡§™ ‡§ú‡•ã‡§¶‡§æ‡§∞ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§§‡§•‡§æ ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§Ö‡§ß‡•ç‡§Ø‡§ï‡•ç‡§∑‡§æ ‡§∏‡•ã‡§®‡§ø‡§Ø‡§æ ‡§ó‡§æ‡§Ç‡§ß‡•Ä ‡§µ ‡§≠‡§æ‡§ú‡§™‡§æ ‡§ï‡•Ä ‡§µ‡§∞‡§ø‡§∑‡•ç‡§† ‡§§‡•ç‡§∞‡•Ä ‡§∏‡•Å‡§∑‡§Æ‡§æ ‡§∏‡•ç‡§µ‡§∞‡§æ‡§ú ‡§ï‡§æ ‡§™‡•Å‡§§‡§≤‡§æ ‡§´‡•Ç‡§Ç‡§ï‡§æ ‡•§\n",
      "Correct Seentence: ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Ø ‡§ú‡§®‡§§‡§æ ‡§¶‡§≤ ‡§π‡§∞‡§ø‡§Ø‡§æ‡§£‡§æ ‡§®‡•á ‡§Ü‡§ú ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§Ü‡§∞‡§ï‡•ç‡§∑‡§£ ‡§µ‡§ø‡§ß‡•á‡§Ø‡§ï ‡§ï‡•á ‡§µ‡§ø‡§∞‡•ã‡§ß ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§¶‡•á‡§∂ ‡§∏‡§ö‡§ø‡§µ ‡§µ‡§ø‡§ú‡§Ø ‡§∏‡§ø‡§Ç‡§π ‡§ï‡§æ‡§∞‡§ó‡§µ‡§æ‡§≤ ‡§ï‡•á ‡§®‡•á‡§§‡•É‡§§‡•ç‡§µ ‡§Æ‡•á‡§Ç ‡§Ö‡§Ç‡§¨‡•á‡§°‡§ï‡§∞ ‡§™‡§æ‡§∞‡•ç‡§ï ‡§ï‡•á ‡§∏‡§Æ‡•Ä‡§™ ‡§ú‡•ã‡§¶‡§æ‡§∞ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§§‡§•‡§æ ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§Ö‡§ß‡•ç‡§Ø‡§ï‡•ç‡§∑‡§æ ‡§∏‡•ã‡§®‡§ø‡§Ø‡§æ ‡§ó‡§æ‡§Ç‡§ß‡•Ä ‡§µ ‡§≠‡§æ‡§ú‡§™‡§æ ‡§ï‡•Ä ‡§µ‡§∞‡§ø‡§∑‡•ç‡§† ‡§®‡•á‡§§‡•ç‡§∞‡•Ä ‡§∏‡•Å‡§∑‡§Æ‡§æ ‡§∏‡•ç‡§µ‡§∞‡§æ‡§ú ‡§ï‡§æ ‡§™‡•Å‡§§‡§≤‡§æ ‡§´‡•Ç‡§Ç‡§ï‡§æ ‡•§ \n",
      "Predicted Sentence: ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Ø ‡§ú‡§®‡§§‡§æ ‡§¶‡§≤ ‡§π‡§∞‡§ø‡§Ø‡§æ‡§£‡§æ ‡§Ü‡§ú ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§Ü‡§∞‡§ï‡•ç‡§∑‡§£ ‡§µ‡§ø‡§ß‡•á‡§Ø‡§ï ‡§ï‡•á ‡§µ‡§ø‡§∞‡•ã‡§ß ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§¶‡•á‡§∂ ‡§∏‡§ö‡§ø‡§µ ‡§µ‡§ø‡§ú‡§Ø ‡§∏‡§ø‡§Ç‡§π ‡§®‡•á ‡§ï‡§æ‡§∞‡§ó‡§µ‡§æ‡§≤ ‡§ï‡•á ‡§®‡•á‡§§‡•É‡§§‡•ç‡§µ ‡§Æ‡•á‡§Ç ‡§Ö‡§Ç‡§¨‡•á‡§°‡§ï‡§∞ ‡§™‡§æ‡§∞‡•ç‡§ï ‡§ï‡•á ‡§∏‡§Æ‡•Ä‡§™ ‡§ú‡•ã‡§¶‡§æ‡§∞ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§§‡§•‡§æ ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§Ö‡§ß‡•ç‡§Ø‡§ï‡•ç‡§∑‡§æ ‡§∏‡•ã‡§®‡§ø‡§Ø‡§æ ‡§ó‡§æ‡§Ç‡§ß‡•Ä ‡§µ ‡§≠‡§æ‡§ú‡§™‡§æ ‡§ï‡•Ä ‡§µ‡§∞‡§ø‡§∑‡•ç‡§† ‡§§‡•ç‡§∞‡•Ä ‡§∏‡•Å‡§∑‡§Æ‡§æ ‡§∏‡•ç‡§µ‡§∞‡§æ‡§ú ‡§ï‡§æ ‡§™‡•Å‡§§‡§≤‡§æ ‡§´‡•Ç‡§Ç‡§ï‡§æ ‡•§\n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[50]\n",
    "correct = test_df['output'].iat[50]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f05b016-c983-4167-a2f7-c5f1736de779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§ú‡§º‡§æ‡§π‡§ø‡§∞ ‡§π‡•à ‡§ú‡§¨ ‡§Æ‡•à‡§Ç‡§®‡•á ‡§≤‡§ø‡§ñ‡•Ç‡§Ç‡§ó‡§æ ‡§§‡•ã ‡§µ‡•ã ‡§Æ‡•á‡§∞‡•á ‡§π‡•Ä ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§ó‡§§ ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§ï‡§π‡§≤‡§æ‡§Ø‡•á‡§Ç‡§ó‡•á ‡§ï‡§ø‡§∏‡•Ä ‡§∏‡§Ç‡§∏‡•ç‡§•‡§æ‡§® ‡§ï‡•á ‡§®‡§π‡•Ä‡§Ç ‡•§\n",
      "Correct Seentence: ‡§ú‡§º‡§æ‡§π‡§ø‡§∞ ‡§π‡•à ‡§ú‡§¨ ‡§Æ‡•à‡§Ç ‡§≤‡§ø‡§ñ‡•Ç‡§Ç‡§ó‡§æ ‡§§‡•ã ‡§µ‡•ã ‡§Æ‡•á‡§∞‡•á ‡§π‡•Ä ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§ó‡§§ ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§ï‡§π‡§≤‡§æ‡§Ø‡•á‡§Ç‡§ó‡•á ‡§ï‡§ø‡§∏‡•Ä ‡§∏‡§Ç‡§∏‡•ç‡§•‡§æ‡§® ‡§ï‡•á ‡§®‡§π‡•Ä‡§Ç ‡•§ \n",
      "Predicted Sentence: ‡§ú‡§º‡§æ‡§π‡§ø‡§∞ ‡§π‡•à ‡§ú‡§¨ ‡§Æ‡•à‡§Ç ‡§≤‡§ø‡§ñ‡•Ç‡§Ç‡§ó‡§æ ‡§§‡•ã ‡§µ‡•ã ‡§Æ‡•á‡§∞‡•á ‡§π‡•Ä ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§ó‡§§ ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§ï‡§π‡§≤‡§æ‡§Ø‡•á‡§Ç‡§ó‡•á ‡§ï‡§ø‡§∏‡•Ä ‡§∏‡§Ç‡§∏‡•ç‡§•‡§æ‡§® ‡§ï‡•á ‡§®‡§π‡•Ä‡§Ç ‡•§\n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[60]\n",
    "correct = test_df['output'].iat[60]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7cf190-899c-4fc6-9954-59ba79ef6614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d488748e-8345-4b94-baf3-84925a29ba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§ó‡§®‡•Ä‡§Æ‡§§ ‡§π‡•à‡§Ç ‡§ó‡§£‡•á‡§∂‡§∂‡§Ç‡§ï‡§∞ ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡•Ä ‡§®‡•á ‡•®‡•¶‡•¶ ‡§∞‡•Å‡§™‡§Ø‡•á ‡§ï‡§æ ‡§Æ‡§®‡•Ä‡§Ü‡§∞‡•ç‡§°‡§∞ ‡§≠‡•á‡§ú‡§ï‡§∞ ‡§Ö‡§∂‡§´‡§º‡§æ‡§ï‡§º ‡§ï‡•Ä ‡§Æ‡§ú‡§æ‡§∞ ‡§™‡§∞ ‡§õ‡§§ ‡§°‡§≤‡§µ‡§æ ‡§ï‡§∞ ‡§â‡§∏‡•á ‡§ß‡•Ç‡§™ ‡§ï‡•á ‡§∏‡§æ‡§Ø‡•á ‡§∏‡•á ‡§¨‡§ö‡§æ ‡§≤‡§ø‡§Ø‡§æ ‡•§\n",
      "Correct Seentence: ‡§ó‡§®‡•Ä‡§Æ‡§§ ‡§π‡•à ‡§ó‡§£‡•á‡§∂‡§∂‡§Ç‡§ï‡§∞ ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡•Ä ‡§®‡•á ‡•®‡•¶‡•¶ ‡§∞‡•Å‡§™‡§Ø‡•á ‡§ï‡§æ ‡§Æ‡§®‡•Ä‡§Ü‡§∞‡•ç‡§°‡§∞ ‡§≠‡•á‡§ú‡§ï‡§∞ ‡§Ö‡§∂‡§´‡§º‡§æ‡§ï‡§º ‡§ï‡•Ä ‡§Æ‡§ú‡§æ‡§∞ ‡§™‡§∞ ‡§õ‡§§ ‡§°‡§≤‡§µ‡§æ ‡§ï‡§∞ ‡§â‡§∏‡•á ‡§ß‡•Ç‡§™ ‡§ï‡•á ‡§∏‡§æ‡§Ø‡•á ‡§∏‡•á ‡§¨‡§ö‡§æ ‡§≤‡§ø‡§Ø‡§æ ‡•§ \n",
      "Predicted Sentence: ‡§ó‡§®‡•Ä‡§Æ‡§§ ‡§π‡•à ‡§ó‡§£‡•á‡§∂‡§∂‡§Ç‡§ï‡§∞ ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡•Ä ‡§®‡•á ‡•®‡•¶‡•¶ ‡§∞‡•Å‡§™‡§Ø‡•á ‡§ï‡§æ ‡§Æ‡§®‡•Ä‡§Ü‡§∞‡•ç‡§°‡§∞ ‡§≠‡•á‡§ú‡§ï‡§∞ ‡§Ö‡§∂‡§´‡§º‡§æ‡§ï‡§º ‡§ï‡•Ä ‡§Æ‡§ú‡§æ‡§∞ ‡§™‡§∞ ‡§õ‡§§ ‡§°‡§≤‡§µ‡§æ ‡§ï‡§∞ ‡§â‡§∏‡•á ‡§ß‡•Ç‡§™ ‡§ï‡•á ‡§∏‡§æ‡§Ø‡•á ‡§∏‡•á ‡§¨‡§ö‡§æ ‡§≤‡§ø‡§Ø‡§æ ‡•§\n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[51]\n",
    "correct = test_df['output'].iat[51]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2adc94a-4fc3-4eb6-b336-574d99c230eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§π‡§Æ ‡§™‡§æ‡§∏ ‡§ñ‡•Ä‡§Ç‡§ö ‡§≤‡•á‡§§‡•á ‡§π‡•à‡§Ç‡§Ç ‡•§\n",
      "Correct Seentence: ‡§π‡§Æ ‡§™‡§æ‡§∏ ‡§ñ‡•Ä‡§Ç‡§ö ‡§≤‡•á‡§§‡•á ‡§π‡•à‡§Ç ‡•§ \n",
      "Predicted Sentence: ‡§π‡§Æ ‡§™‡§æ‡§∏ ‡§ñ‡•Ä‡§Ç‡§ö ‡§≤‡•á‡§§‡•á ‡§π‡•à‡§Ç ‡•§ \n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[20]\n",
    "correct = test_df['output'].iat[20]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "p=len(predicted_s)\n",
    "l = len(text)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b37d9f-25df-483f-a3c3-29603e854408",
   "metadata": {},
   "source": [
    "# Performance Analysis\n",
    "1. BLEU Score\n",
    "2. GLEU Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18317b87-03d3-4d73-ae22-9cf8cb8e1db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da058ef4-524c-4808-a1ba-bbd143f534d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>input_token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40665</th>\n",
       "      <td>‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡•Ä ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§</td>\n",
       "      <td>‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡§æ ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48520</th>\n",
       "      <td>‡§§‡§•‡•á ‡§ó‡•å‡§∂‡§æ‡§≤‡§æ ‡§ï‡§æ</td>\n",
       "      <td>‡§§‡§•‡§æ ‡§ó‡•å‡§∂‡§æ‡§≤‡§æ ‡§ï‡§æ</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138403</th>\n",
       "      <td>‡§¨‡§ö‡•ç‡§ö‡•á ‡§ñ‡•á‡§≤ ‡§∞‡§π‡•á ‡§π‡•à ‡§ï‡§ø‡§≤‡§ï‡§æ‡§∞‡§ø‡§Ø‡§æ‡§Å ‡§ó‡•Ç‡§Ç‡§ú ‡§∞‡§π‡•Ä ‡§π‡•à ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§æ‡§£‡•ç‡§° ‡§Æ‡•á‡§Ç</td>\n",
       "      <td>‡§¨‡§ö‡•ç‡§ö‡•á ‡§ñ‡•á‡§≤ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§ï‡§ø‡§≤‡§ï‡§æ‡§∞‡§ø‡§Ø‡§æ‡§Å ‡§ó‡•Ç‡§Ç‡§ú ‡§∞‡§π‡•Ä ‡§π‡•à‡§Ç ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§æ‡§£‡•ç‡§° ‡§Æ‡•á‡§Ç</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130079</th>\n",
       "      <td>‡§ß‡§∞‡§®‡•á ‡§™‡§∞ ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø ‡§Ö‡§¨ ‡§∏‡§Æ‡§Ø ‡§Ü ‡§ó‡§Ø‡§æ ‡§π‡•à‡§Ç ‡§ï‡§ø ‡§ó‡§®‡•ç‡§®‡•á ‡§ï‡•Ä ‡§ñ‡•á‡§§‡•Ä ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§ï‡•á ‡§≤‡§ø‡§Ø‡•á ‡§®‡•Å‡§ï‡§∏‡§æ‡§® ‡§ï‡§æ ‡§∏‡•å‡§¶‡§æ ‡§π‡•ã ‡§ó‡§Ø‡•Ä ‡§π‡•à‡§Ç ‡•§</td>\n",
       "      <td>‡§ß‡§∞‡§®‡•á ‡§™‡§∞ ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø ‡§Ö‡§¨ ‡§∏‡§Æ‡§Ø ‡§Ü ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ó‡§®‡•ç‡§®‡•á ‡§ï‡•Ä ‡§ñ‡•á‡§§‡•Ä ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§ï‡•á ‡§≤‡§ø‡§Ø‡•á ‡§®‡•Å‡§ï‡§∏‡§æ‡§® ‡§ï‡§æ ‡§∏‡•å‡§¶‡§æ ‡§π‡•ã ‡§ó‡§Ø‡•Ä ‡§π‡•à ‡•§</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50146</th>\n",
       "      <td>‡§∏‡§°‡§º‡§ï ‡§Æ‡§≤‡§¨‡•á ‡§ï‡§æ ‡§¢‡•á‡§∞ ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§¶‡•Å‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ ‡§ö‡§æ‡§∞ ‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ‡§æ‡§π‡§® ‡§ö‡§æ‡§≤‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§∞‡•á‡§∂‡§æ‡§® ‡§π‡•ã‡§®‡§æ ‡§™‡§°‡§º ‡§∞‡§π‡§æ ‡§π‡•à‡§Ç ‡•§</td>\n",
       "      <td>‡§∏‡§°‡§º‡§ï ‡§Æ‡§≤‡§¨‡•á ‡§ï‡§æ ‡§¢‡•á‡§∞ ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§¶‡•Å‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ ‡§ö‡§æ‡§∞ ‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ‡§æ‡§π‡§® ‡§ö‡§æ‡§≤‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§∞‡•á‡§∂‡§æ‡§® ‡§π‡•ã‡§®‡§æ ‡§™‡§°‡§º ‡§∞‡§π‡§æ ‡§π‡•à ‡•§</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        input  \\\n",
       "40665         ‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡•Ä ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§   \n",
       "48520                                                                                           ‡§§‡§•‡•á ‡§ó‡•å‡§∂‡§æ‡§≤‡§æ ‡§ï‡§æ   \n",
       "138403                                                 ‡§¨‡§ö‡•ç‡§ö‡•á ‡§ñ‡•á‡§≤ ‡§∞‡§π‡•á ‡§π‡•à ‡§ï‡§ø‡§≤‡§ï‡§æ‡§∞‡§ø‡§Ø‡§æ‡§Å ‡§ó‡•Ç‡§Ç‡§ú ‡§∞‡§π‡•Ä ‡§π‡•à ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§æ‡§£‡•ç‡§° ‡§Æ‡•á‡§Ç   \n",
       "130079  ‡§ß‡§∞‡§®‡•á ‡§™‡§∞ ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø ‡§Ö‡§¨ ‡§∏‡§Æ‡§Ø ‡§Ü ‡§ó‡§Ø‡§æ ‡§π‡•à‡§Ç ‡§ï‡§ø ‡§ó‡§®‡•ç‡§®‡•á ‡§ï‡•Ä ‡§ñ‡•á‡§§‡•Ä ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§ï‡•á ‡§≤‡§ø‡§Ø‡•á ‡§®‡•Å‡§ï‡§∏‡§æ‡§® ‡§ï‡§æ ‡§∏‡•å‡§¶‡§æ ‡§π‡•ã ‡§ó‡§Ø‡•Ä ‡§π‡•à‡§Ç ‡•§   \n",
       "50146                   ‡§∏‡§°‡§º‡§ï ‡§Æ‡§≤‡§¨‡•á ‡§ï‡§æ ‡§¢‡•á‡§∞ ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§¶‡•Å‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ ‡§ö‡§æ‡§∞ ‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ‡§æ‡§π‡§® ‡§ö‡§æ‡§≤‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§∞‡•á‡§∂‡§æ‡§® ‡§π‡•ã‡§®‡§æ ‡§™‡§°‡§º ‡§∞‡§π‡§æ ‡§π‡•à‡§Ç ‡•§   \n",
       "\n",
       "                                                                                                      output  \\\n",
       "40665       ‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡§æ ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§    \n",
       "48520                                                                                          ‡§§‡§•‡§æ ‡§ó‡•å‡§∂‡§æ‡§≤‡§æ ‡§ï‡§æ   \n",
       "138403                                              ‡§¨‡§ö‡•ç‡§ö‡•á ‡§ñ‡•á‡§≤ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§ï‡§ø‡§≤‡§ï‡§æ‡§∞‡§ø‡§Ø‡§æ‡§Å ‡§ó‡•Ç‡§Ç‡§ú ‡§∞‡§π‡•Ä ‡§π‡•à‡§Ç ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§æ‡§£‡•ç‡§° ‡§Æ‡•á‡§Ç   \n",
       "130079  ‡§ß‡§∞‡§®‡•á ‡§™‡§∞ ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø ‡§Ö‡§¨ ‡§∏‡§Æ‡§Ø ‡§Ü ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ó‡§®‡•ç‡§®‡•á ‡§ï‡•Ä ‡§ñ‡•á‡§§‡•Ä ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§ï‡•á ‡§≤‡§ø‡§Ø‡•á ‡§®‡•Å‡§ï‡§∏‡§æ‡§® ‡§ï‡§æ ‡§∏‡•å‡§¶‡§æ ‡§π‡•ã ‡§ó‡§Ø‡•Ä ‡§π‡•à ‡•§    \n",
       "50146                  ‡§∏‡§°‡§º‡§ï ‡§Æ‡§≤‡§¨‡•á ‡§ï‡§æ ‡§¢‡•á‡§∞ ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§¶‡•Å‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ ‡§ö‡§æ‡§∞ ‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ‡§æ‡§π‡§® ‡§ö‡§æ‡§≤‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§∞‡•á‡§∂‡§æ‡§® ‡§π‡•ã‡§®‡§æ ‡§™‡§°‡§º ‡§∞‡§π‡§æ ‡§π‡•à ‡•§    \n",
       "\n",
       "        input_token_len  \n",
       "40665                28  \n",
       "48520                 6  \n",
       "138403               14  \n",
       "130079               29  \n",
       "50146                23  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c922895e-82d6-43db-a2c8-113bacd42372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv('DATA/etoori_test.csv')\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab40e4eb-00a5-429e-a837-ed75ea61cc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ ‡§Æ‡§æ‡§á‡§ï‡§≤ ‡§∂‡•Ç‡§Æ‡§æ‡§ï‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ö‡§≤‡§æ‡§à ‡§ó‡§à ‡§è‡§ï ‡§´‡§∞‡§æ‡§∞...</td>\n",
       "      <td>‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ ‡§Æ‡§æ‡§á‡§ï‡§≤ ‡§∂‡•Ç‡§Æ‡§æ‡§ï‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ö‡§≤‡§æ‡§à ‡§ó‡§à ‡§è‡§ï ‡§´‡§∞‡§æ‡§∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡§Ø‡§π ‡§Æ‡§® ‡§ï‡•ã ‡§ï‡§æ‡§¨‡•Ç ‡§Æ‡•á‡§Ç ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§π‡•à‡§Ç ‡§á‡§∏‡•Ä‡§≤‡§ø‡§è ...</td>\n",
       "      <td>‡§Ø‡§π ‡§Æ‡§® ‡§ï‡•ã ‡§ï‡§æ‡§¨‡•Ç ‡§Æ‡•á‡§Ç ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§π‡•à ‡§á‡§∏‡•Ä‡§≤‡§ø‡§è ‡§á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡§Ü‡§™ ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§™‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡§≤‡•ç‡§≤‡•Ä‡§® ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§¨‡§ö‡•ç‡§ö‡§æ ‡§â‡§∏‡§®‡•á‡§æ‡§Å...</td>\n",
       "      <td>‡§Ü‡§™ ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§™‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡§≤‡•ç‡§≤‡•Ä‡§® ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§¨‡§ö‡•ç‡§ö‡§æ ‡§µ‡§π‡§æ‡§Å ‡§™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§ï‡•á ‡§Æ‡•Å‡§§‡§æ‡§¨‡§ø‡§ï ‡§ß‡§Æ‡§æ‡§ï‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§≤‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•á ...</td>\n",
       "      <td>‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§ï‡•á ‡§Æ‡•Å‡§§‡§æ‡§¨‡§ø‡§ï ‡§ß‡§Æ‡§æ‡§ï‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§≤‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•á ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡§â‡§®‡§ï‡•Ä ‡§µ‡•ã ‡§µ‡§æ‡§≤‡•Ä ‡§¨‡§æ‡§§ ‡§≠‡•Ä ‡§Ö‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§∞‡§π‡§§‡•Ä ‡§π‡•à‡§Ç ‡•§</td>\n",
       "      <td>‡§â‡§®‡§ï‡•Ä ‡§µ‡•ã ‡§µ‡§æ‡§≤‡•Ä ‡§¨‡§æ‡§§ ‡§≠‡•Ä ‡§Ö‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§∞‡§π‡§§‡•Ä ‡§π‡•à ‡•§</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           enc_input  \\\n",
       "0  ‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ ‡§Æ‡§æ‡§á‡§ï‡§≤ ‡§∂‡•Ç‡§Æ‡§æ‡§ï‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ö‡§≤‡§æ‡§à ‡§ó‡§à ‡§è‡§ï ‡§´‡§∞‡§æ‡§∞...   \n",
       "1  ‡§Ø‡§π ‡§Æ‡§® ‡§ï‡•ã ‡§ï‡§æ‡§¨‡•Ç ‡§Æ‡•á‡§Ç ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§π‡•à‡§Ç ‡§á‡§∏‡•Ä‡§≤‡§ø‡§è ...   \n",
       "2  ‡§Ü‡§™ ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§™‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡§≤‡•ç‡§≤‡•Ä‡§® ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§¨‡§ö‡•ç‡§ö‡§æ ‡§â‡§∏‡§®‡•á‡§æ‡§Å...   \n",
       "3  ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§ï‡•á ‡§Æ‡•Å‡§§‡§æ‡§¨‡§ø‡§ï ‡§ß‡§Æ‡§æ‡§ï‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§≤‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•á ...   \n",
       "4            ‡§â‡§®‡§ï‡•Ä ‡§µ‡•ã ‡§µ‡§æ‡§≤‡•Ä ‡§¨‡§æ‡§§ ‡§≠‡•Ä ‡§Ö‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§∞‡§π‡§§‡•Ä ‡§π‡•à‡§Ç ‡•§   \n",
       "\n",
       "                                           dec_input  \n",
       "0  ‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ ‡§Æ‡§æ‡§á‡§ï‡§≤ ‡§∂‡•Ç‡§Æ‡§æ‡§ï‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ö‡§≤‡§æ‡§à ‡§ó‡§à ‡§è‡§ï ‡§´‡§∞‡§æ‡§∞...  \n",
       "1  ‡§Ø‡§π ‡§Æ‡§® ‡§ï‡•ã ‡§ï‡§æ‡§¨‡•Ç ‡§Æ‡•á‡§Ç ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§π‡•à ‡§á‡§∏‡•Ä‡§≤‡§ø‡§è ‡§á...  \n",
       "2  ‡§Ü‡§™ ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§™‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡§≤‡•ç‡§≤‡•Ä‡§® ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§¨‡§ö‡•ç‡§ö‡§æ ‡§µ‡§π‡§æ‡§Å ‡§™...  \n",
       "3  ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§ï‡•á ‡§Æ‡•Å‡§§‡§æ‡§¨‡§ø‡§ï ‡§ß‡§Æ‡§æ‡§ï‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§≤‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•á ...  \n",
       "4            ‡§â‡§®‡§ï‡•Ä ‡§µ‡•ã ‡§µ‡§æ‡§≤‡•Ä ‡§¨‡§æ‡§§ ‡§≠‡•Ä ‡§Ö‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§∞‡§π‡§§‡•Ä ‡§π‡•à ‡•§   "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9216427e-77e4-4d58-a4c1-0fd582745e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ ‡§Æ‡§æ‡§á‡§ï‡§≤ ‡§∂‡•Ç‡§Æ‡§æ‡§ï‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ö‡§≤‡§æ‡§à ‡§ó‡§à ‡§è‡§ï ‡§´‡§∞‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§∞ ‡§≠‡•Ä ‡§ï‡•ã ‡§Æ‡§ø‡§≤‡§ø‡§Ø‡§® ‡§°‡•â‡§≤‡§∞ ‡§ï‡•ç‡§≤‡§¨ ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§®‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§\n",
      "Correct Seentence: ‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ ‡§Æ‡§æ‡§á‡§ï‡§≤ ‡§∂‡•Ç‡§Æ‡§æ‡§ï‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ö‡§≤‡§æ‡§à ‡§ó‡§à ‡§è‡§ï ‡§´‡§∞‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§∞ ‡§ï‡•ã ‡§≠‡•Ä ‡§Æ‡§ø‡§≤‡§ø‡§Ø‡§® ‡§°‡•â‡§≤‡§∞ ‡§ï‡•ç‡§≤‡§¨ ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§®‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§ \n",
      "Predicted Sentence: ‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ ‡§Æ‡§æ‡§á‡§ï‡§≤ ‡§∂‡•Ç‡§Æ‡§æ‡§ï‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ö‡§≤‡§æ‡§à ‡§ó‡§à ‡§è‡§ï ‡§´‡§∞‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§∞ ‡§ï‡•ã ‡§≠‡•Ä ‡§Æ‡§ø‡§≤‡§ø‡§Ø‡§® ‡§°‡•â‡§≤‡§∞ ‡§ï‡•ç‡§≤‡§¨ ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§®‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§\n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[0]\n",
    "correct = df_test['dec_input'].iat[0]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fa5d601-9ba5-4961-8b1c-cb658cbcfdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.26 s, sys: 84.4 ms, total: 1.34 s\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d972ce-dab2-46ac-9de8-f0ffcfb53f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba007951-7c5e-4d99-8ca6-e9a4aa81d026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§Ø‡§π ‡§Æ‡§® ‡§ï‡•ã ‡§ï‡§æ‡§¨‡•Ç ‡§Æ‡•á‡§Ç ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§π‡•à‡§Ç ‡§á‡§∏‡•Ä‡§≤‡§ø‡§è ‡§á‡§∏‡•á ‡§ö‡§ø‡§§‡•ç‡§§ ‡§π‡§∏‡•ç‡§§ ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§Ø‡•ã‡§ó ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç‡§Ç ‡•§\n",
      "Correct Seentence: ‡§Ø‡§π ‡§Æ‡§® ‡§ï‡•ã ‡§ï‡§æ‡§¨‡•Ç ‡§Æ‡•á‡§Ç ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§π‡•à ‡§á‡§∏‡•Ä‡§≤‡§ø‡§è ‡§á‡§∏‡•á ‡§ö‡§ø‡§§‡•ç‡§§ ‡§π‡§∏‡•ç‡§§ ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§Ø‡•ã‡§ó ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç ‡•§ \n",
      "Predicted Sentence: ‡§Ø‡§π ‡§Æ‡§® ‡§ï‡•ã ‡§ï‡§æ‡§¨‡•Ç ‡§Æ‡•á‡§Ç ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§π‡•à ‡§á‡§∏‡•Ä‡§≤‡§ø‡§è ‡§á‡§∏‡•á ‡§ö‡§ø‡§§‡•ç‡§§ ‡§π‡§∏‡•ç‡§§ ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§Ø‡•ã‡§ó ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç ‡•§\n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[1]\n",
    "correct = df_test['dec_input'].iat[1]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b3169-ccf8-4ce1-adf8-4dd112c41f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c0c0e87-c89e-4143-bacf-d6fe15b9cca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§Ü‡§™ ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§™‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡§≤‡•ç‡§≤‡•Ä‡§® ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§¨‡§ö‡•ç‡§ö‡§æ ‡§â‡§∏‡§®‡•á‡§æ‡§Å ‡§™‡§∞ ‡§∂‡•ã‡§∞ ‡§Æ‡§ö‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§\n",
      "Correct Seentence: ‡§Ü‡§™ ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§™‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡§≤‡•ç‡§≤‡•Ä‡§® ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§¨‡§ö‡•ç‡§ö‡§æ ‡§µ‡§π‡§æ‡§Å ‡§™‡§∞ ‡§∂‡•ã‡§∞ ‡§Æ‡§ö‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§ \n",
      "Predicted Sentence: ‡§Ü‡§™ ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§™‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡§≤‡•ç‡§≤‡•Ä‡§® ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§¨‡§ö‡•ç‡§ö‡§æ ‡§µ‡§π‡§æ‡§Å ‡§™‡§∞ ‡§∂‡•ã‡§∞ ‡§Æ‡§ö‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§\n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[2]\n",
    "correct = df_test['dec_input'].iat[2]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6f233a47-440e-49b7-ba4f-b952be54da5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§ï‡•á ‡§Æ‡•Å‡§§‡§æ‡§¨‡§ø‡§ï ‡§ß‡§Æ‡§æ‡§ï‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§≤‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§ï‡•ã‡§à ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§≤‡•ã‡§ó ‡§•‡•á ‡§î‡§∞ ‡§Ø‡•á ‡§ó‡§≤‡§§ ‡§™‡§π‡§ö‡§æ‡§® ‡§ï‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡§æ ‡§π‡•à‡§Ç ‡•§\n",
      "Correct Seentence: ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§ï‡•á ‡§Æ‡•Å‡§§‡§æ‡§¨‡§ø‡§ï ‡§ß‡§Æ‡§æ‡§ï‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§≤‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§ï‡•ã‡§à ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§≤‡•ã‡§ó ‡§•‡•á ‡§î‡§∞ ‡§Ø‡•á ‡§ó‡§≤‡§§ ‡§™‡§π‡§ö‡§æ‡§® ‡§ï‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡§æ ‡§π‡•à ‡•§ \n",
      "Predicted Sentence: ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§ï‡•á ‡§Æ‡•Å‡§§‡§æ‡§¨‡§ø‡§ï ‡§ß‡§Æ‡§æ‡§ï‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§≤‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§ï‡•ã‡§à ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§≤‡•ã‡§ó ‡§•‡•á ‡§î‡§∞ ‡§Ø‡•á ‡§ó‡§≤‡§§ ‡§™‡§π‡§ö‡§æ‡§® ‡§ï‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡§æ ‡§π‡•à ‡•§\n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[3]\n",
    "correct = df_test['dec_input'].iat[3]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ff4d00c-6d49-4e6e-b4f9-a765967fa1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§â‡§®‡§ï‡•Ä ‡§µ‡•ã ‡§µ‡§æ‡§≤‡•Ä ‡§¨‡§æ‡§§ ‡§≠‡•Ä ‡§Ö‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§∞‡§π‡§§‡•Ä ‡§π‡•à‡§Ç ‡•§\n",
      "Correct Seentence: ‡§â‡§®‡§ï‡•Ä ‡§µ‡•ã ‡§µ‡§æ‡§≤‡•Ä ‡§¨‡§æ‡§§ ‡§≠‡•Ä ‡§Ö‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§∞‡§π‡§§‡•Ä ‡§π‡•à ‡•§ \n",
      "Predicted Sentence: ‡§â‡§®‡§ï‡•Ä ‡§µ‡•ã ‡§µ‡§æ‡§≤‡•Ä ‡§¨‡§æ‡§§ ‡§≠‡•Ä ‡§Ö‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§∞‡§π‡§§‡•Ä ‡§π‡•à ‡•§\n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[4]\n",
    "correct = df_test['dec_input'].iat[4]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ad90991-9465-4991-9562-bd6e88cfe5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§Æ‡•à‡§Ç‡§®‡•á ‡§≤‡§≤‡§ø‡§§ ‡§π‡•Ç‡§Å ‡•§\n",
      "Correct Seentence: ‡§Æ‡•à‡§Ç ‡§≤‡§≤‡§ø‡§§ ‡§π‡•Ç‡§Å ‡•§ \n",
      "Predicted Sentence: ‡§Æ‡•à‡§Ç ‡§≤‡§≤‡§ø‡§§ ‡§π‡•Ç‡§Å ‡•§ \n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[9]\n",
    "correct = df_test['dec_input'].iat[9]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f63ceef-8b22-4af4-85b7-3b8ea69b275b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffee1d-4065-4509-98cf-069cb233593b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1c130-a68b-4bf1-86d9-d677f8ed9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "BLEU = []\n",
    "index = []\n",
    "test_data = df_test.head(10000)\n",
    "np.random.seed(1)\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        #print(act)\n",
    "        #print(pred_s)\n",
    "        b = bleu.sentence_bleu(act,pred_s)\n",
    "        BLEU.append(b)\n",
    "    except:\n",
    "        index.append(ind)\n",
    "        continue\n",
    "print(\"BELU = \", np.mean(BLEU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f19ddaa2-1873-4cab-9974-353a53fdc8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:13,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score =  0.8984656084656084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test.head(10)\n",
    "print(test_data.shape)\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "    \n",
    "        #print(act)\n",
    "        #print(pred_s)\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        #print(b)\n",
    "        GLEU_val_emb.append(b)\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb825de7-9bcd-4d6e-8ba4-433f7ac00ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  1  data point 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [21:58,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  1001  data point 0.9150283140478553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [42:44,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  2001  data point 0.9204379715628371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [1:01:56,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  3001  data point 0.9182989341695741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [1:20:27,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  4001  data point 0.9186494493296361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [1:38:55,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  5001  data point 0.91909503414758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6001it [1:57:23,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  6001  data point 0.918329036146066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7001it [2:16:38,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  7001  data point 0.9192907337567936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8001it [2:35:05,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  8001  data point 0.9201028812376454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9001it [2:53:42,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  9001  data point 0.9205000001846352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [3:12:33,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score =  0.9208625332430048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test.head(10000)\n",
    "print(test_data.shape)\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        GLEU_val_emb.append(b)\n",
    "        if(ind%1000 ==0):\n",
    "            print(\"GELU Score for \",ind+1,\" data point\",np.mean(GLEU_val_emb))\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f776a52-2f84-4b17-a268-62f049e9cc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score =  0.9208625332430048\n"
     ]
    }
   ],
   "source": [
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "17fdf583-24db-48c5-b68e-32a44294e3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [3:10:43,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F0.5 Score =  0.9521005068789202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "FScore_val_emb = []\n",
    "test_data = df_test.head(10000)\n",
    "print(test_data.shape)\n",
    "\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        #act = [str(i.dec_output).split()]\n",
    "        \n",
    "        act = str(i.dec_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0]\n",
    "        text = str(i.enc_input)\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        \n",
    "        #b =sentence_gleu(act,pred)\n",
    "        recall,precision,f_beta = rouge_n(act,pred)\n",
    "        #print(recall,precision,f_beta)\n",
    "        \n",
    "        #print(\"BELU Score\",b)\n",
    "        FScore_val_emb.append(f_beta)\n",
    "    except:\n",
    "        continue\n",
    "print(\"F0.5 Score = \",np.mean(FScore_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1cbfaf65-fdef-498a-be31-5a7ae7ccdbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F0.5 Score =  0.9521005068789202\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"F0.5 Score = \",np.mean(FScore_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970fa4e2-367c-4cd2-ba3d-83042f2ce2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3252f34-628a-4e12-9569-b81228062c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
