{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9c7780-28f4-4676-8620-cfeffba88c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar  3 13:45:04 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.84       Driver Version: 460.84       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 20%   22C    P8     8W / 250W |   5815MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 20%   22C    P8     6W / 250W |  10727MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:07:00.0 Off |                  N/A |\n",
      "| 20%   45C    P2    56W / 250W |  10902MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "| 20%   43C    P2    51W / 250W |   9493MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce GTX 108...  Off  | 00000000:0C:00.0 Off |                  N/A |\n",
      "| 66%   85C    P2   192W / 250W |   6609MiB / 11178MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce GTX 108...  Off  | 00000000:0D:00.0 Off |                  N/A |\n",
      "| 45%   76C    P2   236W / 250W |   6609MiB / 11178MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce GTX 108...  Off  | 00000000:0E:00.0 Off |                  N/A |\n",
      "| 54%   83C    P2   209W / 250W |   6615MiB / 11178MiB |     86%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce GTX 108...  Off  | 00000000:0F:00.0 Off |                  N/A |\n",
      "| 39%   72C    P2   230W / 250W |   5505MiB / 11178MiB |     84%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     13679      C   /usr/bin/python3                 3657MiB |\n",
      "|    0   N/A  N/A     13680      C   /usr/bin/python3                 1711MiB |\n",
      "|    0   N/A  N/A     38504      C   /opt/conda/bin/python             445MiB |\n",
      "|    1   N/A  N/A     28490      C   /usr/bin/python3                10725MiB |\n",
      "|    2   N/A  N/A     14515      C   python3                          2919MiB |\n",
      "|    2   N/A  N/A     30017      C   /usr/bin/python3                 7981MiB |\n",
      "|    3   N/A  N/A     13681      C   /usr/bin/python3                 9491MiB |\n",
      "|    4   N/A  N/A     45458      C   python3                          6607MiB |\n",
      "|    5   N/A  N/A     45970      C   python3                          6607MiB |\n",
      "|    6   N/A  N/A     46611      C   python3                          6613MiB |\n",
      "|    7   N/A  N/A      6774      C   python3                          5503MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1472674-e209-489e-93b9-3d3c8da03a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, \"0\" to  \"7\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "torch.cuda.set_device(0)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(n_gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba40e0c-5fcc-43a9-8d28-47ff9e9870f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "340acc79-3319-47aa-8140-07059948a04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /DATA/gupta92/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "#Set a seed\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7712b672-2f06-47dd-906a-42ab51326a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc22694-eff1-4618-ad9c-8b6110972cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4aaf87c-52e6-4e3d-870c-1955b9afd376",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e4420cd-d086-467d-8384-bc29bb4d91ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('DATA/etoori_train.csv')\n",
    "df.shape\n",
    "#df = df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc84c40-1152-4f97-b749-186365fff1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'enc_input':'input'}, inplace = True)\n",
    "df.rename(columns = {'dec_input':'output'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a0a74f8-b795-47a1-aa0a-e3a293a0165b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>परन्तु वे दोनों उन बातों को ज़्यादा समय तक अप हृदय में गाँठ बनाकर नहीं रखते थे ।</td>\n",
       "      <td>परन्तु वे दोनों उन बातों को ज़्यादा समय तक अपने हृदय में गाँठ बनाकर नहीं रखते थे ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>देश में हिन्दी को विस्थापित कर का षड़यंत्र चल रहा है जो चिंता का विषय है ।</td>\n",
       "      <td>देश में हिन्दी को विस्थापित करने का षड़यंत्र चल रहा है जो चिंता का विषय है ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>तीन साल पहले कातिलाना हमले के प्रकरण में एफआर लग जा के बाद डेरा प्रेमी इस बार दर्ज हुए मुकदमे में सभी अभियुक्तों की गिरफ्तारी की मांग को लेकर आंदोलनरत हैं ।</td>\n",
       "      <td>तीन साल पहले कातिलाना हमले के प्रकरण में एफआर लग जाने के बाद डेरा प्रेमी इस बार दर्ज हुए मुकदमे में सभी अभियुक्तों की गिरफ्तारी की मांग को लेकर आंदोलनरत हैं ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम से सिंगापुर के पेरानांका म्यूजियम में प्रदर्शनी लगाई गई हैं ।</td>\n",
       "      <td>रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम से सिंगापुर के पेरानांका म्यूजियम में प्रदर्शनी लगाई गई है ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>तब तक के लिए हमें विराम ले की अनुमति दीजिए ।</td>\n",
       "      <td>तब तक के लिए हमें विराम लेने की अनुमति दीजिए ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>देव स्थेन तक सड़क बनाने की परियोजना चल रही है ।</td>\n",
       "      <td>देव स्थान तक सड़क बनाने की परियोजना चल रही है ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>वर्ड वाचर व प्रसिद्ध चित्रकार अनूप साह आरएनएस को बताया कि श्वेतकंठ चिलचिल पक्षी का वैज्ञानिक नाम ह्वाइट थ्रोटेट लाफिंग थ्रस है ।</td>\n",
       "      <td>वर्ड वाचर व प्रसिद्ध चित्रकार अनूप साह ने आरएनएस को बताया कि श्वेतकंठ चिलचिल पक्षी का वैज्ञानिक नाम ह्वाइट थ्रोटेट लाफिंग थ्रस है ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>दो दिन पहले ज्ञानजी पोस्ट ठेली जिसमें</td>\n",
       "      <td>दो दिन पहले ज्ञानजी ने पोस्ट ठेली जिसमें</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>दुर्घटना की स्थिति में बीमा कम्पनी द्वारा दी जा वाली राशि निर्धारित कर दी गई है ।</td>\n",
       "      <td>दुर्घटना की स्थिति में बीमा कम्पनी द्वारा दी जाने वाली राशि निर्धारित कर दी गई है ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>देर से जाऊंगा तो दुकां बंद हो जाएंगी ।</td>\n",
       "      <td>देर से जाऊंगा तो दुकानें बंद हो जाएंगी ।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                          input  \\\n",
       "0                                                                              परन्तु वे दोनों उन बातों को ज़्यादा समय तक अप हृदय में गाँठ बनाकर नहीं रखते थे ।   \n",
       "1                                                                                    देश में हिन्दी को विस्थापित कर का षड़यंत्र चल रहा है जो चिंता का विषय है ।   \n",
       "2  तीन साल पहले कातिलाना हमले के प्रकरण में एफआर लग जा के बाद डेरा प्रेमी इस बार दर्ज हुए मुकदमे में सभी अभियुक्तों की गिरफ्तारी की मांग को लेकर आंदोलनरत हैं ।   \n",
       "3                                                    रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम से सिंगापुर के पेरानांका म्यूजियम में प्रदर्शनी लगाई गई हैं ।   \n",
       "4                                                                                                                  तब तक के लिए हमें विराम ले की अनुमति दीजिए ।   \n",
       "5                                                                                                               देव स्थेन तक सड़क बनाने की परियोजना चल रही है ।   \n",
       "6                              वर्ड वाचर व प्रसिद्ध चित्रकार अनूप साह आरएनएस को बताया कि श्वेतकंठ चिलचिल पक्षी का वैज्ञानिक नाम ह्वाइट थ्रोटेट लाफिंग थ्रस है ।   \n",
       "7                                                                                                                         दो दिन पहले ज्ञानजी पोस्ट ठेली जिसमें   \n",
       "8                                                                             दुर्घटना की स्थिति में बीमा कम्पनी द्वारा दी जा वाली राशि निर्धारित कर दी गई है ।   \n",
       "9                                                                                                                        देर से जाऊंगा तो दुकां बंद हो जाएंगी ।   \n",
       "\n",
       "                                                                                                                                                            output  \n",
       "0                                                                              परन्तु वे दोनों उन बातों को ज़्यादा समय तक अपने हृदय में गाँठ बनाकर नहीं रखते थे ।   \n",
       "1                                                                                    देश में हिन्दी को विस्थापित करने का षड़यंत्र चल रहा है जो चिंता का विषय है ।   \n",
       "2  तीन साल पहले कातिलाना हमले के प्रकरण में एफआर लग जाने के बाद डेरा प्रेमी इस बार दर्ज हुए मुकदमे में सभी अभियुक्तों की गिरफ्तारी की मांग को लेकर आंदोलनरत हैं ।   \n",
       "3                                                       रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम से सिंगापुर के पेरानांका म्यूजियम में प्रदर्शनी लगाई गई है ।   \n",
       "4                                                                                                                  तब तक के लिए हमें विराम लेने की अनुमति दीजिए ।   \n",
       "5                                                                                                                 देव स्थान तक सड़क बनाने की परियोजना चल रही है ।   \n",
       "6                             वर्ड वाचर व प्रसिद्ध चित्रकार अनूप साह ने आरएनएस को बताया कि श्वेतकंठ चिलचिल पक्षी का वैज्ञानिक नाम ह्वाइट थ्रोटेट लाफिंग थ्रस है ।   \n",
       "7                                                                                                                         दो दिन पहले ज्ञानजी ने पोस्ट ठेली जिसमें  \n",
       "8                                                                             दुर्घटना की स्थिति में बीमा कम्पनी द्वारा दी जाने वाली राशि निर्धारित कर दी गई है ।   \n",
       "9                                                                                                                        देर से जाऊंगा तो दुकानें बंद हो जाएंगी ।   "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81e7ec2-d0fa-45d6-be30-7c2e2877dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration, T5Tokenizer, \n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "  )\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199d7a2-2b5d-4904-b204-a379c34aefca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "#model_name = 't5-base'\n",
    "# Loaded the trained t5 model (initially trained by pre initializing t5-base) for some epochs \n",
    "#(So as to improve the number of epochs for training)\n",
    "model_name = 't5_gec_hindi'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7056f467-e10c-4c05-b393-a16393afc47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"surajp/RoBERTa-hindi-guj-san\",use_fast=False)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"flax-community/roberta-hindi\",use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aca04df4-d7de-4f44-9800-94eb1c5296ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_token_len(example):\n",
    "    return len(tokenizer(example).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9821b86-5a8b-4956-856c-d4c88d482f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((126000, 2), (14000, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train - Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.10, shuffle=True)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c514352d-1338-4a42-95a6-36151e6b0306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "test_df['input_token_len'] = test_df['input'].apply(calc_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9df2945-fb4d-4f63-b24b-2571dceba51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>input_token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40665</th>\n",
       "      <td>अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसकी वर्चस्व सर्वत्र स्थापित है ।</td>\n",
       "      <td>अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसका वर्चस्व सर्वत्र स्थापित है ।</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48520</th>\n",
       "      <td>तथे गौशाला का</td>\n",
       "      <td>तथा गौशाला का</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138403</th>\n",
       "      <td>बच्चे खेल रहे है किलकारियाँ गूंज रही है ब्रह्माण्ड में</td>\n",
       "      <td>बच्चे खेल रहे हैं किलकारियाँ गूंज रही हैं ब्रह्माण्ड में</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130079</th>\n",
       "      <td>धरने पर किसानो ने कहा कि अब समय आ गया हैं कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी हैं ।</td>\n",
       "      <td>धरने पर किसानो ने कहा कि अब समय आ गया है कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी है ।</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50146</th>\n",
       "      <td>सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा हैं ।</td>\n",
       "      <td>सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा है ।</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        input  \\\n",
       "40665         अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसकी वर्चस्व सर्वत्र स्थापित है ।   \n",
       "48520                                                                                           तथे गौशाला का   \n",
       "138403                                                 बच्चे खेल रहे है किलकारियाँ गूंज रही है ब्रह्माण्ड में   \n",
       "130079  धरने पर किसानो ने कहा कि अब समय आ गया हैं कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी हैं ।   \n",
       "50146                   सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा हैं ।   \n",
       "\n",
       "                                                                                                      output  \\\n",
       "40665       अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसका वर्चस्व सर्वत्र स्थापित है ।    \n",
       "48520                                                                                          तथा गौशाला का   \n",
       "138403                                              बच्चे खेल रहे हैं किलकारियाँ गूंज रही हैं ब्रह्माण्ड में   \n",
       "130079  धरने पर किसानो ने कहा कि अब समय आ गया है कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी है ।    \n",
       "50146                  सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा है ।    \n",
       "\n",
       "        input_token_len  \n",
       "40665                65  \n",
       "48520                12  \n",
       "138403               39  \n",
       "130079               66  \n",
       "50146                57  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd321d92-6f3a-40ee-ba4a-907be30ea244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14000.000000\n",
       "mean        55.171643\n",
       "std         34.661996\n",
       "min          6.000000\n",
       "25%         32.000000\n",
       "50%         48.000000\n",
       "75%         70.000000\n",
       "max        793.000000\n",
       "Name: input_token_len, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['input_token_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5e933a2-ae5f-4c20-87b4-4dbb99b2c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use a token length of 64 since it will cover the vast majority of examples\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ca6fab2-ae06-441f-807e-da87e843249b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'input_token_len', '__index_level_0__'],\n",
       "    num_rows: 14000\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d75fca39-9ccb-4bce-8a14-ca5d573cb60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GrammarDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer,print_text=False):         \n",
    "        self.dataset = dataset\n",
    "        self.pad_to_max_length = False\n",
    "        self.tokenizer = tokenizer\n",
    "        self.print_text = print_text\n",
    "        self.max_len = 64\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "    def tokenize_data(self, example):\n",
    "        input_, target_ = example['input'], example['output']\n",
    "\n",
    "        # tokenize inputs\n",
    "        tokenized_inputs = tokenizer(input_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "    \n",
    "        tokenized_targets = tokenizer(target_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "\n",
    "        inputs={\"input_ids\": tokenized_inputs['input_ids'],\n",
    "            \"attention_mask\": tokenized_inputs['attention_mask'],\n",
    "            \"labels\": tokenized_targets['input_ids']\n",
    "        }\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenize_data(self.dataset[index])\n",
    "        \n",
    "        if self.print_text:\n",
    "            for k in inputs.keys():\n",
    "                print(k, len(inputs[k]))\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b86d1d24-466e-420d-94fd-eeed544aae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids 12\n",
      "attention_mask 12\n",
      "labels 12\n",
      "{'input_ids': [0, 2766, 268, 331, 370, 305, 265, 283, 265, 272, 265, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 2766, 265, 331, 370, 305, 265, 283, 265, 272, 265, 2]}\n"
     ]
    }
   ],
   "source": [
    "dataset = GrammarDataset(test_dataset, tokenizer, True)\n",
    "print(dataset[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc0564-2706-4d95-8dfb-e9970a0e2a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e6bd3fc-b11e-47a1-b473-10705ebe1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Evaluator\n",
    "#!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f50438a6-1ed2-460a-8bb0-307fa7b23806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4df5978-795f-4abf-b80b-18a7848884cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf186c82-6bea-4bb3-b9f3-1b00df72ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model\n",
    "#data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest', return_tensors='pt')\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest', return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b108ae3-2088-44fa-af0f-8b3583bd157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining training related arguments\n",
    "batch_size = 8\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "                output_dir=\"models/hindi/T5_Roberta\",\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                learning_rate=2e-5,\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                per_device_eval_batch_size=batch_size,\n",
    "                weight_decay=0.1,\n",
    "                save_steps=10000,\n",
    "                num_train_epochs=2,\n",
    "                predict_with_generate=True,\n",
    "                fp16=True,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2a430e6-c635-43bb-a305-357ac0773191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfbcd6b1-bdcb-4f9d-a7ca-cd6924d7e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d9743c7-8b58-4bee-9ab1-2fcd8032b17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# defining trainer using 🤗\n",
    "trainer = Seq2SeqTrainer(model=model, \n",
    "                args=args, \n",
    "                train_dataset= GrammarDataset(train_dataset, tokenizer),\n",
    "                eval_dataset=GrammarDataset(test_dataset, tokenizer),\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator)\n",
    "                #compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3432ba00-8cd4-4656-aae6-8b886eb97d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 126000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 31500\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31500' max='31500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31500/31500 2:12:44, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.074561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>0.068154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to models/hindi/T5_Roberta/checkpoint-10000\n",
      "Configuration saved in models/hindi/T5_Roberta/checkpoint-10000/config.json\n",
      "Model weights saved in models/hindi/T5_Roberta/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_Roberta/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_Roberta/checkpoint-10000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hindi/T5_Roberta/checkpoint-20000\n",
      "Configuration saved in models/hindi/T5_Roberta/checkpoint-20000/config.json\n",
      "Model weights saved in models/hindi/T5_Roberta/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_Roberta/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_Roberta/checkpoint-20000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/T5_Roberta/checkpoint-30000\n",
      "Configuration saved in models/hindi/T5_Roberta/checkpoint-30000/config.json\n",
      "Model weights saved in models/hindi/T5_Roberta/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/T5_Roberta/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/T5_Roberta/checkpoint-30000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31500, training_loss=0.08328946831112816, metrics={'train_runtime': 7967.501, 'train_samples_per_second': 31.628, 'train_steps_per_second': 3.954, 'total_flos': 1.907040123482112e+16, 'train_loss': 0.08328946831112816, 'epoch': 2.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Wandb API Key: bcdbdd5ee9d76a20c90b5f2a246eb45f14b341a9\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"     #Disabling Wandb\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed8776-52ca-4d06-9e15-158994267915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch_device = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b68254c3-619e-464f-bfa6-01c21279c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to t5_gec_hindi_Roberta\n",
      "Configuration saved in t5_gec_hindi_Roberta/config.json\n",
      "Model weights saved in t5_gec_hindi_Roberta/pytorch_model.bin\n",
      "tokenizer config file saved in t5_gec_hindi_Roberta/tokenizer_config.json\n",
      "Special tokens file saved in t5_gec_hindi_Roberta/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "#Save the trained Model\n",
    "trainer.save_model('t5_gec_hindi_Roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9669013c-f34f-44b1-a35c-f265f106a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grammar(input_text,num_return_sequences):\n",
    "    batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\").to(device)\n",
    "    translated = model.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2dc10f7d-909a-4d2b-917a-475e43619610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: तथे गौशाला का\n",
      "Correct Seentence: तथा गौशाला का\n",
      "Predicted Sentence: तथा गौशाला का । \n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[1]\n",
    "correct = test_df['output'].iat[1]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "l = len(correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=4)[0]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d77fb00-92ba-4cc0-ba43-7cf0abdbe9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: कुछ विश्लेषकों का मत है कि इस कार्यक्रम के कारण लोग निकम्मे हो रहे है ।\n",
      "Correct Seentence: कुछ विश्लेषकों का मत है कि इस कार्यक्रम के कारण लोग निकम्मे हो रहे हैं । \n",
      "Predicted Sentence: कुछ विश्लेषकों का मत है कि इस कार्यक्रम के कारण लोग निकम्मे हो रहे हैं । \n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[2000]\n",
    "correct = test_df['output'].iat[2000]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "l = len(correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=4)[0][:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11cd1f-b1b8-4c21-badb-c1c384e5dac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1a91539-2f02-4346-8ae4-6de7e17ac9e4",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a58afe1a-4295-4986-8134-ce4a65d8b8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--surajp--RoBERTa-hindi-guj-san/snapshots/4464e6c2a7301ce0b58128b72c679bf6326a4b3b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"surajp/RoBERTa-hindi-guj-san\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--surajp--RoBERTa-hindi-guj-san/snapshots/4464e6c2a7301ce0b58128b72c679bf6326a4b3b/vocab.json\n",
      "loading file merges.txt from cache at /DATA/gupta92/.cache/huggingface/hub/models--surajp--RoBERTa-hindi-guj-san/snapshots/4464e6c2a7301ce0b58128b72c679bf6326a4b3b/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--surajp--RoBERTa-hindi-guj-san/snapshots/4464e6c2a7301ce0b58128b72c679bf6326a4b3b/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--surajp--RoBERTa-hindi-guj-san/snapshots/4464e6c2a7301ce0b58128b72c679bf6326a4b3b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--surajp--RoBERTa-hindi-guj-san/snapshots/4464e6c2a7301ce0b58128b72c679bf6326a4b3b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"surajp/RoBERTa-hindi-guj-san\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--surajp--RoBERTa-hindi-guj-san/snapshots/4464e6c2a7301ce0b58128b72c679bf6326a4b3b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"surajp/RoBERTa-hindi-guj-san\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file t5_gec_hindi_Roberta/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5_gec_hindi\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file t5_gec_hindi_Roberta/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5_gec_hindi_Roberta.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration,AutoTokenizer\n",
    "#Loaded the pretrained model\n",
    "model_name = 't5_gec_hindi_Roberta'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"surajp/RoBERTa-hindi-guj-san\")\n",
    "trained_model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4714ef-a462-4b6f-86e3-d79f8f26453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration,AutoTokenizer\n",
    "model_name = 't5_gec_hindi_Roberta'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"surajp/RoBERTa-hindi-guj-san\")\n",
    "trained_model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265fd17f-dc54-4004-ae7d-263faaf2f334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(30522, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(30522, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(30522, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30522, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe56b4bb-3555-4633-9b4f-9f13e058b3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 845.604MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in trained_model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in trained_model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b10cc66-3a3b-48e7-9bd9-5bf6d171b2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+------------+\n",
      "|                               Modules                                | Parameters |\n",
      "+----------------------------------------------------------------------+------------+\n",
      "|                            shared.weight                             |  23440896  |\n",
      "|            encoder.block.0.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.0.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.0.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.0.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "| encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight |    384     |\n",
      "|              encoder.block.0.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.0.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.0.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.0.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.1.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.1.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.1.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.1.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.1.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.1.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.1.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.1.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.2.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.2.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.2.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.2.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.2.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.2.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.2.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.2.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.3.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.3.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.3.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.3.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.3.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.3.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.3.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.3.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.4.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.4.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.4.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.4.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.4.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.4.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.4.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.4.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.5.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.5.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.5.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.5.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.5.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.5.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.5.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.5.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.6.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.6.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.6.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.6.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.6.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.6.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.6.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.6.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.7.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.7.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.7.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.7.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.7.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.7.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.7.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.7.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.8.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.8.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.8.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.8.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.8.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.8.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.8.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.8.layer.1.layer_norm.weight               |    768     |\n",
      "|            encoder.block.9.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            encoder.block.9.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            encoder.block.9.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            encoder.block.9.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.9.layer.0.layer_norm.weight               |    768     |\n",
      "|           encoder.block.9.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           encoder.block.9.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.9.layer.1.layer_norm.weight               |    768     |\n",
      "|           encoder.block.10.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|           encoder.block.10.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|           encoder.block.10.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|           encoder.block.10.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.10.layer.0.layer_norm.weight              |    768     |\n",
      "|          encoder.block.10.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|          encoder.block.10.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.10.layer.1.layer_norm.weight              |    768     |\n",
      "|           encoder.block.11.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|           encoder.block.11.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|           encoder.block.11.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|           encoder.block.11.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              encoder.block.11.layer.0.layer_norm.weight              |    768     |\n",
      "|          encoder.block.11.layer.1.DenseReluDense.wi.weight           |  2359296   |\n",
      "|          encoder.block.11.layer.1.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              encoder.block.11.layer.1.layer_norm.weight              |    768     |\n",
      "|                   encoder.final_layer_norm.weight                    |    768     |\n",
      "|            decoder.block.0.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.0.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.0.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.0.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "| decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight |    384     |\n",
      "|              decoder.block.0.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.0.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.0.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.0.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.0.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.0.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.0.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.0.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.0.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.1.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.1.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.1.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.1.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.1.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.1.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.1.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.1.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.1.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.1.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.1.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.1.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.1.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.2.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.2.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.2.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.2.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.2.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.2.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.2.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.2.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.2.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.2.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.2.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.2.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.2.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.3.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.3.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.3.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.3.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.3.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.3.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.3.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.3.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.3.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.3.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.3.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.3.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.3.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.4.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.4.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.4.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.4.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.4.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.4.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.4.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.4.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.4.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.4.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.4.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.4.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.4.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.5.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.5.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.5.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.5.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.5.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.5.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.5.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.5.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.5.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.5.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.5.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.5.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.5.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.6.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.6.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.6.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.6.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.6.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.6.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.6.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.6.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.6.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.6.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.6.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.6.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.6.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.7.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.7.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.7.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.7.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.7.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.7.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.7.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.7.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.7.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.7.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.7.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.7.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.7.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.8.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.8.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.8.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.8.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.8.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.8.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.8.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.8.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.8.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.8.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.8.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.8.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.8.layer.2.layer_norm.weight               |    768     |\n",
      "|            decoder.block.9.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|            decoder.block.9.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|            decoder.block.9.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|            decoder.block.9.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.9.layer.0.layer_norm.weight               |    768     |\n",
      "|           decoder.block.9.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|           decoder.block.9.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|           decoder.block.9.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|           decoder.block.9.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.9.layer.1.layer_norm.weight               |    768     |\n",
      "|           decoder.block.9.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|           decoder.block.9.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.9.layer.2.layer_norm.weight               |    768     |\n",
      "|           decoder.block.10.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|           decoder.block.10.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|           decoder.block.10.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|           decoder.block.10.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.10.layer.0.layer_norm.weight              |    768     |\n",
      "|          decoder.block.10.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|          decoder.block.10.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|          decoder.block.10.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|          decoder.block.10.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.10.layer.1.layer_norm.weight              |    768     |\n",
      "|          decoder.block.10.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|          decoder.block.10.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.10.layer.2.layer_norm.weight              |    768     |\n",
      "|           decoder.block.11.layer.0.SelfAttention.q.weight            |   589824   |\n",
      "|           decoder.block.11.layer.0.SelfAttention.k.weight            |   589824   |\n",
      "|           decoder.block.11.layer.0.SelfAttention.v.weight            |   589824   |\n",
      "|           decoder.block.11.layer.0.SelfAttention.o.weight            |   589824   |\n",
      "|              decoder.block.11.layer.0.layer_norm.weight              |    768     |\n",
      "|          decoder.block.11.layer.1.EncDecAttention.q.weight           |   589824   |\n",
      "|          decoder.block.11.layer.1.EncDecAttention.k.weight           |   589824   |\n",
      "|          decoder.block.11.layer.1.EncDecAttention.v.weight           |   589824   |\n",
      "|          decoder.block.11.layer.1.EncDecAttention.o.weight           |   589824   |\n",
      "|              decoder.block.11.layer.1.layer_norm.weight              |    768     |\n",
      "|          decoder.block.11.layer.2.DenseReluDense.wi.weight           |  2359296   |\n",
      "|          decoder.block.11.layer.2.DenseReluDense.wo.weight           |  2359296   |\n",
      "|              decoder.block.11.layer.2.layer_norm.weight              |    768     |\n",
      "|                   decoder.final_layer_norm.weight                    |    768     |\n",
      "+----------------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 221670144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "221670144"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "count_parameters(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e64f8030-6ad4-40fc-87c8-642c2dad0f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b19baa27-25ce-4684-88af-87042ed163b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grammar(input_text,num_return_sequences):\n",
    "    batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\").to(torch_device)\n",
    "    #translated = model1.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    translated = trained_model.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79771c80-b79f-4ac2-8086-4eb0a0d82051",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = test_df['input'].iat[1]\n",
    "correct = test_df['output'].iat[1]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "l = len(correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=4)[0][:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed1ae34f-0274-43ec-afed-f0ce7e723612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसकी वर्चस्व सर्वत्र स्थापित है ।\n",
      "Correct Seentence: अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसका वर्चस्व सर्वत्र स्थापित है । \n",
      "Predicted Sentence: अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थे परंतु आज भी उसकी वर्चस्व सर्वत्र स्थापित ह\n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[0]\n",
    "correct = test_df['output'].iat[0]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "l = len(correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=2)[0]\n",
    "p = len(predicted_s)\n",
    "#if(p>l):\n",
    "    #predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3c96d0d-6d65-42bc-8faa-30a3b931c57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: राष्ट्रीय जनता दल हरियाणा आज महिला आरक्षण विधेयक के विरोध में प्रदेश सचिव विजय सिंह कारगवाल के तृत्व में अंबेडकर पार्क के समीप जोदार प्रदर्शन किया तथा कांग्रेस अध्यक्षा सोनिया गांधी व भाजपा की वरिष्ठ त्री सुषमा स्वराज का पुतला फूंका ।\n",
      "Correct Seentence: राष्ट्रीय जनता दल हरियाणा ने आज महिला आरक्षण विधेयक के विरोध में प्रदेश सचिव विजय सिंह कारगवाल के नेतृत्व में अंबेडकर पार्क के समीप जोदार प्रदर्शन किया तथा कांग्रेस अध्यक्षा सोनिया गांधी व भाजपा की वरिष्ठ नेत्री सुषमा स्वराज का पुतला फूंका । \n",
      "Predicted Sentence: राष्ट्रीय जनता दल ने हरियाणा आज महिला आरक्षण विधेयक के विरोध में प्रदेश सचिव विजय सिंह कारगव\n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[50]\n",
    "correct = test_df['output'].iat[50]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f05b016-c983-4167-a2f7-c5f1736de779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ज़ाहिर है जब मैंने लिखूंगा तो वो मेरे ही व्यक्तिगत विचार कहलायेंगे किसी संस्थान के नहीं ।\n",
      "Correct Seentence: ज़ाहिर है जब मैं लिखूंगा तो वो मेरे ही व्यक्तिगत विचार कहलायेंगे किसी संस्थान के नहीं । \n",
      "Predicted Sentence: ज़ाहिर है जब मैं लिखूंगा तो वो मेरे ही व्यक्तिगत विचार कहलायेंगे किसी संस्थान के नहीं ।\n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[60]\n",
    "correct = test_df['output'].iat[60]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d488748e-8345-4b94-baf3-84925a29ba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: गनीमत हैं गणेशशंकर विद्यार्थी ने २०० रुपये का मनीआर्डर भेजकर अशफ़ाक़ की मजार पर छत डलवा कर उसे धूप के साये से बचा लिया ।\n",
      "Correct Seentence: गनीमत है गणेशशंकर विद्यार्थी ने २०० रुपये का मनीआर्डर भेजकर अशफ़ाक़ की मजार पर छत डलवा कर उसे धूप के साये से बचा लिया । \n",
      "Predicted Sentence: गनीमत है गणेशशंकर विद्यार्थी ने २०० रुपये का मनीआर्डर भेजकर अशफ़ाक़ की मजार पर छत डलवा कर उसे धूप क\n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[51]\n",
    "correct = test_df['output'].iat[51]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2adc94a-4fc3-4eb6-b336-574d99c230eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: हम पास खींच लेते हैंं ।\n",
      "Correct Seentence: हम पास खींच लेते हैं । \n",
      "Predicted Sentence: हम पास खींच लेते हैं । \n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[20]\n",
    "correct = test_df['output'].iat[20]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b37d9f-25df-483f-a3c3-29603e854408",
   "metadata": {},
   "source": [
    "# Performance Analysis\n",
    "1. BLEU Score\n",
    "2. GLEU Score\n",
    "3. F0.5 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18317b87-03d3-4d73-ae22-9cf8cb8e1db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da058ef4-524c-4808-a1ba-bbd143f534d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>input_token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40665</th>\n",
       "      <td>अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसकी वर्चस्व सर्वत्र स्थापित है ।</td>\n",
       "      <td>अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसका वर्चस्व सर्वत्र स्थापित है ।</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48520</th>\n",
       "      <td>तथे गौशाला का</td>\n",
       "      <td>तथा गौशाला का</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138403</th>\n",
       "      <td>बच्चे खेल रहे है किलकारियाँ गूंज रही है ब्रह्माण्ड में</td>\n",
       "      <td>बच्चे खेल रहे हैं किलकारियाँ गूंज रही हैं ब्रह्माण्ड में</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130079</th>\n",
       "      <td>धरने पर किसानो ने कहा कि अब समय आ गया हैं कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी हैं ।</td>\n",
       "      <td>धरने पर किसानो ने कहा कि अब समय आ गया है कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी है ।</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50146</th>\n",
       "      <td>सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा हैं ।</td>\n",
       "      <td>सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा है ।</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        input  \\\n",
       "40665         अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसकी वर्चस्व सर्वत्र स्थापित है ।   \n",
       "48520                                                                                           तथे गौशाला का   \n",
       "138403                                                 बच्चे खेल रहे है किलकारियाँ गूंज रही है ब्रह्माण्ड में   \n",
       "130079  धरने पर किसानो ने कहा कि अब समय आ गया हैं कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी हैं ।   \n",
       "50146                   सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा हैं ।   \n",
       "\n",
       "                                                                                                      output  \\\n",
       "40665       अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसका वर्चस्व सर्वत्र स्थापित है ।    \n",
       "48520                                                                                          तथा गौशाला का   \n",
       "138403                                              बच्चे खेल रहे हैं किलकारियाँ गूंज रही हैं ब्रह्माण्ड में   \n",
       "130079  धरने पर किसानो ने कहा कि अब समय आ गया है कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी है ।    \n",
       "50146                  सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा है ।    \n",
       "\n",
       "        input_token_len  \n",
       "40665                65  \n",
       "48520                12  \n",
       "138403               39  \n",
       "130079               66  \n",
       "50146                57  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c922895e-82d6-43db-a2c8-113bacd42372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv('DATA/etoori_test.csv')\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab40e4eb-00a5-429e-a837-ed75ea61cc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरार...</td>\n",
       "      <td>इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरार...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>यह मन को काबू में करने वाली मुद्रा हैं इसीलिए ...</td>\n",
       "      <td>यह मन को काबू में करने वाली मुद्रा है इसीलिए इ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा उसनेाँ...</td>\n",
       "      <td>आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा वहाँ प...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>परिवार के मुताबिक धमाकों में हिस्सा लेने वाले ...</td>\n",
       "      <td>परिवार के मुताबिक धमाकों में हिस्सा लेने वाले ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>उनकी वो वाली बात भी अनिश्चित रहती हैं ।</td>\n",
       "      <td>उनकी वो वाली बात भी अनिश्चित रहती है ।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           enc_input  \\\n",
       "0  इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरार...   \n",
       "1  यह मन को काबू में करने वाली मुद्रा हैं इसीलिए ...   \n",
       "2  आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा उसनेाँ...   \n",
       "3  परिवार के मुताबिक धमाकों में हिस्सा लेने वाले ...   \n",
       "4            उनकी वो वाली बात भी अनिश्चित रहती हैं ।   \n",
       "\n",
       "                                           dec_input  \n",
       "0  इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरार...  \n",
       "1  यह मन को काबू में करने वाली मुद्रा है इसीलिए इ...  \n",
       "2  आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा वहाँ प...  \n",
       "3  परिवार के मुताबिक धमाकों में हिस्सा लेने वाले ...  \n",
       "4            उनकी वो वाली बात भी अनिश्चित रहती है ।   "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9216427e-77e4-4d58-a4c1-0fd582745e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरारी कार भी को मिलियन डॉलर क्लब में माना जा रहा है ।\n",
      "Correct Seentence: इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरारी कार को भी मिलियन डॉलर क्लब में माना जा रहा है । \n",
      "Predicted Sentence: इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरारी कार को भी मिलियन डॉलर क्लब में माना जा रहा है ।\n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[0]\n",
    "correct = df_test['dec_input'].iat[0]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d8e9a12-fa0b-422c-bbf3-c4957646dd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.27 s, sys: 13.6 ms, total: 1.28 s\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2a81787-9b52-40b6-ac8f-db42e39d46ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरारी कार को भी मिलियन डॉलर क्लब में माना जा रहा है ।'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba007951-7c5e-4d99-8ca6-e9a4aa81d026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: यह मन को काबू में करने वाली मुद्रा हैं इसीलिए इसे चित्त हस्त मुद्रा योग कहते हैंं ।\n",
      "Correct Seentence: यह मन को काबू में करने वाली मुद्रा है इसीलिए इसे चित्त हस्त मुद्रा योग कहते हैं । \n",
      "Predicted Sentence: यह मन को काबू में करने वाली मुद्रा है इसीलिए इसे चित्त हस्त मुद्रा योग कहते हैं । \n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[1]\n",
    "correct = df_test['dec_input'].iat[1]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c0c0e87-c89e-4143-bacf-d6fe15b9cca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा उसनेाँ पर शोर मचा रहा है ।\n",
      "Correct Seentence: आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा वहाँ पर शोर मचा रहा है । \n",
      "Predicted Sentence: आप पुस्तक पढ़ने में तल्लीन है और बच्चा वहाँ पर शोर मचा रहा है ।  \n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[2]\n",
    "correct = df_test['dec_input'].iat[2]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f233a47-440e-49b7-ba4f-b952be54da5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: परिवार के मुताबिक धमाकों में हिस्सा लेने वाले कोई दूसरे लोग थे और ये गलत पहचान का मामला हैं ।\n",
      "Correct Seentence: परिवार के मुताबिक धमाकों में हिस्सा लेने वाले कोई दूसरे लोग थे और ये गलत पहचान का मामला है । \n",
      "Predicted Sentence: परिवार के मुताबिक धमाकों में हिस्सा लेने वाले कोई दूसरे लोग था और ये गलत पहचान का मामला है ।\n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[3]\n",
    "correct = df_test['dec_input'].iat[3]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ff4d00c-6d49-4e6e-b4f9-a765967fa1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: उनकी वो वाली बात भी अनिश्चित रहती हैं ।\n",
      "Correct Seentence: उनकी वो वाली बात भी अनिश्चित रहती है । \n",
      "Predicted Sentence: उनकी वो वाली बात भी अनिश्चित रहती है । \n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[4]\n",
    "correct = df_test['dec_input'].iat[4]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0ad90991-9465-4991-9562-bd6e88cfe5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: मैंने ललित हूँ ।\n",
      "Correct Seentence: मैं ललित हूँ । \n",
      "Predicted Sentence: मैं ललित हूँ । \n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[9]\n",
    "correct = df_test['dec_input'].iat[9]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f63ceef-8b22-4af4-85b7-3b8ea69b275b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffee1d-4065-4509-98cf-069cb233593b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f19ddaa2-1873-4cab-9974-353a53fdc8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:08,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score =  0.8957280520863231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "GLEU_val_emb = []\n",
    "count=0\n",
    "test_data = df_test.head(100)\n",
    "print(test_data.shape)\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "    \n",
    "        #print(act)\n",
    "        #print(pred_s)\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        if(p<l-4):\n",
    "            count+=1\n",
    "        else:\n",
    "        #print(b)\n",
    "            GLEU_val_emb.append(b)\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e9b9918-7473-4f71-8233-5ddca096aeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with less length: 23\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence with less length:\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "38e5cd8d-4865-406a-b8d4-83cdc2561a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849\n",
      "82.1458\n"
     ]
    }
   ],
   "source": [
    "print(df_test.enc_input.str.len().max())\n",
    "print(df_test.enc_input.str.len().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e8a9e8b5-16dc-4fa7-ac03-db54c2764a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरारी कार भी को मिलियन डॉलर क्लब में माना जा रहा है ।</td>\n",
       "      <td>इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरारी कार को भी मिलियन डॉलर क्लब में माना जा रहा है ।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         enc_input  \\\n",
       "0  इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरारी कार भी को मिलियन डॉलर क्लब में माना जा रहा है ।   \n",
       "\n",
       "                                                                                          dec_input  \n",
       "0  इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरारी कार को भी मिलियन डॉलर क्लब में माना जा रहा है ।   "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf5248a2-ccf1-4a42-a3c5-d13d6606eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing if GLEU Score is better for short length sentences or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cb825de7-9bcd-4d6e-8ba4-433f7ac00ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  1  data point 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [22:23,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  1001  data point 0.899760730167574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [45:10,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  2001  data point 0.9046304633555541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [1:07:52,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  3001  data point 0.9032695146899072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [1:30:54,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  4001  data point 0.9018055737180609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [1:54:07,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  5001  data point 0.9013977158780253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6001it [2:16:58,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  6001  data point 0.9001001998697103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7001it [2:39:32,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  7001  data point 0.9008656834289221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8001it [3:03:36,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  8001  data point 0.9016253094710289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9001it [3:26:10,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  9001  data point 0.9010568912145879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [3:48:21,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score =  0.9010645312331664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test.head(10000)\n",
    "count = 0\n",
    "index_sentence=[]\n",
    "print(test_data.shape)\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "        \n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        if(p<l-4):\n",
    "            count+=1\n",
    "            index_sentence.append(ind)\n",
    "        else:\n",
    "        #print(b)\n",
    "            GLEU_val_emb.append(b)\n",
    "            \n",
    "        #GLEU_val_emb.append(b)\n",
    "        if(ind%1000 ==0):\n",
    "            print(\"GELU Score for \",ind+1,\" data point\",np.mean(GLEU_val_emb))\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76abbdcc-7350-49d1-9a30-c334a6be3e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing GLEU Score for all the senetences in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4aec47b0-1ce7-4028-b1a9-e62a28c65215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  1  data point 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [19:33,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  1001  data point 0.8216028998421867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [38:30,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  2001  data point 0.8243468127438326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [57:32,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  3001  data point 0.8224608203188657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [1:16:12,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  4001  data point 0.8191685773976708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [1:34:34,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  5001  data point 0.8194787581434808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6001it [1:52:33,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  6001  data point 0.8182951207182956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7001it [2:10:05,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  7001  data point 0.8176321677456636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8001it [2:27:34,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  8001  data point 0.818803948107944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9001it [2:45:30,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  9001  data point 0.8190208595040448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [3:03:04,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score =  0.8201520843345623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test.head(10000)\n",
    "count = 0\n",
    "index_sentence=[]\n",
    "print(test_data.shape)\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "        \n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        '''\n",
    "        if(p<l-4):\n",
    "            count+=1\n",
    "            index_sentence.append(ind)\n",
    "        else:\n",
    "        #print(b)\n",
    "            GLEU_val_emb.append(b)\n",
    "        '''\n",
    "            \n",
    "        GLEU_val_emb.append(b)\n",
    "        if(ind%1000 ==0):\n",
    "            print(\"GELU Score for \",ind+1,\" data point\",np.mean(GLEU_val_emb))\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a59215-b3ea-4482-bc7f-7a3603261817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9a09e561-2251-4063-aa02-5257416d3be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: इस दिन बुक फेयर में आ वाले बच्चों को रिफ्रेशमेंट भी मिलेगी ।\n",
      "Correct Seentence: इस दिन बुक फेयर में आने वाले बच्चों को रिफ्रेशमेंट भी मिलेगी । \n",
      "Predicted Sentence: इस दिन बुक फेयर में आने वाले बच्चों को रिफ्रेशमेंटने भी मिलेगी ।\n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[173]\n",
    "correct = df_test['dec_input'].iat[173]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l+1]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d14066d7-8ffb-4497-809e-5c675caf677b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: महानगर में बन वाले फ्लाई ओवर्स और आरओबी में अब साइकिल ट्रैक बनाना अनिवार्य होगा ।\n",
      "Correct Seentence: महानगर में बनने वाले फ्लाई ओवर्स और आरओबी में अब साइकिल ट्रैक बनाना अनिवार्य होगा । \n",
      "Predicted Sentence: महानगर में बनने वाले फ्लाई ओवर्स और आरओबी में अब साइकिल ट्रैक बनाना अनिवार्य होगा । \n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[113]\n",
    "correct = df_test['dec_input'].iat[113]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l+1]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dac45c9f-bd8e-4c8e-b7bf-3a859754f552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: नानाजी देशमुख तो उनके दाएं हाथ थी ।\n",
      "Correct Seentence: नानाजी देशमुख तो उनके दाएं हाथ थे । \n",
      "Predicted Sentence: नानाजी देशमुख तो उनके दाएं हाथ थे ।  \n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[185]\n",
    "correct = df_test['dec_input'].iat[185]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l+1]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a308ee-3bc7-40a4-b8a8-8219db025bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a809e7f2-c13c-41de-b83e-dbd81e01c0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "def5dd54-5d45-47cc-8ef7-7bc751f5875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the index of sentences which are giving good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bfdc026d-f441-4278-8f65-6c5938877fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870\n",
      "[10, 11, 12, 13, 14, 16, 21, 40, 41, 49, 57, 58, 59, 60, 61, 65, 68, 69, 76, 78, 80, 89, 97, 106, 107, 108, 109, 112, 116, 118, 122, 124, 127, 128, 137, 138, 140, 146, 150, 151, 156, 157, 167, 168, 171, 174, 176, 177, 183, 186, 187, 188, 189, 190, 194, 195, 197, 198, 199, 208, 217, 232, 236, 246, 248, 250, 255, 256, 262, 264, 266, 268, 272, 273, 275, 283, 291, 292, 294, 296, 298, 299, 305, 309, 311, 315, 316, 318, 326, 337, 341, 342, 345, 348, 350, 352, 355, 356, 357, 358, 359, 365, 366, 368, 378, 381, 389, 390, 395, 398, 400, 401, 402, 410, 413, 414, 415, 431, 437, 445, 446, 451, 455, 458, 461, 462, 463, 465, 469, 470, 472, 476, 480, 483, 484, 485, 492, 499, 500, 501, 504, 510, 513, 514, 518, 520, 526, 531, 542, 543, 550, 557, 559, 561, 563, 564, 565, 566, 574, 594, 596, 597, 600, 601, 602, 603, 610, 612, 613, 614, 616, 621, 622, 631, 632, 635, 637, 638, 640, 645, 646, 647, 649, 655, 656, 657, 658, 659, 660, 661, 662, 666, 668, 670, 671, 679, 684, 685, 691, 692, 711, 712, 713, 714, 721, 722, 727, 733, 749, 752, 765, 767, 769, 773, 777, 778, 786, 793, 794, 801, 804, 808, 809, 814, 815, 816, 818, 819, 820, 821, 822, 823, 831, 836, 838, 842, 845, 846, 849, 855, 858, 859, 860, 862, 863, 866, 870, 876, 877, 878, 884, 888, 891, 892, 895, 899, 907, 908, 910, 923, 924, 929, 934, 935, 939, 942, 943, 949, 956, 962, 965, 969, 970, 975, 976, 979, 984, 987, 989, 1005, 1010, 1014, 1015, 1020, 1022, 1026, 1027, 1037, 1038, 1041, 1044, 1045, 1048, 1051, 1058, 1059, 1065, 1070, 1071, 1074, 1075, 1077, 1085, 1092, 1094, 1096, 1103, 1107, 1111, 1112, 1115, 1116, 1120, 1121, 1122, 1124, 1125, 1134, 1138, 1142, 1143, 1144, 1148, 1152, 1155, 1163, 1165, 1166, 1167, 1179, 1183, 1185, 1186, 1187, 1193, 1194, 1196, 1199, 1203, 1207, 1210, 1214, 1219, 1224, 1227, 1228, 1235, 1246, 1249, 1250, 1262, 1264, 1265, 1268, 1271, 1273, 1276, 1287, 1288, 1289, 1293, 1296, 1301, 1306, 1310, 1314, 1316, 1318, 1323, 1324, 1325, 1327, 1331, 1338, 1340, 1342, 1343, 1355, 1361, 1364, 1365, 1370, 1374, 1377, 1382, 1383, 1384, 1387, 1388, 1392, 1395, 1398, 1407, 1409, 1413, 1414, 1425, 1432, 1437, 1440, 1441, 1445, 1459, 1461, 1462, 1463, 1469, 1470, 1474, 1480, 1484, 1487, 1496, 1504, 1506, 1508, 1509, 1515, 1517, 1518, 1520, 1521, 1523, 1525, 1528, 1529, 1534, 1536, 1543, 1547, 1550, 1553, 1554, 1555, 1560, 1565, 1566, 1575, 1576, 1577, 1579, 1584, 1585, 1587, 1591, 1594, 1597, 1598, 1599, 1600, 1607, 1611, 1615, 1618, 1620, 1624, 1627, 1629, 1630, 1632, 1633, 1635, 1637, 1640, 1644, 1646, 1647, 1650, 1651, 1654, 1659, 1660, 1666, 1667, 1677, 1683, 1685, 1686, 1688, 1689, 1694, 1695, 1697, 1698, 1699, 1706, 1707, 1708, 1709, 1710, 1711, 1713, 1718, 1721, 1725, 1734, 1737, 1741, 1743, 1760, 1770, 1772, 1777, 1785, 1790, 1807, 1808, 1818, 1823, 1829, 1837, 1841, 1842, 1843, 1845, 1846, 1851, 1853, 1856, 1857, 1862, 1863, 1867, 1872, 1873, 1887, 1888, 1889, 1891, 1892, 1894, 1896, 1897, 1899, 1902, 1906, 1908, 1910, 1911, 1916, 1918, 1920, 1923, 1926, 1930, 1932, 1935, 1936, 1938, 1942, 1949, 1950, 1951, 1952, 1953, 1954, 1964, 1965, 1967, 1973, 1976, 1977, 1979, 1980, 1983, 1986, 1995, 1997, 1998, 1999, 2000, 2006, 2009, 2011, 2015, 2019, 2022, 2024, 2025, 2029, 2040, 2043, 2045, 2046, 2047, 2051, 2052, 2054, 2057, 2058, 2062, 2064, 2065, 2066, 2067, 2068, 2076, 2078, 2080, 2088, 2089, 2093, 2094, 2096, 2103, 2107, 2108, 2110, 2111, 2112, 2114, 2116, 2117, 2121, 2126, 2144, 2146, 2147, 2155, 2157, 2164, 2167, 2173, 2175, 2176, 2177, 2178, 2184, 2185, 2192, 2193, 2194, 2198, 2199, 2210, 2211, 2217, 2220, 2221, 2232, 2234, 2237, 2238, 2241, 2242, 2243, 2248, 2249, 2250, 2254, 2256, 2259, 2261, 2267, 2270, 2276, 2277, 2281, 2283, 2286, 2293, 2296, 2297, 2300, 2305, 2311, 2315, 2316, 2317, 2324, 2325, 2330, 2333, 2337, 2339, 2340, 2348, 2352, 2354, 2355, 2356, 2363, 2369, 2378, 2380, 2381, 2382, 2383, 2385, 2388, 2391, 2397, 2399, 2402, 2404, 2406, 2410, 2412, 2416, 2426, 2429, 2430, 2431, 2432, 2434, 2437, 2440, 2441, 2445, 2447, 2450, 2453, 2459, 2462, 2464, 2466, 2472, 2473, 2483, 2486, 2487, 2496, 2499, 2501, 2503, 2507, 2509, 2516, 2525, 2526, 2533, 2534, 2535, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2550, 2552, 2554, 2556, 2559, 2561, 2565, 2568, 2570, 2572, 2575, 2576, 2578, 2579, 2580, 2582, 2592, 2600, 2604, 2605, 2611, 2614, 2617, 2619, 2627, 2628, 2629, 2633, 2634, 2638, 2648, 2655, 2658, 2659, 2667, 2669, 2674, 2678, 2680, 2689, 2694, 2698, 2700, 2703, 2709, 2711, 2713, 2714, 2725, 2728, 2733, 2734, 2735, 2742, 2749, 2750, 2751, 2755, 2757, 2758, 2763, 2764, 2776, 2780, 2784, 2790, 2791, 2793, 2795, 2796, 2797, 2799, 2801, 2804, 2807, 2809, 2814, 2815, 2817, 2823, 2824, 2829, 2830, 2832, 2835, 2836, 2838, 2840, 2849, 2850, 2851, 2856, 2859, 2862, 2869, 2870, 2876, 2881, 2898, 2903, 2906, 2907, 2908, 2910, 2912, 2921, 2922, 2930, 2934, 2937, 2940, 2947, 2949, 2951, 2952, 2956, 2959, 2961, 2962, 2966, 2968, 2969, 2970, 2973, 2976, 2977, 2980, 2984, 2989, 2994, 3002, 3009, 3014, 3018, 3020, 3021, 3022, 3026, 3047, 3048, 3049, 3051, 3052, 3053, 3055, 3057, 3059, 3071, 3072, 3073, 3077, 3078, 3081, 3082, 3083, 3085, 3086, 3087, 3090, 3097, 3101, 3102, 3106, 3111, 3114, 3117, 3119, 3121, 3122, 3123, 3125, 3129, 3133, 3135, 3140, 3142, 3146, 3148, 3149, 3152, 3155, 3157, 3158, 3160, 3162, 3163, 3167, 3176, 3177, 3179, 3180, 3182, 3193, 3200, 3202, 3208, 3211, 3218, 3219, 3223, 3225, 3228, 3235, 3240, 3242, 3244, 3245, 3255, 3256, 3263, 3266, 3272, 3275, 3276, 3277, 3279, 3283, 3285, 3288, 3289, 3294, 3295, 3297, 3299, 3302, 3303, 3307, 3308, 3311, 3320, 3325, 3331, 3339, 3342, 3343, 3344, 3350, 3352, 3354, 3356, 3357, 3361, 3368, 3370, 3382, 3384, 3391, 3393, 3396, 3399, 3404, 3405, 3408, 3409, 3412, 3415, 3416, 3419, 3427, 3429, 3430, 3432, 3435, 3436, 3442, 3444, 3458, 3463, 3465, 3467, 3468, 3469, 3472, 3473, 3474, 3475, 3477, 3480, 3484, 3485, 3490, 3491, 3499, 3504, 3507, 3508, 3514, 3517, 3519, 3524, 3525, 3530, 3531, 3532, 3533, 3534, 3541, 3543, 3546, 3548, 3560, 3562, 3563, 3564, 3567, 3568, 3571, 3577, 3579, 3583, 3586, 3592, 3595, 3599, 3604, 3614, 3627, 3633, 3634, 3643, 3644, 3653, 3656, 3663, 3671, 3672, 3675, 3690, 3695, 3698, 3700, 3701, 3705, 3711, 3712, 3719, 3721, 3724, 3728, 3729, 3732, 3733, 3738, 3743, 3745, 3750, 3755, 3756, 3760, 3761, 3764, 3765, 3769, 3772, 3783, 3784, 3786, 3795, 3812, 3815, 3818, 3819, 3824, 3827, 3832, 3841, 3842, 3844, 3845, 3848, 3849, 3852, 3855, 3863, 3864, 3866, 3876, 3877, 3879, 3885, 3887, 3888, 3891, 3892, 3894, 3899, 3900, 3903, 3907, 3908, 3911, 3912, 3913, 3914, 3915, 3917, 3919, 3921, 3929, 3934, 3935, 3937, 3940, 3941, 3943, 3944, 3951, 3952, 3953, 3955, 3956, 3957, 3959, 3960, 3961, 3966, 3968, 3969, 3973, 3977, 3980, 3981, 3984, 3989, 3991, 3993, 3995, 3997, 4000, 4005, 4013, 4016, 4018, 4019, 4021, 4023, 4024, 4029, 4031, 4036, 4037, 4038, 4048, 4061, 4064, 4067, 4072, 4074, 4075, 4078, 4079, 4083, 4085, 4097, 4098, 4099, 4100, 4102, 4104, 4109, 4114, 4115, 4116, 4124, 4126, 4128, 4136, 4137, 4138, 4139, 4140, 4142, 4144, 4149, 4152, 4163, 4164, 4168, 4169, 4170, 4172, 4173, 4186, 4187, 4188, 4190, 4194, 4195, 4197, 4199, 4208, 4210, 4213, 4214, 4218, 4220, 4221, 4225, 4227, 4229, 4230, 4231, 4234, 4235, 4236, 4237, 4240, 4245, 4248, 4249, 4252, 4255, 4256, 4260, 4266, 4274, 4280, 4285, 4286, 4288, 4290, 4292, 4294, 4297, 4302, 4303, 4305, 4306, 4308, 4309, 4310, 4318, 4319, 4323, 4327, 4328, 4330, 4332, 4334, 4341, 4349, 4358, 4360, 4365, 4366, 4371, 4374, 4378, 4380, 4381, 4382, 4386, 4387, 4393, 4394, 4397, 4402, 4407, 4408, 4417, 4419, 4433, 4448, 4453, 4454, 4455, 4460, 4461, 4466, 4468, 4472, 4480, 4485, 4486, 4487, 4489, 4491, 4492, 4494, 4498, 4500, 4502, 4511, 4521, 4525, 4537, 4544, 4545, 4548, 4550, 4551, 4558, 4563, 4566, 4569, 4571, 4575, 4576, 4578, 4583, 4587, 4595, 4598, 4602, 4605, 4610, 4611, 4615, 4618, 4620, 4621, 4625, 4629, 4630, 4632, 4635, 4638, 4639, 4640, 4647, 4648, 4652, 4656, 4663, 4667, 4669, 4674, 4676, 4682, 4690, 4692, 4695, 4700, 4704, 4705, 4706, 4708, 4709, 4716, 4717, 4720, 4721, 4723, 4726, 4728, 4734, 4753, 4756, 4758, 4761, 4762, 4766, 4768, 4770, 4771, 4776, 4779, 4781, 4787, 4788, 4790, 4796, 4797, 4798, 4803, 4805, 4806, 4810, 4811, 4821, 4824, 4825, 4827, 4828, 4829, 4833, 4836, 4840, 4848, 4850, 4861, 4870, 4882, 4884, 4894, 4897, 4899, 4901, 4903, 4905, 4908, 4909, 4911, 4912, 4914, 4916, 4921, 4926, 4927, 4931, 4938, 4940, 4943, 4946, 4955, 4962, 4970, 4972, 4974, 4976, 4978, 4979, 4983, 4985, 4987, 4988, 4994, 5000, 5002, 5005, 5007, 5009, 5010, 5020, 5021, 5022, 5023, 5024, 5026, 5027, 5029, 5031, 5036, 5041, 5042, 5045, 5046, 5047, 5048, 5053, 5054, 5062, 5063, 5065, 5070, 5071, 5072, 5080, 5082, 5084, 5086, 5095, 5096, 5097, 5099, 5100, 5101, 5104, 5107, 5108, 5112, 5113, 5115, 5123, 5124, 5125, 5132, 5137, 5138, 5139, 5140, 5141, 5142, 5148, 5149, 5150, 5152, 5157, 5159, 5160, 5161, 5162, 5165, 5169, 5172, 5178, 5179, 5181, 5183, 5188, 5190, 5192, 5194, 5195, 5205, 5212, 5217, 5218, 5219, 5221, 5227, 5229, 5232, 5238, 5250, 5251, 5252, 5269, 5271, 5273, 5274, 5275, 5280, 5281, 5283, 5285, 5287, 5291, 5293, 5302, 5304, 5306, 5307, 5309, 5310, 5312, 5315, 5318, 5322, 5330, 5333, 5335, 5338, 5356, 5359, 5360, 5364, 5366, 5371, 5374, 5379, 5382, 5383, 5384, 5386, 5387, 5389, 5393, 5399, 5402, 5405, 5406, 5408, 5411, 5412, 5413, 5418, 5419, 5423, 5425, 5429, 5430, 5433, 5435, 5437, 5441, 5443, 5450, 5452, 5454, 5455, 5457, 5459, 5461, 5464, 5465, 5466, 5485, 5486, 5487, 5493, 5501, 5504, 5507, 5510, 5521, 5525, 5528, 5535, 5550, 5553, 5554, 5555, 5557, 5558, 5563, 5564, 5571, 5573, 5574, 5592, 5593, 5596, 5597, 5600, 5605, 5607, 5609, 5614, 5618, 5622, 5626, 5627, 5629, 5639, 5643, 5645, 5653, 5655, 5659, 5666, 5669, 5670, 5672, 5673, 5675, 5677, 5678, 5681, 5692, 5694, 5704, 5707, 5711, 5717, 5719, 5720, 5723, 5724, 5725, 5726, 5732, 5733, 5735, 5737, 5742, 5745, 5748, 5752, 5759, 5760, 5761, 5765, 5766, 5775, 5778, 5779, 5785, 5786, 5790, 5794, 5796, 5799, 5804, 5805, 5807, 5809, 5813, 5815, 5817, 5819, 5822, 5823, 5824, 5828, 5831, 5833, 5834, 5836, 5838, 5840, 5845, 5848, 5852, 5854, 5857, 5871, 5875, 5882, 5883, 5888, 5907, 5913, 5914, 5915, 5918, 5925, 5929, 5931, 5937, 5939, 5944, 5945, 5946, 5949, 5959, 5965, 5966, 5968, 5970, 5972, 5978, 5980, 5991, 5994, 5998, 5999, 6000, 6003, 6004, 6005, 6010, 6013, 6014, 6015, 6018, 6022, 6025, 6032, 6036, 6040, 6043, 6044, 6046, 6047, 6050, 6054, 6060, 6064, 6066, 6067, 6073, 6074, 6075, 6076, 6079, 6083, 6084, 6090, 6093, 6096, 6097, 6102, 6105, 6111, 6114, 6116, 6121, 6125, 6126, 6127, 6129, 6131, 6132, 6133, 6137, 6139, 6147, 6149, 6150, 6154, 6155, 6156, 6158, 6159, 6162, 6166, 6172, 6178, 6180, 6189, 6190, 6192, 6193, 6194, 6199, 6201, 6204, 6205, 6206, 6208, 6215, 6218, 6219, 6220, 6224, 6230, 6232, 6240, 6241, 6250, 6254, 6256, 6260, 6265, 6270, 6273, 6275, 6282, 6284, 6286, 6288, 6297, 6304, 6305, 6312, 6316, 6318, 6320, 6321, 6328, 6330, 6336, 6337, 6340, 6341, 6343, 6345, 6347, 6350, 6354, 6355, 6357, 6360, 6361, 6362, 6371, 6373, 6381, 6388, 6390, 6391, 6402, 6403, 6404, 6406, 6413, 6414, 6417, 6418, 6419, 6420, 6421, 6428, 6430, 6432, 6435, 6436, 6440, 6444, 6449, 6452, 6453, 6455, 6458, 6460, 6461, 6462, 6465, 6468, 6475, 6489, 6494, 6495, 6496, 6498, 6500, 6502, 6504, 6507, 6508, 6511, 6513, 6515, 6516, 6525, 6527, 6529, 6530, 6533, 6534, 6538, 6539, 6540, 6541, 6549, 6550, 6551, 6552, 6557, 6558, 6559, 6562, 6566, 6572, 6574, 6579, 6582, 6587, 6592, 6594, 6601, 6605, 6609, 6610, 6613, 6616, 6630, 6637, 6638, 6640, 6642, 6646, 6664, 6667, 6668, 6669, 6671, 6673, 6676, 6679, 6682, 6688, 6689, 6692, 6694, 6707, 6708, 6710, 6711, 6718, 6730, 6736, 6738, 6742, 6746, 6748, 6749, 6752, 6754, 6755, 6756, 6759, 6760, 6762, 6763, 6764, 6765, 6769, 6773, 6778, 6784, 6787, 6801, 6803, 6805, 6806, 6807, 6808, 6809, 6812, 6819, 6824, 6831, 6832, 6833, 6841, 6843, 6844, 6847, 6851, 6856, 6858, 6862, 6864, 6866, 6867, 6876, 6877, 6880, 6882, 6896, 6900, 6904, 6905, 6906, 6909, 6910, 6914, 6920, 6924, 6927, 6930, 6935, 6937, 6940, 6942, 6947, 6950, 6953, 6955, 6956, 6962, 6963, 6967, 6969, 6971, 6976, 6978, 6979, 6980, 6989, 6992, 6995, 7003, 7004, 7007, 7009, 7010, 7017, 7019, 7020, 7023, 7027, 7029, 7031, 7038, 7040, 7044, 7046, 7047, 7051, 7053, 7056, 7058, 7063, 7064, 7065, 7067, 7072, 7074, 7075, 7077, 7078, 7081, 7082, 7085, 7088, 7092, 7094, 7098, 7101, 7103, 7105, 7115, 7116, 7118, 7122, 7127, 7128, 7134, 7136, 7137, 7143, 7145, 7148, 7152, 7163, 7165, 7167, 7174, 7179, 7182, 7183, 7187, 7193, 7194, 7201, 7205, 7208, 7209, 7210, 7212, 7217, 7237, 7238, 7245, 7260, 7267, 7268, 7272, 7276, 7283, 7290, 7301, 7302, 7306, 7307, 7310, 7317, 7322, 7323, 7325, 7326, 7330, 7331, 7332, 7334, 7338, 7340, 7343, 7348, 7349, 7352, 7355, 7359, 7361, 7363, 7365, 7367, 7369, 7375, 7376, 7383, 7384, 7388, 7391, 7392, 7395, 7397, 7408, 7409, 7410, 7415, 7416, 7418, 7420, 7421, 7422, 7425, 7429, 7431, 7432, 7434, 7436, 7437, 7439, 7450, 7452, 7453, 7456, 7462, 7469, 7478, 7482, 7485, 7486, 7489, 7491, 7493, 7498, 7500, 7504, 7509, 7512, 7514, 7515, 7516, 7518, 7520, 7528, 7547, 7550, 7551, 7559, 7567, 7569, 7571, 7572, 7575, 7577, 7578, 7588, 7589, 7597, 7599, 7601, 7617, 7618, 7621, 7624, 7630, 7631, 7640, 7642, 7645, 7654, 7655, 7656, 7662, 7670, 7678, 7681, 7682, 7683, 7691, 7713, 7718, 7726, 7729, 7735, 7736, 7742, 7744, 7750, 7751, 7755, 7757, 7770, 7774, 7776, 7778, 7785, 7787, 7791, 7794, 7797, 7799, 7821, 7823, 7824, 7827, 7829, 7834, 7839, 7840, 7842, 7846, 7849, 7850, 7856, 7869, 7873, 7875, 7877, 7894, 7901, 7903, 7905, 7907, 7911, 7913, 7915, 7918, 7921, 7925, 7929, 7931, 7936, 7941, 7945, 7949, 7950, 7952, 7954, 7955, 7957, 7958, 7959, 7962, 7965, 7977, 7987, 7989, 7990, 7991, 7993, 7996, 7999, 8000, 8001, 8005, 8012, 8014, 8015, 8016, 8030, 8033, 8035, 8037, 8039, 8046, 8055, 8060, 8062, 8067, 8070, 8071, 8077, 8079, 8081, 8082, 8083, 8084, 8094, 8101, 8103, 8108, 8110, 8114, 8115, 8118, 8121, 8125, 8127, 8128, 8129, 8141, 8148, 8162, 8165, 8170, 8177, 8178, 8180, 8183, 8190, 8193, 8204, 8210, 8211, 8217, 8218, 8222, 8223, 8226, 8241, 8243, 8246, 8248, 8249, 8250, 8256, 8258, 8263, 8269, 8274, 8276, 8277, 8282, 8284, 8285, 8287, 8292, 8300, 8308, 8311, 8319, 8329, 8337, 8339, 8344, 8347, 8360, 8361, 8362, 8363, 8364, 8365, 8370, 8374, 8377, 8381, 8383, 8389, 8392, 8398, 8399, 8400, 8403, 8405, 8411, 8412, 8414, 8415, 8416, 8422, 8423, 8434, 8439, 8440, 8442, 8443, 8444, 8452, 8453, 8457, 8467, 8468, 8470, 8476, 8482, 8484, 8485, 8490, 8491, 8496, 8502, 8504, 8506, 8508, 8509, 8512, 8513, 8526, 8527, 8534, 8536, 8539, 8544, 8546, 8556, 8565, 8566, 8572, 8577, 8581, 8582, 8584, 8589, 8594, 8601, 8604, 8613, 8614, 8616, 8618, 8619, 8620, 8627, 8629, 8630, 8631, 8639, 8642, 8643, 8644, 8648, 8649, 8653, 8657, 8660, 8661, 8665, 8668, 8671, 8678, 8684, 8686, 8692, 8693, 8700, 8702, 8703, 8706, 8707, 8722, 8723, 8725, 8726, 8731, 8738, 8739, 8741, 8743, 8745, 8746, 8748, 8752, 8753, 8759, 8765, 8771, 8779, 8781, 8790, 8792, 8796, 8798, 8805, 8810, 8814, 8818, 8826, 8828, 8834, 8835, 8841, 8842, 8846, 8848, 8852, 8855, 8857, 8860, 8865, 8866, 8868, 8871, 8874, 8879, 8882, 8888, 8891, 8892, 8893, 8894, 8896, 8900, 8902, 8903, 8908, 8912, 8913, 8917, 8921, 8928, 8930, 8933, 8934, 8935, 8939, 8940, 8944, 8945, 8946, 8948, 8950, 8951, 8952, 8954, 8959, 8960, 8964, 8966, 8971, 8972, 8973, 8974, 8980, 8986, 8989, 8990, 8994, 8996, 9002, 9008, 9012, 9021, 9026, 9030, 9039, 9041, 9043, 9046, 9050, 9051, 9056, 9062, 9063, 9064, 9065, 9066, 9070, 9072, 9074, 9075, 9076, 9079, 9082, 9085, 9093, 9095, 9099, 9100, 9102, 9106, 9112, 9114, 9115, 9117, 9121, 9127, 9128, 9130, 9131, 9138, 9143, 9145, 9157, 9160, 9161, 9163, 9165, 9171, 9172, 9174, 9177, 9181, 9188, 9193, 9197, 9198, 9200, 9204, 9205, 9209, 9210, 9212, 9215, 9223, 9224, 9225, 9228, 9229, 9230, 9233, 9238, 9239, 9248, 9251, 9256, 9257, 9261, 9262, 9267, 9269, 9272, 9273, 9277, 9284, 9298, 9304, 9306, 9321, 9326, 9329, 9330, 9352, 9354, 9355, 9362, 9366, 9367, 9377, 9382, 9391, 9397, 9415, 9416, 9418, 9422, 9429, 9430, 9433, 9435, 9439, 9441, 9442, 9444, 9447, 9456, 9458, 9467, 9470, 9473, 9475, 9488, 9498, 9501, 9504, 9516, 9518, 9526, 9533, 9534, 9535, 9541, 9542, 9543, 9544, 9545, 9546, 9548, 9549, 9554, 9555, 9565, 9570, 9571, 9575, 9576, 9580, 9583, 9588, 9589, 9593, 9596, 9601, 9602, 9603, 9609, 9611, 9615, 9620, 9621, 9626, 9631, 9633, 9634, 9637, 9640, 9643, 9646, 9653, 9655, 9657, 9658, 9659, 9664, 9668, 9672, 9677, 9682, 9690, 9694, 9697, 9698, 9700, 9705, 9710, 9713, 9723, 9725, 9727, 9730, 9731, 9737, 9739, 9742, 9743, 9751, 9755, 9757, 9759, 9760, 9763, 9764, 9767, 9774, 9780, 9783, 9784, 9785, 9787, 9791, 9792, 9796, 9799, 9803, 9807, 9808, 9810, 9813, 9815, 9818, 9819, 9822, 9825, 9826, 9827, 9829, 9831, 9834, 9847, 9852, 9856, 9859, 9867, 9868, 9874, 9878, 9881, 9882, 9883, 9885, 9886, 9889, 9894, 9898, 9900, 9903, 9905, 9907, 9913, 9916, 9931, 9936, 9937, 9948, 9950, 9954, 9957, 9960, 9963, 9966, 9968, 9970, 9990, 9992]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(index_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c57f09-4162-4c68-97b2-c2c475fec39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0c7ee-701e-4dcf-ab0b-d493b3739b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
