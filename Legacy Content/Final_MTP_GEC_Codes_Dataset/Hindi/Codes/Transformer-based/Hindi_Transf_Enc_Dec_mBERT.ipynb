{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b87f2-0915-4de1-a918-a5b5ca4937f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refrence Link: https://github.com/priya-dwivedi/Deep-Learning/blob/master/GrammarCorrector/T5_Grammar.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9c7780-28f4-4676-8620-cfeffba88c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar  2 21:48:59 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.84       Driver Version: 460.84       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 20%   29C    P8     8W / 250W |    447MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 20%   22C    P8     6W / 250W |  10727MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:07:00.0 Off |                  N/A |\n",
      "| 20%   44C    P2    56W / 250W |  10902MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "| 20%   23C    P8     8W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce GTX 108...  Off  | 00000000:0C:00.0 Off |                  N/A |\n",
      "| 67%   85C    P2   181W / 250W |   6609MiB / 11178MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce GTX 108...  Off  | 00000000:0D:00.0 Off |                  N/A |\n",
      "| 45%   76C    P2   249W / 250W |   6609MiB / 11178MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce GTX 108...  Off  | 00000000:0E:00.0 Off |                  N/A |\n",
      "| 52%   82C    P2   252W / 250W |   6615MiB / 11178MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce GTX 108...  Off  | 00000000:0F:00.0 Off |                  N/A |\n",
      "| 20%   33C    P8     8W / 250W |    725MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     38504      C   /opt/conda/bin/python             445MiB |\n",
      "|    1   N/A  N/A     21584      C   /usr/bin/python3                10725MiB |\n",
      "|    2   N/A  N/A     14515      C   python3                          2919MiB |\n",
      "|    2   N/A  N/A     30017      C   /usr/bin/python3                 7981MiB |\n",
      "|    4   N/A  N/A     45458      C   python3                          6607MiB |\n",
      "|    5   N/A  N/A     45970      C   python3                          6607MiB |\n",
      "|    6   N/A  N/A     46611      C   python3                          6613MiB |\n",
      "|    7   N/A  N/A      6774      C   python3                           723MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1472674-e209-489e-93b9-3d3c8da03a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, \"0\" to  \"7\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "#torch.cuda.set_device(0)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(n_gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba40e0c-5fcc-43a9-8d28-47ff9e9870f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "340acc79-3319-47aa-8140-07059948a04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /DATA/gupta92/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "#Set a seed\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7712b672-2f06-47dd-906a-42ab51326a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc22694-eff1-4618-ad9c-8b6110972cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4aaf87c-52e6-4e3d-870c-1955b9afd376",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e4420cd-d086-467d-8384-bc29bb4d91ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('DATA/etoori_train.csv')\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc84c40-1152-4f97-b749-186365fff1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'enc_input':'input'}, inplace = True)\n",
    "df.rename(columns = {'dec_input':'output'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a0a74f8-b795-47a1-aa0a-e3a293a0165b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>परन्तु वे दोनों उन बातों को ज़्यादा समय तक अप हृदय में गाँठ बनाकर नहीं रखते थे ।</td>\n",
       "      <td>परन्तु वे दोनों उन बातों को ज़्यादा समय तक अपने हृदय में गाँठ बनाकर नहीं रखते थे ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>देश में हिन्दी को विस्थापित कर का षड़यंत्र चल रहा है जो चिंता का विषय है ।</td>\n",
       "      <td>देश में हिन्दी को विस्थापित करने का षड़यंत्र चल रहा है जो चिंता का विषय है ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>तीन साल पहले कातिलाना हमले के प्रकरण में एफआर लग जा के बाद डेरा प्रेमी इस बार दर्ज हुए मुकदमे में सभी अभियुक्तों की गिरफ्तारी की मांग को लेकर आंदोलनरत हैं ।</td>\n",
       "      <td>तीन साल पहले कातिलाना हमले के प्रकरण में एफआर लग जाने के बाद डेरा प्रेमी इस बार दर्ज हुए मुकदमे में सभी अभियुक्तों की गिरफ्तारी की मांग को लेकर आंदोलनरत हैं ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम से सिंगापुर के पेरानांका म्यूजियम में प्रदर्शनी लगाई गई हैं ।</td>\n",
       "      <td>रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम से सिंगापुर के पेरानांका म्यूजियम में प्रदर्शनी लगाई गई है ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>तब तक के लिए हमें विराम ले की अनुमति दीजिए ।</td>\n",
       "      <td>तब तक के लिए हमें विराम लेने की अनुमति दीजिए ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>देव स्थेन तक सड़क बनाने की परियोजना चल रही है ।</td>\n",
       "      <td>देव स्थान तक सड़क बनाने की परियोजना चल रही है ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>वर्ड वाचर व प्रसिद्ध चित्रकार अनूप साह आरएनएस को बताया कि श्वेतकंठ चिलचिल पक्षी का वैज्ञानिक नाम ह्वाइट थ्रोटेट लाफिंग थ्रस है ।</td>\n",
       "      <td>वर्ड वाचर व प्रसिद्ध चित्रकार अनूप साह ने आरएनएस को बताया कि श्वेतकंठ चिलचिल पक्षी का वैज्ञानिक नाम ह्वाइट थ्रोटेट लाफिंग थ्रस है ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>दो दिन पहले ज्ञानजी पोस्ट ठेली जिसमें</td>\n",
       "      <td>दो दिन पहले ज्ञानजी ने पोस्ट ठेली जिसमें</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>दुर्घटना की स्थिति में बीमा कम्पनी द्वारा दी जा वाली राशि निर्धारित कर दी गई है ।</td>\n",
       "      <td>दुर्घटना की स्थिति में बीमा कम्पनी द्वारा दी जाने वाली राशि निर्धारित कर दी गई है ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>देर से जाऊंगा तो दुकां बंद हो जाएंगी ।</td>\n",
       "      <td>देर से जाऊंगा तो दुकानें बंद हो जाएंगी ।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                          input  \\\n",
       "0                                                                              परन्तु वे दोनों उन बातों को ज़्यादा समय तक अप हृदय में गाँठ बनाकर नहीं रखते थे ।   \n",
       "1                                                                                    देश में हिन्दी को विस्थापित कर का षड़यंत्र चल रहा है जो चिंता का विषय है ।   \n",
       "2  तीन साल पहले कातिलाना हमले के प्रकरण में एफआर लग जा के बाद डेरा प्रेमी इस बार दर्ज हुए मुकदमे में सभी अभियुक्तों की गिरफ्तारी की मांग को लेकर आंदोलनरत हैं ।   \n",
       "3                                                    रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम से सिंगापुर के पेरानांका म्यूजियम में प्रदर्शनी लगाई गई हैं ।   \n",
       "4                                                                                                                  तब तक के लिए हमें विराम ले की अनुमति दीजिए ।   \n",
       "5                                                                                                               देव स्थेन तक सड़क बनाने की परियोजना चल रही है ।   \n",
       "6                              वर्ड वाचर व प्रसिद्ध चित्रकार अनूप साह आरएनएस को बताया कि श्वेतकंठ चिलचिल पक्षी का वैज्ञानिक नाम ह्वाइट थ्रोटेट लाफिंग थ्रस है ।   \n",
       "7                                                                                                                         दो दिन पहले ज्ञानजी पोस्ट ठेली जिसमें   \n",
       "8                                                                             दुर्घटना की स्थिति में बीमा कम्पनी द्वारा दी जा वाली राशि निर्धारित कर दी गई है ।   \n",
       "9                                                                                                                        देर से जाऊंगा तो दुकां बंद हो जाएंगी ।   \n",
       "\n",
       "                                                                                                                                                            output  \n",
       "0                                                                              परन्तु वे दोनों उन बातों को ज़्यादा समय तक अपने हृदय में गाँठ बनाकर नहीं रखते थे ।   \n",
       "1                                                                                    देश में हिन्दी को विस्थापित करने का षड़यंत्र चल रहा है जो चिंता का विषय है ।   \n",
       "2  तीन साल पहले कातिलाना हमले के प्रकरण में एफआर लग जाने के बाद डेरा प्रेमी इस बार दर्ज हुए मुकदमे में सभी अभियुक्तों की गिरफ्तारी की मांग को लेकर आंदोलनरत हैं ।   \n",
       "3                                                       रामायण रिविजिटेड अ टेल ऑफ लव एंड एडवेंचर नाम से सिंगापुर के पेरानांका म्यूजियम में प्रदर्शनी लगाई गई है ।   \n",
       "4                                                                                                                  तब तक के लिए हमें विराम लेने की अनुमति दीजिए ।   \n",
       "5                                                                                                                 देव स्थान तक सड़क बनाने की परियोजना चल रही है ।   \n",
       "6                             वर्ड वाचर व प्रसिद्ध चित्रकार अनूप साह ने आरएनएस को बताया कि श्वेतकंठ चिलचिल पक्षी का वैज्ञानिक नाम ह्वाइट थ्रोटेट लाफिंग थ्रस है ।   \n",
       "7                                                                                                                         दो दिन पहले ज्ञानजी ने पोस्ट ठेली जिसमें  \n",
       "8                                                                             दुर्घटना की स्थिति में बीमा कम्पनी द्वारा दी जाने वाली राशि निर्धारित कर दी गई है ।   \n",
       "9                                                                                                                        देर से जाऊंगा तो दुकानें बंद हो जाएंगी ।   "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81e7ec2-d0fa-45d6-be30-7c2e2877dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration, T5Tokenizer, \n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "  )\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c509b356-7320-4dbb-9184-9750dd208a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fc016f6-2311-4978-a799-f40e96abaef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b199d7a2-2b5d-4904-b204-a379c34aefca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from transformers import EncoderDecoderModel, BertTokenizer\n",
    "\n",
    "#model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")\n",
    "from transformers import EncoderDecoderModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"bert-base-multilingual-cased\", \"bert-base-multilingual-cased\"\n",
    ")  # initialize Bert2Bert from pre-trained checkpoints\n",
    "\n",
    "# training\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aca04df4-d7de-4f44-9800-94eb1c5296ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_token_len(example):\n",
    "    return len(tokenizer(example).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9821b86-5a8b-4956-856c-d4c88d482f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((126000, 2), (14000, 2))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train - Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.10, shuffle=True)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c514352d-1338-4a42-95a6-36151e6b0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['input_token_len'] = test_df['input'].apply(calc_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9df2945-fb4d-4f63-b24b-2571dceba51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>input_token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40665</th>\n",
       "      <td>अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसकी वर्चस्व सर्वत्र स्थापित है ।</td>\n",
       "      <td>अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसका वर्चस्व सर्वत्र स्थापित है ।</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48520</th>\n",
       "      <td>तथे गौशाला का</td>\n",
       "      <td>तथा गौशाला का</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138403</th>\n",
       "      <td>बच्चे खेल रहे है किलकारियाँ गूंज रही है ब्रह्माण्ड में</td>\n",
       "      <td>बच्चे खेल रहे हैं किलकारियाँ गूंज रही हैं ब्रह्माण्ड में</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130079</th>\n",
       "      <td>धरने पर किसानो ने कहा कि अब समय आ गया हैं कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी हैं ।</td>\n",
       "      <td>धरने पर किसानो ने कहा कि अब समय आ गया है कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी है ।</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50146</th>\n",
       "      <td>सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा हैं ।</td>\n",
       "      <td>सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा है ।</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        input  \\\n",
       "40665         अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसकी वर्चस्व सर्वत्र स्थापित है ।   \n",
       "48520                                                                                           तथे गौशाला का   \n",
       "138403                                                 बच्चे खेल रहे है किलकारियाँ गूंज रही है ब्रह्माण्ड में   \n",
       "130079  धरने पर किसानो ने कहा कि अब समय आ गया हैं कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी हैं ।   \n",
       "50146                   सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा हैं ।   \n",
       "\n",
       "                                                                                                      output  \\\n",
       "40665       अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसका वर्चस्व सर्वत्र स्थापित है ।    \n",
       "48520                                                                                          तथा गौशाला का   \n",
       "138403                                              बच्चे खेल रहे हैं किलकारियाँ गूंज रही हैं ब्रह्माण्ड में   \n",
       "130079  धरने पर किसानो ने कहा कि अब समय आ गया है कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी है ।    \n",
       "50146                  सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा है ।    \n",
       "\n",
       "        input_token_len  \n",
       "40665                36  \n",
       "48520                 9  \n",
       "138403               25  \n",
       "130079               44  \n",
       "50146                37  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd321d92-6f3a-40ee-ba4a-907be30ea244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14000.000000\n",
       "mean        34.456571\n",
       "std         20.822809\n",
       "min          4.000000\n",
       "25%         21.000000\n",
       "50%         30.000000\n",
       "75%         43.000000\n",
       "max        474.000000\n",
       "Name: input_token_len, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['input_token_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5e933a2-ae5f-4c20-87b4-4dbb99b2c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use a token length of 64 since it will cover the vast majority of examples\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ca6fab2-ae06-441f-807e-da87e843249b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'input_token_len', '__index_level_0__'],\n",
       "    num_rows: 14000\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d75fca39-9ccb-4bce-8a14-ca5d573cb60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GrammarDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer,print_text=False):         \n",
    "        self.dataset = dataset\n",
    "        self.pad_to_max_length = False\n",
    "        self.tokenizer = tokenizer\n",
    "        self.print_text = print_text\n",
    "        self.max_len = 64\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "    def tokenize_data(self, example):\n",
    "        input_, target_ = example['input'], example['output']\n",
    "\n",
    "        # tokenize inputs\n",
    "        tokenized_inputs = tokenizer(input_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "    \n",
    "        tokenized_targets = tokenizer(target_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "\n",
    "        inputs={\"input_ids\": tokenized_inputs['input_ids'],\n",
    "            \"attention_mask\": tokenized_inputs['attention_mask'],\n",
    "            \"labels\": tokenized_targets['input_ids']\n",
    "        }\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenize_data(self.dataset[index])\n",
    "        \n",
    "        if self.print_text:\n",
    "            for k in inputs.keys():\n",
    "                print(k, len(inputs[k]))\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8aa2940-f691-4bc5-9c05-d70ff42e05ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids 50\n",
      "attention_mask 50\n",
      "labels 50\n",
      "{'input_ids': [101, 11081, 11549, 19741, 27391, 12213, 67147, 10914, 868, 14070, 13432, 18187, 18438, 35247, 19966, 13088, 865, 18351, 73649, 11208, 16192, 37444, 17798, 10977, 89652, 893, 78530, 17110, 77386, 11267, 889, 24667, 70288, 19255, 14265, 868, 14070, 13432, 18187, 18438, 35247, 10826, 866, 41082, 14080, 16791, 893, 34315, 920, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [101, 11081, 11549, 19741, 27391, 12213, 67147, 10914, 868, 14070, 13432, 18187, 18438, 35247, 19966, 13088, 865, 18351, 73649, 11208, 16192, 37444, 13794, 10977, 89652, 893, 78530, 17110, 77386, 11267, 889, 24667, 70288, 19255, 14265, 868, 14070, 13432, 18187, 18438, 35247, 10826, 866, 41082, 14080, 16791, 893, 34315, 920, 102]}\n"
     ]
    }
   ],
   "source": [
    "dataset = GrammarDataset(test_dataset, tokenizer, True)\n",
    "print(dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbfc0564-2706-4d95-8dfb-e9970a0e2a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids 36\n",
      "attention_mask 36\n",
      "labels 36\n",
      "{'input_ids': [101, 851, 28462, 45086, 73184, 35773, 10914, 898, 17110, 88547, 10412, 15236, 10532, 895, 14251, 15070, 108775, 29161, 13220, 19810, 63214, 15837, 81583, 59444, 13286, 64897, 895, 92943, 13432, 27155, 66326, 27373, 36072, 10569, 920, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [101, 851, 28462, 45086, 73184, 35773, 10914, 898, 17110, 88547, 10412, 15236, 10532, 895, 14251, 15070, 108775, 29161, 13220, 19810, 63214, 15837, 81583, 59444, 13286, 79610, 895, 92943, 13432, 27155, 66326, 27373, 36072, 10569, 920, 102]}\n"
     ]
    }
   ],
   "source": [
    "dataset = GrammarDataset(test_dataset, tokenizer, True)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e6bd3fc-b11e-47a1-b473-10705ebe1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Evaluator\n",
    "#!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f50438a6-1ed2-460a-8bb0-307fa7b23806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf186c82-6bea-4bb3-b9f3-1b00df72ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model\n",
    "#data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest', return_tensors='pt')\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest', return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46056d09-d0ba-46ac-823e-adf72d83ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 't5_gec_hindi_muRIL_1'\n",
    "#torch_device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "#model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b108ae3-2088-44fa-af0f-8b3583bd157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining training related arguments\n",
    "batch_size = 8\n",
    "args = Seq2SeqTrainingArguments(\n",
    "            output_dir=\"models/hindi/EncDec_bert\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            weight_decay=0.1,\n",
    "            save_steps=10000,\n",
    "            num_train_epochs=2,\n",
    "            predict_with_generate=True,\n",
    "            fp16=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2a430e6-c635-43bb-a305-357ac0773191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfbcd6b1-bdcb-4f9d-a7ca-cd6924d7e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d9743c7-8b58-4bee-9ab1-2fcd8032b17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# defining trainer using 🤗\n",
    "trainer = Seq2SeqTrainer(model=model, \n",
    "                args=args, \n",
    "                train_dataset= GrammarDataset(train_dataset, tokenizer),\n",
    "                eval_dataset=GrammarDataset(test_dataset, tokenizer),\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator)\n",
    "                #compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50e692da-5dee-4325-b082-b660f82d1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aaaea036-afff-4988-b9c3-605dace1b50e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 126000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 31500\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31500' max='31500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31500/31500 3:06:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.089137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.224824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to models/hindi/EncDec_bert/checkpoint-10000\n",
      "Configuration saved in models/hindi/EncDec_bert/checkpoint-10000/config.json\n",
      "Model weights saved in models/hindi/EncDec_bert/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/EncDec_bert/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/EncDec_bert/checkpoint-10000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to models/hindi/EncDec_bert/checkpoint-20000\n",
      "Configuration saved in models/hindi/EncDec_bert/checkpoint-20000/config.json\n",
      "Model weights saved in models/hindi/EncDec_bert/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/EncDec_bert/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/EncDec_bert/checkpoint-20000/special_tokens_map.json\n",
      "Saving model checkpoint to models/hindi/EncDec_bert/checkpoint-30000\n",
      "Configuration saved in models/hindi/EncDec_bert/checkpoint-30000/config.json\n",
      "Model weights saved in models/hindi/EncDec_bert/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in models/hindi/EncDec_bert/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in models/hindi/EncDec_bert/checkpoint-30000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31500, training_loss=0.32566606230962847, metrics={'train_runtime': 11218.625, 'train_samples_per_second': 22.463, 'train_steps_per_second': 2.808, 'total_flos': 1.714918853611512e+16, 'train_loss': 0.32566606230962847, 'epoch': 2.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Wandb API Key: bcdbdd5ee9d76a20c90b5f2a246eb45f14b341a9\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"     #Disabling Wandb\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f5246-3f39-4d82-b4b6-a31f62d60a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed8776-52ca-4d06-9e15-158994267915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch_device = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b68254c3-619e-464f-bfa6-01c21279c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to hindi_EncDec_mbert\n",
      "Configuration saved in hindi_EncDec_mbert/config.json\n",
      "Model weights saved in hindi_EncDec_mbert/pytorch_model.bin\n",
      "tokenizer config file saved in hindi_EncDec_mbert/tokenizer_config.json\n",
      "Special tokens file saved in hindi_EncDec_mbert/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "#Save the trained Model\n",
    "trainer.save_model('hindi_EncDec_mbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b9bd5-acdd-4728-8bb5-8ac6bb809b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895631b-8f35-4ea2-930f-ef9c56e17072",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da45d37b-cd1f-43e7-a4e2-bc18ef984b14",
   "metadata": {},
   "source": [
    "# Testing Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ea2de44-6fbe-40a6-ba4f-f074a82b6767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /DATA/gupta92/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/vocab.txt\n",
      "loading file tokenizer.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration,AutoTokenizer\n",
    "model_name = 'hindi_EncDec_mbert'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "41028ce6-40fa-4740-9f0d-579b4503d993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file hindi_EncDec_mbert/config.json\n",
      "Model config EncoderDecoderConfig {\n",
      "  \"_commit_hash\": null,\n",
      "  \"architectures\": [\n",
      "    \"EncoderDecoderModel\"\n",
      "  ],\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.22.2\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 119547\n",
      "  },\n",
      "  \"decoder_start_token_id\": 101,\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"BertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"directionality\": \"bidi\",\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"bert\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"pooler_fc_size\": 768,\n",
      "    \"pooler_num_attention_heads\": 12,\n",
      "    \"pooler_num_fc_layers\": 3,\n",
      "    \"pooler_size_per_head\": 128,\n",
      "    \"pooler_type\": \"first_token_transform\",\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.22.2\",\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 119547\n",
      "  },\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"model_type\": \"encoder-decoder\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": null,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file hindi_EncDec_mbert/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
      "\n",
      "All the weights of EncoderDecoderModel were initialized from the model checkpoint at hindi_EncDec_mbert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import EncoderDecoderModel\n",
    "model_name = 'hindi_EncDec_mbert'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "# load fine-tuned model\n",
    "trained_model = EncoderDecoderModel.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "514da5cd-644c-4f27-82fb-d126e26544bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoderModel(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): BertLMHeadModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "67ec5841-d3e0-43a5-8a28-b5e9ea24dee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+------------+\n",
      "|                               Modules                                | Parameters |\n",
      "+----------------------------------------------------------------------+------------+\n",
      "|              encoder.embeddings.word_embeddings.weight               |  91812096  |\n",
      "|            encoder.embeddings.position_embeddings.weight             |   393216   |\n",
      "|           encoder.embeddings.token_type_embeddings.weight            |    1536    |\n",
      "|                 encoder.embeddings.LayerNorm.weight                  |    768     |\n",
      "|                  encoder.embeddings.LayerNorm.bias                   |    768     |\n",
      "|         encoder.encoder.layer.0.attention.self.query.weight          |   589824   |\n",
      "|          encoder.encoder.layer.0.attention.self.query.bias           |    768     |\n",
      "|          encoder.encoder.layer.0.attention.self.key.weight           |   589824   |\n",
      "|           encoder.encoder.layer.0.attention.self.key.bias            |    768     |\n",
      "|         encoder.encoder.layer.0.attention.self.value.weight          |   589824   |\n",
      "|          encoder.encoder.layer.0.attention.self.value.bias           |    768     |\n",
      "|        encoder.encoder.layer.0.attention.output.dense.weight         |   589824   |\n",
      "|         encoder.encoder.layer.0.attention.output.dense.bias          |    768     |\n",
      "|      encoder.encoder.layer.0.attention.output.LayerNorm.weight       |    768     |\n",
      "|       encoder.encoder.layer.0.attention.output.LayerNorm.bias        |    768     |\n",
      "|          encoder.encoder.layer.0.intermediate.dense.weight           |  2359296   |\n",
      "|           encoder.encoder.layer.0.intermediate.dense.bias            |    3072    |\n",
      "|             encoder.encoder.layer.0.output.dense.weight              |  2359296   |\n",
      "|              encoder.encoder.layer.0.output.dense.bias               |    768     |\n",
      "|           encoder.encoder.layer.0.output.LayerNorm.weight            |    768     |\n",
      "|            encoder.encoder.layer.0.output.LayerNorm.bias             |    768     |\n",
      "|         encoder.encoder.layer.1.attention.self.query.weight          |   589824   |\n",
      "|          encoder.encoder.layer.1.attention.self.query.bias           |    768     |\n",
      "|          encoder.encoder.layer.1.attention.self.key.weight           |   589824   |\n",
      "|           encoder.encoder.layer.1.attention.self.key.bias            |    768     |\n",
      "|         encoder.encoder.layer.1.attention.self.value.weight          |   589824   |\n",
      "|          encoder.encoder.layer.1.attention.self.value.bias           |    768     |\n",
      "|        encoder.encoder.layer.1.attention.output.dense.weight         |   589824   |\n",
      "|         encoder.encoder.layer.1.attention.output.dense.bias          |    768     |\n",
      "|      encoder.encoder.layer.1.attention.output.LayerNorm.weight       |    768     |\n",
      "|       encoder.encoder.layer.1.attention.output.LayerNorm.bias        |    768     |\n",
      "|          encoder.encoder.layer.1.intermediate.dense.weight           |  2359296   |\n",
      "|           encoder.encoder.layer.1.intermediate.dense.bias            |    3072    |\n",
      "|             encoder.encoder.layer.1.output.dense.weight              |  2359296   |\n",
      "|              encoder.encoder.layer.1.output.dense.bias               |    768     |\n",
      "|           encoder.encoder.layer.1.output.LayerNorm.weight            |    768     |\n",
      "|            encoder.encoder.layer.1.output.LayerNorm.bias             |    768     |\n",
      "|         encoder.encoder.layer.2.attention.self.query.weight          |   589824   |\n",
      "|          encoder.encoder.layer.2.attention.self.query.bias           |    768     |\n",
      "|          encoder.encoder.layer.2.attention.self.key.weight           |   589824   |\n",
      "|           encoder.encoder.layer.2.attention.self.key.bias            |    768     |\n",
      "|         encoder.encoder.layer.2.attention.self.value.weight          |   589824   |\n",
      "|          encoder.encoder.layer.2.attention.self.value.bias           |    768     |\n",
      "|        encoder.encoder.layer.2.attention.output.dense.weight         |   589824   |\n",
      "|         encoder.encoder.layer.2.attention.output.dense.bias          |    768     |\n",
      "|      encoder.encoder.layer.2.attention.output.LayerNorm.weight       |    768     |\n",
      "|       encoder.encoder.layer.2.attention.output.LayerNorm.bias        |    768     |\n",
      "|          encoder.encoder.layer.2.intermediate.dense.weight           |  2359296   |\n",
      "|           encoder.encoder.layer.2.intermediate.dense.bias            |    3072    |\n",
      "|             encoder.encoder.layer.2.output.dense.weight              |  2359296   |\n",
      "|              encoder.encoder.layer.2.output.dense.bias               |    768     |\n",
      "|           encoder.encoder.layer.2.output.LayerNorm.weight            |    768     |\n",
      "|            encoder.encoder.layer.2.output.LayerNorm.bias             |    768     |\n",
      "|         encoder.encoder.layer.3.attention.self.query.weight          |   589824   |\n",
      "|          encoder.encoder.layer.3.attention.self.query.bias           |    768     |\n",
      "|          encoder.encoder.layer.3.attention.self.key.weight           |   589824   |\n",
      "|           encoder.encoder.layer.3.attention.self.key.bias            |    768     |\n",
      "|         encoder.encoder.layer.3.attention.self.value.weight          |   589824   |\n",
      "|          encoder.encoder.layer.3.attention.self.value.bias           |    768     |\n",
      "|        encoder.encoder.layer.3.attention.output.dense.weight         |   589824   |\n",
      "|         encoder.encoder.layer.3.attention.output.dense.bias          |    768     |\n",
      "|      encoder.encoder.layer.3.attention.output.LayerNorm.weight       |    768     |\n",
      "|       encoder.encoder.layer.3.attention.output.LayerNorm.bias        |    768     |\n",
      "|          encoder.encoder.layer.3.intermediate.dense.weight           |  2359296   |\n",
      "|           encoder.encoder.layer.3.intermediate.dense.bias            |    3072    |\n",
      "|             encoder.encoder.layer.3.output.dense.weight              |  2359296   |\n",
      "|              encoder.encoder.layer.3.output.dense.bias               |    768     |\n",
      "|           encoder.encoder.layer.3.output.LayerNorm.weight            |    768     |\n",
      "|            encoder.encoder.layer.3.output.LayerNorm.bias             |    768     |\n",
      "|         encoder.encoder.layer.4.attention.self.query.weight          |   589824   |\n",
      "|          encoder.encoder.layer.4.attention.self.query.bias           |    768     |\n",
      "|          encoder.encoder.layer.4.attention.self.key.weight           |   589824   |\n",
      "|           encoder.encoder.layer.4.attention.self.key.bias            |    768     |\n",
      "|         encoder.encoder.layer.4.attention.self.value.weight          |   589824   |\n",
      "|          encoder.encoder.layer.4.attention.self.value.bias           |    768     |\n",
      "|        encoder.encoder.layer.4.attention.output.dense.weight         |   589824   |\n",
      "|         encoder.encoder.layer.4.attention.output.dense.bias          |    768     |\n",
      "|      encoder.encoder.layer.4.attention.output.LayerNorm.weight       |    768     |\n",
      "|       encoder.encoder.layer.4.attention.output.LayerNorm.bias        |    768     |\n",
      "|          encoder.encoder.layer.4.intermediate.dense.weight           |  2359296   |\n",
      "|           encoder.encoder.layer.4.intermediate.dense.bias            |    3072    |\n",
      "|             encoder.encoder.layer.4.output.dense.weight              |  2359296   |\n",
      "|              encoder.encoder.layer.4.output.dense.bias               |    768     |\n",
      "|           encoder.encoder.layer.4.output.LayerNorm.weight            |    768     |\n",
      "|            encoder.encoder.layer.4.output.LayerNorm.bias             |    768     |\n",
      "|         encoder.encoder.layer.5.attention.self.query.weight          |   589824   |\n",
      "|          encoder.encoder.layer.5.attention.self.query.bias           |    768     |\n",
      "|          encoder.encoder.layer.5.attention.self.key.weight           |   589824   |\n",
      "|           encoder.encoder.layer.5.attention.self.key.bias            |    768     |\n",
      "|         encoder.encoder.layer.5.attention.self.value.weight          |   589824   |\n",
      "|          encoder.encoder.layer.5.attention.self.value.bias           |    768     |\n",
      "|        encoder.encoder.layer.5.attention.output.dense.weight         |   589824   |\n",
      "|         encoder.encoder.layer.5.attention.output.dense.bias          |    768     |\n",
      "|      encoder.encoder.layer.5.attention.output.LayerNorm.weight       |    768     |\n",
      "|       encoder.encoder.layer.5.attention.output.LayerNorm.bias        |    768     |\n",
      "|          encoder.encoder.layer.5.intermediate.dense.weight           |  2359296   |\n",
      "|           encoder.encoder.layer.5.intermediate.dense.bias            |    3072    |\n",
      "|             encoder.encoder.layer.5.output.dense.weight              |  2359296   |\n",
      "|              encoder.encoder.layer.5.output.dense.bias               |    768     |\n",
      "|           encoder.encoder.layer.5.output.LayerNorm.weight            |    768     |\n",
      "|            encoder.encoder.layer.5.output.LayerNorm.bias             |    768     |\n",
      "|         encoder.encoder.layer.6.attention.self.query.weight          |   589824   |\n",
      "|          encoder.encoder.layer.6.attention.self.query.bias           |    768     |\n",
      "|          encoder.encoder.layer.6.attention.self.key.weight           |   589824   |\n",
      "|           encoder.encoder.layer.6.attention.self.key.bias            |    768     |\n",
      "|         encoder.encoder.layer.6.attention.self.value.weight          |   589824   |\n",
      "|          encoder.encoder.layer.6.attention.self.value.bias           |    768     |\n",
      "|        encoder.encoder.layer.6.attention.output.dense.weight         |   589824   |\n",
      "|         encoder.encoder.layer.6.attention.output.dense.bias          |    768     |\n",
      "|      encoder.encoder.layer.6.attention.output.LayerNorm.weight       |    768     |\n",
      "|       encoder.encoder.layer.6.attention.output.LayerNorm.bias        |    768     |\n",
      "|          encoder.encoder.layer.6.intermediate.dense.weight           |  2359296   |\n",
      "|           encoder.encoder.layer.6.intermediate.dense.bias            |    3072    |\n",
      "|             encoder.encoder.layer.6.output.dense.weight              |  2359296   |\n",
      "|              encoder.encoder.layer.6.output.dense.bias               |    768     |\n",
      "|           encoder.encoder.layer.6.output.LayerNorm.weight            |    768     |\n",
      "|            encoder.encoder.layer.6.output.LayerNorm.bias             |    768     |\n",
      "|         encoder.encoder.layer.7.attention.self.query.weight          |   589824   |\n",
      "|          encoder.encoder.layer.7.attention.self.query.bias           |    768     |\n",
      "|          encoder.encoder.layer.7.attention.self.key.weight           |   589824   |\n",
      "|           encoder.encoder.layer.7.attention.self.key.bias            |    768     |\n",
      "|         encoder.encoder.layer.7.attention.self.value.weight          |   589824   |\n",
      "|          encoder.encoder.layer.7.attention.self.value.bias           |    768     |\n",
      "|        encoder.encoder.layer.7.attention.output.dense.weight         |   589824   |\n",
      "|         encoder.encoder.layer.7.attention.output.dense.bias          |    768     |\n",
      "|      encoder.encoder.layer.7.attention.output.LayerNorm.weight       |    768     |\n",
      "|       encoder.encoder.layer.7.attention.output.LayerNorm.bias        |    768     |\n",
      "|          encoder.encoder.layer.7.intermediate.dense.weight           |  2359296   |\n",
      "|           encoder.encoder.layer.7.intermediate.dense.bias            |    3072    |\n",
      "|             encoder.encoder.layer.7.output.dense.weight              |  2359296   |\n",
      "|              encoder.encoder.layer.7.output.dense.bias               |    768     |\n",
      "|           encoder.encoder.layer.7.output.LayerNorm.weight            |    768     |\n",
      "|            encoder.encoder.layer.7.output.LayerNorm.bias             |    768     |\n",
      "|         encoder.encoder.layer.8.attention.self.query.weight          |   589824   |\n",
      "|          encoder.encoder.layer.8.attention.self.query.bias           |    768     |\n",
      "|          encoder.encoder.layer.8.attention.self.key.weight           |   589824   |\n",
      "|           encoder.encoder.layer.8.attention.self.key.bias            |    768     |\n",
      "|         encoder.encoder.layer.8.attention.self.value.weight          |   589824   |\n",
      "|          encoder.encoder.layer.8.attention.self.value.bias           |    768     |\n",
      "|        encoder.encoder.layer.8.attention.output.dense.weight         |   589824   |\n",
      "|         encoder.encoder.layer.8.attention.output.dense.bias          |    768     |\n",
      "|      encoder.encoder.layer.8.attention.output.LayerNorm.weight       |    768     |\n",
      "|       encoder.encoder.layer.8.attention.output.LayerNorm.bias        |    768     |\n",
      "|          encoder.encoder.layer.8.intermediate.dense.weight           |  2359296   |\n",
      "|           encoder.encoder.layer.8.intermediate.dense.bias            |    3072    |\n",
      "|             encoder.encoder.layer.8.output.dense.weight              |  2359296   |\n",
      "|              encoder.encoder.layer.8.output.dense.bias               |    768     |\n",
      "|           encoder.encoder.layer.8.output.LayerNorm.weight            |    768     |\n",
      "|            encoder.encoder.layer.8.output.LayerNorm.bias             |    768     |\n",
      "|         encoder.encoder.layer.9.attention.self.query.weight          |   589824   |\n",
      "|          encoder.encoder.layer.9.attention.self.query.bias           |    768     |\n",
      "|          encoder.encoder.layer.9.attention.self.key.weight           |   589824   |\n",
      "|           encoder.encoder.layer.9.attention.self.key.bias            |    768     |\n",
      "|         encoder.encoder.layer.9.attention.self.value.weight          |   589824   |\n",
      "|          encoder.encoder.layer.9.attention.self.value.bias           |    768     |\n",
      "|        encoder.encoder.layer.9.attention.output.dense.weight         |   589824   |\n",
      "|         encoder.encoder.layer.9.attention.output.dense.bias          |    768     |\n",
      "|      encoder.encoder.layer.9.attention.output.LayerNorm.weight       |    768     |\n",
      "|       encoder.encoder.layer.9.attention.output.LayerNorm.bias        |    768     |\n",
      "|          encoder.encoder.layer.9.intermediate.dense.weight           |  2359296   |\n",
      "|           encoder.encoder.layer.9.intermediate.dense.bias            |    3072    |\n",
      "|             encoder.encoder.layer.9.output.dense.weight              |  2359296   |\n",
      "|              encoder.encoder.layer.9.output.dense.bias               |    768     |\n",
      "|           encoder.encoder.layer.9.output.LayerNorm.weight            |    768     |\n",
      "|            encoder.encoder.layer.9.output.LayerNorm.bias             |    768     |\n",
      "|         encoder.encoder.layer.10.attention.self.query.weight         |   589824   |\n",
      "|          encoder.encoder.layer.10.attention.self.query.bias          |    768     |\n",
      "|          encoder.encoder.layer.10.attention.self.key.weight          |   589824   |\n",
      "|           encoder.encoder.layer.10.attention.self.key.bias           |    768     |\n",
      "|         encoder.encoder.layer.10.attention.self.value.weight         |   589824   |\n",
      "|          encoder.encoder.layer.10.attention.self.value.bias          |    768     |\n",
      "|        encoder.encoder.layer.10.attention.output.dense.weight        |   589824   |\n",
      "|         encoder.encoder.layer.10.attention.output.dense.bias         |    768     |\n",
      "|      encoder.encoder.layer.10.attention.output.LayerNorm.weight      |    768     |\n",
      "|       encoder.encoder.layer.10.attention.output.LayerNorm.bias       |    768     |\n",
      "|          encoder.encoder.layer.10.intermediate.dense.weight          |  2359296   |\n",
      "|           encoder.encoder.layer.10.intermediate.dense.bias           |    3072    |\n",
      "|             encoder.encoder.layer.10.output.dense.weight             |  2359296   |\n",
      "|              encoder.encoder.layer.10.output.dense.bias              |    768     |\n",
      "|           encoder.encoder.layer.10.output.LayerNorm.weight           |    768     |\n",
      "|            encoder.encoder.layer.10.output.LayerNorm.bias            |    768     |\n",
      "|         encoder.encoder.layer.11.attention.self.query.weight         |   589824   |\n",
      "|          encoder.encoder.layer.11.attention.self.query.bias          |    768     |\n",
      "|          encoder.encoder.layer.11.attention.self.key.weight          |   589824   |\n",
      "|           encoder.encoder.layer.11.attention.self.key.bias           |    768     |\n",
      "|         encoder.encoder.layer.11.attention.self.value.weight         |   589824   |\n",
      "|          encoder.encoder.layer.11.attention.self.value.bias          |    768     |\n",
      "|        encoder.encoder.layer.11.attention.output.dense.weight        |   589824   |\n",
      "|         encoder.encoder.layer.11.attention.output.dense.bias         |    768     |\n",
      "|      encoder.encoder.layer.11.attention.output.LayerNorm.weight      |    768     |\n",
      "|       encoder.encoder.layer.11.attention.output.LayerNorm.bias       |    768     |\n",
      "|          encoder.encoder.layer.11.intermediate.dense.weight          |  2359296   |\n",
      "|           encoder.encoder.layer.11.intermediate.dense.bias           |    3072    |\n",
      "|             encoder.encoder.layer.11.output.dense.weight             |  2359296   |\n",
      "|              encoder.encoder.layer.11.output.dense.bias              |    768     |\n",
      "|           encoder.encoder.layer.11.output.LayerNorm.weight           |    768     |\n",
      "|            encoder.encoder.layer.11.output.LayerNorm.bias            |    768     |\n",
      "|                     encoder.pooler.dense.weight                      |   589824   |\n",
      "|                      encoder.pooler.dense.bias                       |    768     |\n",
      "|            decoder.bert.embeddings.word_embeddings.weight            |  91812096  |\n",
      "|          decoder.bert.embeddings.position_embeddings.weight          |   393216   |\n",
      "|         decoder.bert.embeddings.token_type_embeddings.weight         |    1536    |\n",
      "|               decoder.bert.embeddings.LayerNorm.weight               |    768     |\n",
      "|                decoder.bert.embeddings.LayerNorm.bias                |    768     |\n",
      "|       decoder.bert.encoder.layer.0.attention.self.query.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.0.attention.self.query.bias        |    768     |\n",
      "|        decoder.bert.encoder.layer.0.attention.self.key.weight        |   589824   |\n",
      "|         decoder.bert.encoder.layer.0.attention.self.key.bias         |    768     |\n",
      "|       decoder.bert.encoder.layer.0.attention.self.value.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.0.attention.self.value.bias        |    768     |\n",
      "|      decoder.bert.encoder.layer.0.attention.output.dense.weight      |   589824   |\n",
      "|       decoder.bert.encoder.layer.0.attention.output.dense.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight    |    768     |\n",
      "|     decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias     |    768     |\n",
      "|    decoder.bert.encoder.layer.0.crossattention.self.query.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.0.crossattention.self.query.bias      |    768     |\n",
      "|     decoder.bert.encoder.layer.0.crossattention.self.key.weight      |   589824   |\n",
      "|      decoder.bert.encoder.layer.0.crossattention.self.key.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.0.crossattention.self.value.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.0.crossattention.self.value.bias      |    768     |\n",
      "|   decoder.bert.encoder.layer.0.crossattention.output.dense.weight    |   589824   |\n",
      "|    decoder.bert.encoder.layer.0.crossattention.output.dense.bias     |    768     |\n",
      "| decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight  |    768     |\n",
      "|  decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias   |    768     |\n",
      "|        decoder.bert.encoder.layer.0.intermediate.dense.weight        |  2359296   |\n",
      "|         decoder.bert.encoder.layer.0.intermediate.dense.bias         |    3072    |\n",
      "|           decoder.bert.encoder.layer.0.output.dense.weight           |  2359296   |\n",
      "|            decoder.bert.encoder.layer.0.output.dense.bias            |    768     |\n",
      "|         decoder.bert.encoder.layer.0.output.LayerNorm.weight         |    768     |\n",
      "|          decoder.bert.encoder.layer.0.output.LayerNorm.bias          |    768     |\n",
      "|       decoder.bert.encoder.layer.1.attention.self.query.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.1.attention.self.query.bias        |    768     |\n",
      "|        decoder.bert.encoder.layer.1.attention.self.key.weight        |   589824   |\n",
      "|         decoder.bert.encoder.layer.1.attention.self.key.bias         |    768     |\n",
      "|       decoder.bert.encoder.layer.1.attention.self.value.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.1.attention.self.value.bias        |    768     |\n",
      "|      decoder.bert.encoder.layer.1.attention.output.dense.weight      |   589824   |\n",
      "|       decoder.bert.encoder.layer.1.attention.output.dense.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight    |    768     |\n",
      "|     decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias     |    768     |\n",
      "|    decoder.bert.encoder.layer.1.crossattention.self.query.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.1.crossattention.self.query.bias      |    768     |\n",
      "|     decoder.bert.encoder.layer.1.crossattention.self.key.weight      |   589824   |\n",
      "|      decoder.bert.encoder.layer.1.crossattention.self.key.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.1.crossattention.self.value.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.1.crossattention.self.value.bias      |    768     |\n",
      "|   decoder.bert.encoder.layer.1.crossattention.output.dense.weight    |   589824   |\n",
      "|    decoder.bert.encoder.layer.1.crossattention.output.dense.bias     |    768     |\n",
      "| decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight  |    768     |\n",
      "|  decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias   |    768     |\n",
      "|        decoder.bert.encoder.layer.1.intermediate.dense.weight        |  2359296   |\n",
      "|         decoder.bert.encoder.layer.1.intermediate.dense.bias         |    3072    |\n",
      "|           decoder.bert.encoder.layer.1.output.dense.weight           |  2359296   |\n",
      "|            decoder.bert.encoder.layer.1.output.dense.bias            |    768     |\n",
      "|         decoder.bert.encoder.layer.1.output.LayerNorm.weight         |    768     |\n",
      "|          decoder.bert.encoder.layer.1.output.LayerNorm.bias          |    768     |\n",
      "|       decoder.bert.encoder.layer.2.attention.self.query.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.2.attention.self.query.bias        |    768     |\n",
      "|        decoder.bert.encoder.layer.2.attention.self.key.weight        |   589824   |\n",
      "|         decoder.bert.encoder.layer.2.attention.self.key.bias         |    768     |\n",
      "|       decoder.bert.encoder.layer.2.attention.self.value.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.2.attention.self.value.bias        |    768     |\n",
      "|      decoder.bert.encoder.layer.2.attention.output.dense.weight      |   589824   |\n",
      "|       decoder.bert.encoder.layer.2.attention.output.dense.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight    |    768     |\n",
      "|     decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias     |    768     |\n",
      "|    decoder.bert.encoder.layer.2.crossattention.self.query.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.2.crossattention.self.query.bias      |    768     |\n",
      "|     decoder.bert.encoder.layer.2.crossattention.self.key.weight      |   589824   |\n",
      "|      decoder.bert.encoder.layer.2.crossattention.self.key.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.2.crossattention.self.value.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.2.crossattention.self.value.bias      |    768     |\n",
      "|   decoder.bert.encoder.layer.2.crossattention.output.dense.weight    |   589824   |\n",
      "|    decoder.bert.encoder.layer.2.crossattention.output.dense.bias     |    768     |\n",
      "| decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight  |    768     |\n",
      "|  decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias   |    768     |\n",
      "|        decoder.bert.encoder.layer.2.intermediate.dense.weight        |  2359296   |\n",
      "|         decoder.bert.encoder.layer.2.intermediate.dense.bias         |    3072    |\n",
      "|           decoder.bert.encoder.layer.2.output.dense.weight           |  2359296   |\n",
      "|            decoder.bert.encoder.layer.2.output.dense.bias            |    768     |\n",
      "|         decoder.bert.encoder.layer.2.output.LayerNorm.weight         |    768     |\n",
      "|          decoder.bert.encoder.layer.2.output.LayerNorm.bias          |    768     |\n",
      "|       decoder.bert.encoder.layer.3.attention.self.query.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.3.attention.self.query.bias        |    768     |\n",
      "|        decoder.bert.encoder.layer.3.attention.self.key.weight        |   589824   |\n",
      "|         decoder.bert.encoder.layer.3.attention.self.key.bias         |    768     |\n",
      "|       decoder.bert.encoder.layer.3.attention.self.value.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.3.attention.self.value.bias        |    768     |\n",
      "|      decoder.bert.encoder.layer.3.attention.output.dense.weight      |   589824   |\n",
      "|       decoder.bert.encoder.layer.3.attention.output.dense.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight    |    768     |\n",
      "|     decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias     |    768     |\n",
      "|    decoder.bert.encoder.layer.3.crossattention.self.query.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.3.crossattention.self.query.bias      |    768     |\n",
      "|     decoder.bert.encoder.layer.3.crossattention.self.key.weight      |   589824   |\n",
      "|      decoder.bert.encoder.layer.3.crossattention.self.key.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.3.crossattention.self.value.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.3.crossattention.self.value.bias      |    768     |\n",
      "|   decoder.bert.encoder.layer.3.crossattention.output.dense.weight    |   589824   |\n",
      "|    decoder.bert.encoder.layer.3.crossattention.output.dense.bias     |    768     |\n",
      "| decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight  |    768     |\n",
      "|  decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias   |    768     |\n",
      "|        decoder.bert.encoder.layer.3.intermediate.dense.weight        |  2359296   |\n",
      "|         decoder.bert.encoder.layer.3.intermediate.dense.bias         |    3072    |\n",
      "|           decoder.bert.encoder.layer.3.output.dense.weight           |  2359296   |\n",
      "|            decoder.bert.encoder.layer.3.output.dense.bias            |    768     |\n",
      "|         decoder.bert.encoder.layer.3.output.LayerNorm.weight         |    768     |\n",
      "|          decoder.bert.encoder.layer.3.output.LayerNorm.bias          |    768     |\n",
      "|       decoder.bert.encoder.layer.4.attention.self.query.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.4.attention.self.query.bias        |    768     |\n",
      "|        decoder.bert.encoder.layer.4.attention.self.key.weight        |   589824   |\n",
      "|         decoder.bert.encoder.layer.4.attention.self.key.bias         |    768     |\n",
      "|       decoder.bert.encoder.layer.4.attention.self.value.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.4.attention.self.value.bias        |    768     |\n",
      "|      decoder.bert.encoder.layer.4.attention.output.dense.weight      |   589824   |\n",
      "|       decoder.bert.encoder.layer.4.attention.output.dense.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight    |    768     |\n",
      "|     decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias     |    768     |\n",
      "|    decoder.bert.encoder.layer.4.crossattention.self.query.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.4.crossattention.self.query.bias      |    768     |\n",
      "|     decoder.bert.encoder.layer.4.crossattention.self.key.weight      |   589824   |\n",
      "|      decoder.bert.encoder.layer.4.crossattention.self.key.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.4.crossattention.self.value.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.4.crossattention.self.value.bias      |    768     |\n",
      "|   decoder.bert.encoder.layer.4.crossattention.output.dense.weight    |   589824   |\n",
      "|    decoder.bert.encoder.layer.4.crossattention.output.dense.bias     |    768     |\n",
      "| decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight  |    768     |\n",
      "|  decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias   |    768     |\n",
      "|        decoder.bert.encoder.layer.4.intermediate.dense.weight        |  2359296   |\n",
      "|         decoder.bert.encoder.layer.4.intermediate.dense.bias         |    3072    |\n",
      "|           decoder.bert.encoder.layer.4.output.dense.weight           |  2359296   |\n",
      "|            decoder.bert.encoder.layer.4.output.dense.bias            |    768     |\n",
      "|         decoder.bert.encoder.layer.4.output.LayerNorm.weight         |    768     |\n",
      "|          decoder.bert.encoder.layer.4.output.LayerNorm.bias          |    768     |\n",
      "|       decoder.bert.encoder.layer.5.attention.self.query.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.5.attention.self.query.bias        |    768     |\n",
      "|        decoder.bert.encoder.layer.5.attention.self.key.weight        |   589824   |\n",
      "|         decoder.bert.encoder.layer.5.attention.self.key.bias         |    768     |\n",
      "|       decoder.bert.encoder.layer.5.attention.self.value.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.5.attention.self.value.bias        |    768     |\n",
      "|      decoder.bert.encoder.layer.5.attention.output.dense.weight      |   589824   |\n",
      "|       decoder.bert.encoder.layer.5.attention.output.dense.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight    |    768     |\n",
      "|     decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias     |    768     |\n",
      "|    decoder.bert.encoder.layer.5.crossattention.self.query.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.5.crossattention.self.query.bias      |    768     |\n",
      "|     decoder.bert.encoder.layer.5.crossattention.self.key.weight      |   589824   |\n",
      "|      decoder.bert.encoder.layer.5.crossattention.self.key.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.5.crossattention.self.value.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.5.crossattention.self.value.bias      |    768     |\n",
      "|   decoder.bert.encoder.layer.5.crossattention.output.dense.weight    |   589824   |\n",
      "|    decoder.bert.encoder.layer.5.crossattention.output.dense.bias     |    768     |\n",
      "| decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight  |    768     |\n",
      "|  decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias   |    768     |\n",
      "|        decoder.bert.encoder.layer.5.intermediate.dense.weight        |  2359296   |\n",
      "|         decoder.bert.encoder.layer.5.intermediate.dense.bias         |    3072    |\n",
      "|           decoder.bert.encoder.layer.5.output.dense.weight           |  2359296   |\n",
      "|            decoder.bert.encoder.layer.5.output.dense.bias            |    768     |\n",
      "|         decoder.bert.encoder.layer.5.output.LayerNorm.weight         |    768     |\n",
      "|          decoder.bert.encoder.layer.5.output.LayerNorm.bias          |    768     |\n",
      "|       decoder.bert.encoder.layer.6.attention.self.query.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.6.attention.self.query.bias        |    768     |\n",
      "|        decoder.bert.encoder.layer.6.attention.self.key.weight        |   589824   |\n",
      "|         decoder.bert.encoder.layer.6.attention.self.key.bias         |    768     |\n",
      "|       decoder.bert.encoder.layer.6.attention.self.value.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.6.attention.self.value.bias        |    768     |\n",
      "|      decoder.bert.encoder.layer.6.attention.output.dense.weight      |   589824   |\n",
      "|       decoder.bert.encoder.layer.6.attention.output.dense.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight    |    768     |\n",
      "|     decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias     |    768     |\n",
      "|    decoder.bert.encoder.layer.6.crossattention.self.query.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.6.crossattention.self.query.bias      |    768     |\n",
      "|     decoder.bert.encoder.layer.6.crossattention.self.key.weight      |   589824   |\n",
      "|      decoder.bert.encoder.layer.6.crossattention.self.key.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.6.crossattention.self.value.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.6.crossattention.self.value.bias      |    768     |\n",
      "|   decoder.bert.encoder.layer.6.crossattention.output.dense.weight    |   589824   |\n",
      "|    decoder.bert.encoder.layer.6.crossattention.output.dense.bias     |    768     |\n",
      "| decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight  |    768     |\n",
      "|  decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias   |    768     |\n",
      "|        decoder.bert.encoder.layer.6.intermediate.dense.weight        |  2359296   |\n",
      "|         decoder.bert.encoder.layer.6.intermediate.dense.bias         |    3072    |\n",
      "|           decoder.bert.encoder.layer.6.output.dense.weight           |  2359296   |\n",
      "|            decoder.bert.encoder.layer.6.output.dense.bias            |    768     |\n",
      "|         decoder.bert.encoder.layer.6.output.LayerNorm.weight         |    768     |\n",
      "|          decoder.bert.encoder.layer.6.output.LayerNorm.bias          |    768     |\n",
      "|       decoder.bert.encoder.layer.7.attention.self.query.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.7.attention.self.query.bias        |    768     |\n",
      "|        decoder.bert.encoder.layer.7.attention.self.key.weight        |   589824   |\n",
      "|         decoder.bert.encoder.layer.7.attention.self.key.bias         |    768     |\n",
      "|       decoder.bert.encoder.layer.7.attention.self.value.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.7.attention.self.value.bias        |    768     |\n",
      "|      decoder.bert.encoder.layer.7.attention.output.dense.weight      |   589824   |\n",
      "|       decoder.bert.encoder.layer.7.attention.output.dense.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.7.attention.output.LayerNorm.weight    |    768     |\n",
      "|     decoder.bert.encoder.layer.7.attention.output.LayerNorm.bias     |    768     |\n",
      "|    decoder.bert.encoder.layer.7.crossattention.self.query.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.7.crossattention.self.query.bias      |    768     |\n",
      "|     decoder.bert.encoder.layer.7.crossattention.self.key.weight      |   589824   |\n",
      "|      decoder.bert.encoder.layer.7.crossattention.self.key.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.7.crossattention.self.value.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.7.crossattention.self.value.bias      |    768     |\n",
      "|   decoder.bert.encoder.layer.7.crossattention.output.dense.weight    |   589824   |\n",
      "|    decoder.bert.encoder.layer.7.crossattention.output.dense.bias     |    768     |\n",
      "| decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight  |    768     |\n",
      "|  decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias   |    768     |\n",
      "|        decoder.bert.encoder.layer.7.intermediate.dense.weight        |  2359296   |\n",
      "|         decoder.bert.encoder.layer.7.intermediate.dense.bias         |    3072    |\n",
      "|           decoder.bert.encoder.layer.7.output.dense.weight           |  2359296   |\n",
      "|            decoder.bert.encoder.layer.7.output.dense.bias            |    768     |\n",
      "|         decoder.bert.encoder.layer.7.output.LayerNorm.weight         |    768     |\n",
      "|          decoder.bert.encoder.layer.7.output.LayerNorm.bias          |    768     |\n",
      "|       decoder.bert.encoder.layer.8.attention.self.query.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.8.attention.self.query.bias        |    768     |\n",
      "|        decoder.bert.encoder.layer.8.attention.self.key.weight        |   589824   |\n",
      "|         decoder.bert.encoder.layer.8.attention.self.key.bias         |    768     |\n",
      "|       decoder.bert.encoder.layer.8.attention.self.value.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.8.attention.self.value.bias        |    768     |\n",
      "|      decoder.bert.encoder.layer.8.attention.output.dense.weight      |   589824   |\n",
      "|       decoder.bert.encoder.layer.8.attention.output.dense.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.8.attention.output.LayerNorm.weight    |    768     |\n",
      "|     decoder.bert.encoder.layer.8.attention.output.LayerNorm.bias     |    768     |\n",
      "|    decoder.bert.encoder.layer.8.crossattention.self.query.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.8.crossattention.self.query.bias      |    768     |\n",
      "|     decoder.bert.encoder.layer.8.crossattention.self.key.weight      |   589824   |\n",
      "|      decoder.bert.encoder.layer.8.crossattention.self.key.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.8.crossattention.self.value.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.8.crossattention.self.value.bias      |    768     |\n",
      "|   decoder.bert.encoder.layer.8.crossattention.output.dense.weight    |   589824   |\n",
      "|    decoder.bert.encoder.layer.8.crossattention.output.dense.bias     |    768     |\n",
      "| decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight  |    768     |\n",
      "|  decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias   |    768     |\n",
      "|        decoder.bert.encoder.layer.8.intermediate.dense.weight        |  2359296   |\n",
      "|         decoder.bert.encoder.layer.8.intermediate.dense.bias         |    3072    |\n",
      "|           decoder.bert.encoder.layer.8.output.dense.weight           |  2359296   |\n",
      "|            decoder.bert.encoder.layer.8.output.dense.bias            |    768     |\n",
      "|         decoder.bert.encoder.layer.8.output.LayerNorm.weight         |    768     |\n",
      "|          decoder.bert.encoder.layer.8.output.LayerNorm.bias          |    768     |\n",
      "|       decoder.bert.encoder.layer.9.attention.self.query.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.9.attention.self.query.bias        |    768     |\n",
      "|        decoder.bert.encoder.layer.9.attention.self.key.weight        |   589824   |\n",
      "|         decoder.bert.encoder.layer.9.attention.self.key.bias         |    768     |\n",
      "|       decoder.bert.encoder.layer.9.attention.self.value.weight       |   589824   |\n",
      "|        decoder.bert.encoder.layer.9.attention.self.value.bias        |    768     |\n",
      "|      decoder.bert.encoder.layer.9.attention.output.dense.weight      |   589824   |\n",
      "|       decoder.bert.encoder.layer.9.attention.output.dense.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.9.attention.output.LayerNorm.weight    |    768     |\n",
      "|     decoder.bert.encoder.layer.9.attention.output.LayerNorm.bias     |    768     |\n",
      "|    decoder.bert.encoder.layer.9.crossattention.self.query.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.9.crossattention.self.query.bias      |    768     |\n",
      "|     decoder.bert.encoder.layer.9.crossattention.self.key.weight      |   589824   |\n",
      "|      decoder.bert.encoder.layer.9.crossattention.self.key.bias       |    768     |\n",
      "|    decoder.bert.encoder.layer.9.crossattention.self.value.weight     |   589824   |\n",
      "|     decoder.bert.encoder.layer.9.crossattention.self.value.bias      |    768     |\n",
      "|   decoder.bert.encoder.layer.9.crossattention.output.dense.weight    |   589824   |\n",
      "|    decoder.bert.encoder.layer.9.crossattention.output.dense.bias     |    768     |\n",
      "| decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight  |    768     |\n",
      "|  decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias   |    768     |\n",
      "|        decoder.bert.encoder.layer.9.intermediate.dense.weight        |  2359296   |\n",
      "|         decoder.bert.encoder.layer.9.intermediate.dense.bias         |    3072    |\n",
      "|           decoder.bert.encoder.layer.9.output.dense.weight           |  2359296   |\n",
      "|            decoder.bert.encoder.layer.9.output.dense.bias            |    768     |\n",
      "|         decoder.bert.encoder.layer.9.output.LayerNorm.weight         |    768     |\n",
      "|          decoder.bert.encoder.layer.9.output.LayerNorm.bias          |    768     |\n",
      "|      decoder.bert.encoder.layer.10.attention.self.query.weight       |   589824   |\n",
      "|       decoder.bert.encoder.layer.10.attention.self.query.bias        |    768     |\n",
      "|       decoder.bert.encoder.layer.10.attention.self.key.weight        |   589824   |\n",
      "|        decoder.bert.encoder.layer.10.attention.self.key.bias         |    768     |\n",
      "|      decoder.bert.encoder.layer.10.attention.self.value.weight       |   589824   |\n",
      "|       decoder.bert.encoder.layer.10.attention.self.value.bias        |    768     |\n",
      "|     decoder.bert.encoder.layer.10.attention.output.dense.weight      |   589824   |\n",
      "|      decoder.bert.encoder.layer.10.attention.output.dense.bias       |    768     |\n",
      "|   decoder.bert.encoder.layer.10.attention.output.LayerNorm.weight    |    768     |\n",
      "|    decoder.bert.encoder.layer.10.attention.output.LayerNorm.bias     |    768     |\n",
      "|    decoder.bert.encoder.layer.10.crossattention.self.query.weight    |   589824   |\n",
      "|     decoder.bert.encoder.layer.10.crossattention.self.query.bias     |    768     |\n",
      "|     decoder.bert.encoder.layer.10.crossattention.self.key.weight     |   589824   |\n",
      "|      decoder.bert.encoder.layer.10.crossattention.self.key.bias      |    768     |\n",
      "|    decoder.bert.encoder.layer.10.crossattention.self.value.weight    |   589824   |\n",
      "|     decoder.bert.encoder.layer.10.crossattention.self.value.bias     |    768     |\n",
      "|   decoder.bert.encoder.layer.10.crossattention.output.dense.weight   |   589824   |\n",
      "|    decoder.bert.encoder.layer.10.crossattention.output.dense.bias    |    768     |\n",
      "| decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight |    768     |\n",
      "|  decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias  |    768     |\n",
      "|       decoder.bert.encoder.layer.10.intermediate.dense.weight        |  2359296   |\n",
      "|        decoder.bert.encoder.layer.10.intermediate.dense.bias         |    3072    |\n",
      "|          decoder.bert.encoder.layer.10.output.dense.weight           |  2359296   |\n",
      "|           decoder.bert.encoder.layer.10.output.dense.bias            |    768     |\n",
      "|        decoder.bert.encoder.layer.10.output.LayerNorm.weight         |    768     |\n",
      "|         decoder.bert.encoder.layer.10.output.LayerNorm.bias          |    768     |\n",
      "|      decoder.bert.encoder.layer.11.attention.self.query.weight       |   589824   |\n",
      "|       decoder.bert.encoder.layer.11.attention.self.query.bias        |    768     |\n",
      "|       decoder.bert.encoder.layer.11.attention.self.key.weight        |   589824   |\n",
      "|        decoder.bert.encoder.layer.11.attention.self.key.bias         |    768     |\n",
      "|      decoder.bert.encoder.layer.11.attention.self.value.weight       |   589824   |\n",
      "|       decoder.bert.encoder.layer.11.attention.self.value.bias        |    768     |\n",
      "|     decoder.bert.encoder.layer.11.attention.output.dense.weight      |   589824   |\n",
      "|      decoder.bert.encoder.layer.11.attention.output.dense.bias       |    768     |\n",
      "|   decoder.bert.encoder.layer.11.attention.output.LayerNorm.weight    |    768     |\n",
      "|    decoder.bert.encoder.layer.11.attention.output.LayerNorm.bias     |    768     |\n",
      "|    decoder.bert.encoder.layer.11.crossattention.self.query.weight    |   589824   |\n",
      "|     decoder.bert.encoder.layer.11.crossattention.self.query.bias     |    768     |\n",
      "|     decoder.bert.encoder.layer.11.crossattention.self.key.weight     |   589824   |\n",
      "|      decoder.bert.encoder.layer.11.crossattention.self.key.bias      |    768     |\n",
      "|    decoder.bert.encoder.layer.11.crossattention.self.value.weight    |   589824   |\n",
      "|     decoder.bert.encoder.layer.11.crossattention.self.value.bias     |    768     |\n",
      "|   decoder.bert.encoder.layer.11.crossattention.output.dense.weight   |   589824   |\n",
      "|    decoder.bert.encoder.layer.11.crossattention.output.dense.bias    |    768     |\n",
      "| decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight |    768     |\n",
      "|  decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias  |    768     |\n",
      "|       decoder.bert.encoder.layer.11.intermediate.dense.weight        |  2359296   |\n",
      "|        decoder.bert.encoder.layer.11.intermediate.dense.bias         |    3072    |\n",
      "|          decoder.bert.encoder.layer.11.output.dense.weight           |  2359296   |\n",
      "|           decoder.bert.encoder.layer.11.output.dense.bias            |    768     |\n",
      "|        decoder.bert.encoder.layer.11.output.LayerNorm.weight         |    768     |\n",
      "|         decoder.bert.encoder.layer.11.output.LayerNorm.bias          |    768     |\n",
      "|                     decoder.cls.predictions.bias                     |   119547   |\n",
      "|            decoder.cls.predictions.transform.dense.weight            |   589824   |\n",
      "|             decoder.cls.predictions.transform.dense.bias             |    768     |\n",
      "|          decoder.cls.predictions.transform.LayerNorm.weight          |    768     |\n",
      "|           decoder.cls.predictions.transform.LayerNorm.bias           |    768     |\n",
      "+----------------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 384194811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "384194811"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "count_parameters(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5ad4a089-142e-4c1b-bd56-1456dfa32686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting prettytable\n",
      "  Downloading prettytable-3.6.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: wcwidth in ./.local/lib/python3.7/site-packages (from prettytable) (0.2.5)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from prettytable) (4.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in ./.local/lib/python3.7/site-packages (from importlib-metadata->prettytable) (4.1.1)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6e07a1e0-62d2-4eed-8659-a9472d5e8f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 1465.603MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in trained_model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in trained_model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2367d4c9-e1d0-4f58-8c86-521cecfc13ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchview\n",
      "  Downloading torchview-0.2.6-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: torchview\n",
      "Successfully installed torchview-0.2.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9669013c-f34f-44b1-a35c-f265f106a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grammar(input_text,num_return_sequences):\n",
    "    #batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\").to(torch_device)\n",
    "    batch = tokenizer1([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\",return_token_type_ids=False).to(device)\n",
    "    #print(batch)\n",
    "    #translated = model1.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    translated= trained_model.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    #print(\"here\")\n",
    "    tgt_text = tokenizer1.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b786ccc2-8dea-4c4d-b60c-c242ef187a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(T5ForConditionalGeneration.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2dc10f7d-909a-4d2b-917a-475e43619610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसकी वर्चस्व सर्वत्र स्थापित है ।\n",
      "Correct Seentence: अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसका वर्चस्व सर्वत्र स्थापित है । \n",
      "Predicted Sentence: अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसका वर्चस्व सर्वत्र स्थापित है । \n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[0]\n",
    "correct = test_df['output'].iat[0]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "l = len(correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=4)[0]\n",
    "p=len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "33497c92-ccc5-4eba-89d9-004161a5e100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: प्रत्येक शिक्षक जानता हैं कि उसके मासिक वेतन का कुछ अंश घूस में जा रहा हैं ।\n",
      "Correct Seentence: प्रत्येक शिक्षक जानता है कि उसके मासिक वेतन का कुछ अंश घूस में जा रहा है । \n",
      "Predicted Sentence: प्रत्येक शिक्षक जानता है कि उसके मासिक वेतन का कुछ अंश घूस में जा रहा है । \n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[13]\n",
    "correct = test_df['output'].iat[13]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "l = len(correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=2)[0]\n",
    "p = len(predicted_s)\n",
    "\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3f8b0823-1f45-41f2-a239-c4b724f58309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: इस सबमें अहम बात मुस्लिम मतदाताओं का रुझान हैं ।\n",
      "Correct Seentence: इस सबमें अहम बात मुस्लिम मतदाताओं का रुझान है । \n",
      "Predicted Sentence: इस सबमें अहम बात मुस्लिम मतदाताओं का रुझान है । \n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[43]\n",
    "correct = test_df['output'].iat[43]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "l = len(correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=2)[0]\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "79771c80-b79f-4ac2-8086-4eb0a0d82051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: तथे गौशाला का\n",
      "Correct Seentence: तथा गौशाला का\n",
      "Predicted Sentence: तथा गौशाला का\n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[1]\n",
    "correct = test_df['output'].iat[1]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "l = len(correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=4)[0][:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed1ae34f-0274-43ec-afed-f0ce7e723612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसकी वर्चस्व सर्वत्र स्थापित है ।\n",
      "Correct Seentence: अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसका वर्चस्व सर्वत्र स्थापित है । \n",
      "Predicted Sentence: अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसका वर्चस्व सर्वत्र स्थापित है । \n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[0]\n",
    "correct = test_df['output'].iat[0]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "l = len(correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=2)[0]\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a3c96d0d-6d65-42bc-8faa-30a3b931c57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: गनीमत हैं गणेशशंकर विद्यार्थी ने २०० रुपये का मनीआर्डर भेजकर अशफ़ाक़ की मजार पर छत डलवा कर उसे धूप के साये से बचा लिया ।\n",
      "Correct Seentence: गनीमत है गणेशशंकर विद्यार्थी ने २०० रुपये का मनीआर्डर भेजकर अशफ़ाक़ की मजार पर छत डलवा कर उसे धूप के साये से बचा लिया । \n",
      "Predicted Sentence: गनीमत है गणेशशंकर विद्यार्थी ने २०० रुपये का मनीआर्डर भेजकर अशफ़ाक़ की मजार पर छत डलवा कर उसे धूप के साये से बचा लिया । \n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[51]\n",
    "correct = test_df['output'].iat[51]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9f05b016-c983-4167-a2f7-c5f1736de779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ज़ाहिर है जब मैंने लिखूंगा तो वो मेरे ही व्यक्तिगत विचार कहलायेंगे किसी संस्थान के नहीं ।\n",
      "Correct Seentence: ज़ाहिर है जब मैं लिखूंगा तो वो मेरे ही व्यक्तिगत विचार कहलायेंगे किसी संस्थान के नहीं । \n",
      "Predicted Sentence: ज़ाहिर है जब मैं लिखूंगा तो वो मेरे ही व्यक्तिगत विचार कहलायेंगे किसी संस्थान के नहीं । \n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[60]\n",
    "correct = test_df['output'].iat[60]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7cf190-899c-4fc6-9954-59ba79ef6614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d488748e-8345-4b94-baf3-84925a29ba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: गनीमत हैं गणेशशंकर विद्यार्थी ने २०० रुपये का मनीआर्डर भेजकर अशफ़ाक़ की मजार पर छत डलवा कर उसे धूप के साये से बचा लिया ।\n",
      "Correct Seentence: गनीमत है गणेशशंकर विद्यार्थी ने २०० रुपये का मनीआर्डर भेजकर अशफ़ाक़ की मजार पर छत डलवा कर उसे धूप के साये से बचा लिया । \n",
      "Predicted Sentence: गनीमत है गणेशशंकर विद्यार्थी ने २०० रुपये का मनीआर्डर भेजकर अशफ़ाक़ की मजार पर छत डलवा कर उसे धूप के साये से बचा लिया । \n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[51]\n",
    "correct = test_df['output'].iat[51]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7ee9ef54-c851-49e0-9c2b-8ea53c26705a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 23.9 ms, total: 1.14 s\n",
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a2adc94a-4fc3-4eb6-b336-574d99c230eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: हम पास खींच लेते हैंं ।\n",
      "Correct Seentence: हम पास खींच लेते हैं । \n",
      "Predicted Sentence: हम पास खींच लेते हैं । \n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[20]\n",
    "correct = test_df['output'].iat[20]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "p=len(predicted_s)\n",
    "l = len(text)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b37d9f-25df-483f-a3c3-29603e854408",
   "metadata": {},
   "source": [
    "# Performance Analysis\n",
    "1. BLEU Score\n",
    "2. GLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18317b87-03d3-4d73-ae22-9cf8cb8e1db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da058ef4-524c-4808-a1ba-bbd143f534d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>input_token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40665</th>\n",
       "      <td>अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसकी वर्चस्व सर्वत्र स्थापित है ।</td>\n",
       "      <td>अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसका वर्चस्व सर्वत्र स्थापित है ।</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48520</th>\n",
       "      <td>तथे गौशाला का</td>\n",
       "      <td>तथा गौशाला का</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138403</th>\n",
       "      <td>बच्चे खेल रहे है किलकारियाँ गूंज रही है ब्रह्माण्ड में</td>\n",
       "      <td>बच्चे खेल रहे हैं किलकारियाँ गूंज रही हैं ब्रह्माण्ड में</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130079</th>\n",
       "      <td>धरने पर किसानो ने कहा कि अब समय आ गया हैं कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी हैं ।</td>\n",
       "      <td>धरने पर किसानो ने कहा कि अब समय आ गया है कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी है ।</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50146</th>\n",
       "      <td>सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा हैं ।</td>\n",
       "      <td>सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा है ।</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        input  \\\n",
       "40665         अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसकी वर्चस्व सर्वत्र स्थापित है ।   \n",
       "48520                                                                                           तथे गौशाला का   \n",
       "138403                                                 बच्चे खेल रहे है किलकारियाँ गूंज रही है ब्रह्माण्ड में   \n",
       "130079  धरने पर किसानो ने कहा कि अब समय आ गया हैं कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी हैं ।   \n",
       "50146                   सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा हैं ।   \n",
       "\n",
       "                                                                                                      output  \\\n",
       "40665       अँगरेज़ी सहभाषा के रूप में व्यवहत्त होनी चाहिए थी परंतु आज भी उसका वर्चस्व सर्वत्र स्थापित है ।    \n",
       "48520                                                                                          तथा गौशाला का   \n",
       "138403                                              बच्चे खेल रहे हैं किलकारियाँ गूंज रही हैं ब्रह्माण्ड में   \n",
       "130079  धरने पर किसानो ने कहा कि अब समय आ गया है कि गन्ने की खेती किसानो के लिये नुकसान का सौदा हो गयी है ।    \n",
       "50146                  सड़क मलबे का ढेर होने से दुपहिया व चार पहिया वाहन चालकों को परेशान होना पड़ रहा है ।    \n",
       "\n",
       "        input_token_len  \n",
       "40665                36  \n",
       "48520                 9  \n",
       "138403               25  \n",
       "130079               44  \n",
       "50146                37  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c922895e-82d6-43db-a2c8-113bacd42372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv('DATA/etoori_test.csv')\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab40e4eb-00a5-429e-a837-ed75ea61cc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरारी कार भी को मिलियन डॉलर क्लब में माना जा रहा है ।</td>\n",
       "      <td>इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरारी कार को भी मिलियन डॉलर क्लब में माना जा रहा है ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>यह मन को काबू में करने वाली मुद्रा हैं इसीलिए इसे चित्त हस्त मुद्रा योग कहते हैंं ।</td>\n",
       "      <td>यह मन को काबू में करने वाली मुद्रा है इसीलिए इसे चित्त हस्त मुद्रा योग कहते हैं ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा उसनेाँ पर शोर मचा रहा है ।</td>\n",
       "      <td>आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा वहाँ पर शोर मचा रहा है ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>परिवार के मुताबिक धमाकों में हिस्सा लेने वाले कोई दूसरे लोग थे और ये गलत पहचान का मामला हैं ।</td>\n",
       "      <td>परिवार के मुताबिक धमाकों में हिस्सा लेने वाले कोई दूसरे लोग थे और ये गलत पहचान का मामला है ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>उनकी वो वाली बात भी अनिश्चित रहती हैं ।</td>\n",
       "      <td>उनकी वो वाली बात भी अनिश्चित रहती है ।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         enc_input  \\\n",
       "0  इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरारी कार भी को मिलियन डॉलर क्लब में माना जा रहा है ।   \n",
       "1              यह मन को काबू में करने वाली मुद्रा हैं इसीलिए इसे चित्त हस्त मुद्रा योग कहते हैंं ।   \n",
       "2                               आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा उसनेाँ पर शोर मचा रहा है ।   \n",
       "3    परिवार के मुताबिक धमाकों में हिस्सा लेने वाले कोई दूसरे लोग थे और ये गलत पहचान का मामला हैं ।   \n",
       "4                                                          उनकी वो वाली बात भी अनिश्चित रहती हैं ।   \n",
       "\n",
       "                                                                                          dec_input  \n",
       "0  इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरारी कार को भी मिलियन डॉलर क्लब में माना जा रहा है ।   \n",
       "1                यह मन को काबू में करने वाली मुद्रा है इसीलिए इसे चित्त हस्त मुद्रा योग कहते हैं ।   \n",
       "2                                 आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा वहाँ पर शोर मचा रहा है ।   \n",
       "3     परिवार के मुताबिक धमाकों में हिस्सा लेने वाले कोई दूसरे लोग थे और ये गलत पहचान का मामला है ।   \n",
       "4                                                           उनकी वो वाली बात भी अनिश्चित रहती है ।   "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9216427e-77e4-4d58-a4c1-0fd582745e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरारी कार भी को मिलियन डॉलर क्लब में माना जा रहा है ।\n",
      "Correct Seentence: इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरारी कार को भी मिलियन डॉलर क्लब में माना जा रहा है । \n",
      "Predicted Sentence: इसके अलावा माइकल शूमाकर द्वारा चलाई गई एक फरारी कार को भी मिलियन डॉलर क्लब में माना जा रहा है । \n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[0]\n",
    "correct = df_test['dec_input'].iat[0]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba007951-7c5e-4d99-8ca6-e9a4aa81d026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: यह मन को काबू में करने वाली मुद्रा हैं इसीलिए इसे चित्त हस्त मुद्रा योग कहते हैंं ।\n",
      "Correct Seentence: यह मन को काबू में करने वाली मुद्रा है इसीलिए इसे चित्त हस्त मुद्रा योग कहते हैं । \n",
      "Predicted Sentence: यह मन को काबू में करने वाली मुद्रा है इसीलिए इसे चित्त हस्त मुद्रा योग कहते हैं । \n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[1]\n",
    "correct = df_test['dec_input'].iat[1]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c0c0e87-c89e-4143-bacf-d6fe15b9cca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा उसनेाँ पर शोर मचा रहा है ।\n",
      "Correct Seentence: आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा वहाँ पर शोर मचा रहा है । \n",
      "Predicted Sentence: आप पुस्तक पढ़ने में तल्लीन हैं और बच्चा वहाँ पर शोर मचा रहा है । \n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[2]\n",
    "correct = df_test['dec_input'].iat[2]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6f233a47-440e-49b7-ba4f-b952be54da5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: परिवार के मुताबिक धमाकों में हिस्सा लेने वाले कोई दूसरे लोग थे और ये गलत पहचान का मामला हैं ।\n",
      "Correct Seentence: परिवार के मुताबिक धमाकों में हिस्सा लेने वाले कोई दूसरे लोग थे और ये गलत पहचान का मामला है । \n",
      "Predicted Sentence: परिवार के मुताबिक धमाकों में हिस्सा लेने वाले कोई दूसरे लोग थे और ये गलत पहचान का मामला है । \n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[3]\n",
    "correct = df_test['dec_input'].iat[3]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7ff4d00c-6d49-4e6e-b4f9-a765967fa1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: उनकी वो वाली बात भी अनिश्चित रहती हैं ।\n",
      "Correct Seentence: उनकी वो वाली बात भी अनिश्चित रहती है । \n",
      "Predicted Sentence: उनकी वो वाली बात भी अनिश्चित रहती है । \n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[4]\n",
    "correct = df_test['dec_input'].iat[4]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ad90991-9465-4991-9562-bd6e88cfe5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: मैंने ललित हूँ ।\n",
      "Correct Seentence: मैं ललित हूँ । \n",
      "Predicted Sentence: मैं ललित हूँ । \n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[9]\n",
    "correct = df_test['dec_input'].iat[9]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f592c8-73e0-485f-bfe7-a6b0b40c4382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb825de7-9bcd-4d6e-8ba4-433f7ac00ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  1  data point 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [19:12,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  1001  data point 0.9146081119649399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [38:39,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  2001  data point 0.9168191401240283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [58:17,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  3001  data point 0.916348388219836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [1:17:47,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  4001  data point 0.9163256339310865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [1:37:02,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  5001  data point 0.916290736338747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6001it [1:56:16,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  6001  data point 0.9161448853897615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7001it [2:15:33,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  7001  data point 0.9160246959311623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8001it [2:34:45,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  8001  data point 0.9165724757187045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9001it [2:53:46,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  9001  data point 0.916987191679546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [3:12:49,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score =  0.9175768315121448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test.head(10000)\n",
    "print(test_data.shape)\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        GLEU_val_emb.append(b)\n",
    "        if(ind%1000 ==0):\n",
    "            print(\"GELU Score for \",ind+1,\" data point\",np.mean(GLEU_val_emb))\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8f776a52-2f84-4b17-a268-62f049e9cc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score =  0.9175768315121448\n"
     ]
    }
   ],
   "source": [
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3252f34-628a-4e12-9569-b81228062c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85b2481-c407-4c08-b4b9-d7a29c76cbda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
