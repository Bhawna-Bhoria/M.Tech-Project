{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9c7780-28f4-4676-8620-cfeffba88c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 28 10:04:45 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.84       Driver Version: 460.84       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 40%   72C    P2   153W / 250W |   4945MiB / 11178MiB |     63%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 43%   74C    P2   163W / 250W |   2565MiB / 11178MiB |     67%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:07:00.0 Off |                  N/A |\n",
      "| 40%   71C    P2   204W / 250W |   5480MiB / 11178MiB |     68%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "| 40%   72C    P2   208W / 250W |   2537MiB / 11178MiB |     70%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce GTX 108...  Off  | 00000000:0C:00.0 Off |                  N/A |\n",
      "| 20%   35C    P8     8W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce GTX 108...  Off  | 00000000:0D:00.0 Off |                  N/A |\n",
      "| 20%   22C    P8     7W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce GTX 108...  Off  | 00000000:0E:00.0 Off |                  N/A |\n",
      "| 34%   67C    P2    99W / 250W |   5483MiB / 11178MiB |     26%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce GTX 108...  Off  | 00000000:0F:00.0 Off |                  N/A |\n",
      "| 20%   25C    P8     8W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      8860      C   /usr/bin/python3                 2551MiB |\n",
      "|    0   N/A  N/A      8861      C   /usr/bin/python3                  649MiB |\n",
      "|    0   N/A  N/A      8862      C   /usr/bin/python3                  649MiB |\n",
      "|    0   N/A  N/A      8863      C   /usr/bin/python3                  649MiB |\n",
      "|    0   N/A  N/A     38504      C   /opt/conda/bin/python             445MiB |\n",
      "|    1   N/A  N/A      8861      C   /usr/bin/python3                 2563MiB |\n",
      "|    2   N/A  N/A      8862      C   /usr/bin/python3                 2559MiB |\n",
      "|    2   N/A  N/A     14515      C   python3                          2919MiB |\n",
      "|    3   N/A  N/A      8863      C   /usr/bin/python3                 2535MiB |\n",
      "|    6   N/A  N/A      9079      C   python3                          5481MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1472674-e209-489e-93b9-3d3c8da03a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, \"0\" to  \"7\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4,5,6,7\"\n",
    "#torch.cuda.set_device(0)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(n_gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba40e0c-5fcc-43a9-8d28-47ff9e9870f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "340acc79-3319-47aa-8140-07059948a04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /DATA/gupta92/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "#Set a seed\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7712b672-2f06-47dd-906a-42ab51326a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc22694-eff1-4618-ad9c-8b6110972cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4aaf87c-52e6-4e3d-870c-1955b9afd376",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e4420cd-d086-467d-8384-bc29bb4d91ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('DATA/etoori_train.csv')\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5382e90c-56d2-4370-876c-5829c0c3c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44742dea-9094-4df9-809a-f4c662c755da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('DATA/Hindi_Artificial_train.csv')\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0430727a-5f58-494a-a2af-70c742d2f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60dd0008-7592-44dd-a2af-0d748bd383d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡§™‡§∞‡§®‡•ç‡§§‡•Å ‡§µ‡•á ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§â‡§® ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§ú‡§º‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§∏‡§Æ‡§Ø ‡§§‡§ï ‡§Ö‡§™ ‡§π‡•É‡§¶‡§Ø ‡§Æ‡•á‡§Ç ‡§ó‡§æ‡§Å‡§† ‡§¨‡§®‡§æ‡§ï‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§ñ‡§§‡•á ‡§•‡•á ‡•§</td>\n",
       "      <td>‡§™‡§∞‡§®‡•ç‡§§‡•Å ‡§µ‡•á ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§â‡§® ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§ú‡§º‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§∏‡§Æ‡§Ø ‡§§‡§ï ‡§Ö‡§™‡§®‡•á ‡§π‡•É‡§¶‡§Ø ‡§Æ‡•á‡§Ç ‡§ó‡§æ‡§Å‡§† ‡§¨‡§®‡§æ‡§ï‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§ñ‡§§‡•á ‡§•‡•á ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ï‡•ã ‡§µ‡§ø‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞ ‡§ï‡§æ ‡§∑‡§°‡§º‡§Ø‡§Ç‡§§‡•ç‡§∞ ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à ‡§ú‡•ã ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§ï‡§æ ‡§µ‡§ø‡§∑‡§Ø ‡§π‡•à ‡•§</td>\n",
       "      <td>‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ï‡•ã ‡§µ‡§ø‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ ‡§∑‡§°‡§º‡§Ø‡§Ç‡§§‡•ç‡§∞ ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à ‡§ú‡•ã ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§ï‡§æ ‡§µ‡§ø‡§∑‡§Ø ‡§π‡•à ‡•§</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          enc_input  \\\n",
       "0  ‡§™‡§∞‡§®‡•ç‡§§‡•Å ‡§µ‡•á ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§â‡§® ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§ú‡§º‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§∏‡§Æ‡§Ø ‡§§‡§ï ‡§Ö‡§™ ‡§π‡•É‡§¶‡§Ø ‡§Æ‡•á‡§Ç ‡§ó‡§æ‡§Å‡§† ‡§¨‡§®‡§æ‡§ï‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§ñ‡§§‡•á ‡§•‡•á ‡•§   \n",
       "1        ‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ï‡•ã ‡§µ‡§ø‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞ ‡§ï‡§æ ‡§∑‡§°‡§º‡§Ø‡§Ç‡§§‡•ç‡§∞ ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à ‡§ú‡•ã ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§ï‡§æ ‡§µ‡§ø‡§∑‡§Ø ‡§π‡•à ‡•§   \n",
       "\n",
       "                                                                             dec_input  \n",
       "0  ‡§™‡§∞‡§®‡•ç‡§§‡•Å ‡§µ‡•á ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§â‡§® ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§ú‡§º‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§∏‡§Æ‡§Ø ‡§§‡§ï ‡§Ö‡§™‡§®‡•á ‡§π‡•É‡§¶‡§Ø ‡§Æ‡•á‡§Ç ‡§ó‡§æ‡§Å‡§† ‡§¨‡§®‡§æ‡§ï‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§ñ‡§§‡•á ‡§•‡•á ‡•§   \n",
       "1        ‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ï‡•ã ‡§µ‡§ø‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ ‡§∑‡§°‡§º‡§Ø‡§Ç‡§§‡•ç‡§∞ ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à ‡§ú‡•ã ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§ï‡§æ ‡§µ‡§ø‡§∑‡§Ø ‡§π‡•à ‡•§   "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f23822f-77c3-4e13-9245-404f08e1743a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fc84c40-1152-4f97-b749-186365fff1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'enc_input':'input'}, inplace = True)\n",
    "df.rename(columns = {'dec_input':'output'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b51b0af9-4f5f-4e8f-92e5-10d8ed20e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dd252f1-a794-4cef-be0b-1b59e0d8185c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡§™‡§∞‡§®‡•ç‡§§‡•Å ‡§µ‡•á ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§â‡§® ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§ú‡§º‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§∏‡§Æ‡§Ø ‡§§‡§ï ‡§Ö‡§™ ‡§π‡•É‡§¶‡§Ø ‡§Æ‡•á‡§Ç ‡§ó‡§æ‡§Å‡§† ‡§¨‡§®‡§æ‡§ï‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§ñ‡§§‡•á ‡§•‡•á ‡•§</td>\n",
       "      <td>‡§™‡§∞‡§®‡•ç‡§§‡•Å ‡§µ‡•á ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§â‡§® ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§ú‡§º‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§∏‡§Æ‡§Ø ‡§§‡§ï ‡§Ö‡§™‡§®‡•á ‡§π‡•É‡§¶‡§Ø ‡§Æ‡•á‡§Ç ‡§ó‡§æ‡§Å‡§† ‡§¨‡§®‡§æ‡§ï‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§ñ‡§§‡•á ‡§•‡•á ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ï‡•ã ‡§µ‡§ø‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞ ‡§ï‡§æ ‡§∑‡§°‡§º‡§Ø‡§Ç‡§§‡•ç‡§∞ ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à ‡§ú‡•ã ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§ï‡§æ ‡§µ‡§ø‡§∑‡§Ø ‡§π‡•à ‡•§</td>\n",
       "      <td>‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ï‡•ã ‡§µ‡§ø‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ ‡§∑‡§°‡§º‡§Ø‡§Ç‡§§‡•ç‡§∞ ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à ‡§ú‡•ã ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§ï‡§æ ‡§µ‡§ø‡§∑‡§Ø ‡§π‡•à ‡•§</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              input  \\\n",
       "0  ‡§™‡§∞‡§®‡•ç‡§§‡•Å ‡§µ‡•á ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§â‡§® ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§ú‡§º‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§∏‡§Æ‡§Ø ‡§§‡§ï ‡§Ö‡§™ ‡§π‡•É‡§¶‡§Ø ‡§Æ‡•á‡§Ç ‡§ó‡§æ‡§Å‡§† ‡§¨‡§®‡§æ‡§ï‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§ñ‡§§‡•á ‡§•‡•á ‡•§   \n",
       "1        ‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ï‡•ã ‡§µ‡§ø‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞ ‡§ï‡§æ ‡§∑‡§°‡§º‡§Ø‡§Ç‡§§‡•ç‡§∞ ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à ‡§ú‡•ã ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§ï‡§æ ‡§µ‡§ø‡§∑‡§Ø ‡§π‡•à ‡•§   \n",
       "\n",
       "                                                                                output  \n",
       "0  ‡§™‡§∞‡§®‡•ç‡§§‡•Å ‡§µ‡•á ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§â‡§® ‡§¨‡§æ‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§ú‡§º‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§∏‡§Æ‡§Ø ‡§§‡§ï ‡§Ö‡§™‡§®‡•á ‡§π‡•É‡§¶‡§Ø ‡§Æ‡•á‡§Ç ‡§ó‡§æ‡§Å‡§† ‡§¨‡§®‡§æ‡§ï‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§ñ‡§§‡•á ‡§•‡•á ‡•§   \n",
       "1        ‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ï‡•ã ‡§µ‡§ø‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ ‡§∑‡§°‡§º‡§Ø‡§Ç‡§§‡•ç‡§∞ ‡§ö‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à ‡§ú‡•ã ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§ï‡§æ ‡§µ‡§ø‡§∑‡§Ø ‡§π‡•à ‡•§   "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c81e7ec2-d0fa-45d6-be30-7c2e2877dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration, T5Tokenizer, \n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "  )\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c509b356-7320-4dbb-9184-9750dd208a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fc016f6-2311-4978-a799-f40e96abaef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='google/muril-base-cased', vocab_size=197285, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbc19d2f-5c42-46ea-8c21-decc47553c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98a081dd-fbb1-4b1d-a762-4d99ecc0ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModelForMaskedLM.from_pretrained(\"google/muril-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b199d7a2-2b5d-4904-b204-a379c34aefca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import MT5Model, AutoTokenizer,MT5ForConditionalGeneration\n",
    "\n",
    "#model_name ='t5_gec_hindi_muRIL_best'\n",
    "#model_name = \"google/mt5-base\"\n",
    "#model_name = \"mT5_Full_1\"\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n",
    "#model_name = \"google/mt5-small\"\n",
    "model_name =\"mT5_Etoori_2\"\n",
    "#tokenizer = T5Tokenizer.from_pretrained(model_name).to(torch_device)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2bf371-139e-4fbd-9cc4-1e99e2244ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82d8c1-f015-4374-be7b-56a7d9b8010b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "760e2b81-d01b-4ba0-8498-d9789d71afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, source_lang, target_lang):\n",
    "    input_text = f\"{source_lang} to {target_lang}: {text}\"\n",
    "    #input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(torch_device)\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(torch_device)\n",
    "    outputs = model.generate(input_ids=input_ids, max_length=64, num_beams=4, early_stopping=True)\n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c17de668-5831-499e-a38b-1133502dda6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##‡§Ç‡§¶‡§ï ‡§¨‡•Ä ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§§‡§ï ‡§¨‡•Ä\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, how are you?\"\n",
    "source_lang = \"english\"\n",
    "target_lang = \"french\"\n",
    "translation = translate(text, source_lang, target_lang)\n",
    "print(translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c6adf-a47f-4ed9-b110-36846927ebee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aca04df4-d7de-4f44-9800-94eb1c5296ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_token_len(example):\n",
    "    return len(tokenizer(example).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9821b86-5a8b-4956-856c-d4c88d482f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((126000, 2), (14000, 2))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train - Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.10, shuffle=True)\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c514352d-1338-4a42-95a6-36151e6b0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['input_token_len'] = test_df['input'].apply(calc_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9df2945-fb4d-4f63-b24b-2571dceba51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>input_token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40665</th>\n",
       "      <td>‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡•Ä ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§</td>\n",
       "      <td>‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡§æ ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48520</th>\n",
       "      <td>‡§§‡§•‡•á ‡§ó‡•å‡§∂‡§æ‡§≤‡§æ ‡§ï‡§æ</td>\n",
       "      <td>‡§§‡§•‡§æ ‡§ó‡•å‡§∂‡§æ‡§≤‡§æ ‡§ï‡§æ</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138403</th>\n",
       "      <td>‡§¨‡§ö‡•ç‡§ö‡•á ‡§ñ‡•á‡§≤ ‡§∞‡§π‡•á ‡§π‡•à ‡§ï‡§ø‡§≤‡§ï‡§æ‡§∞‡§ø‡§Ø‡§æ‡§Å ‡§ó‡•Ç‡§Ç‡§ú ‡§∞‡§π‡•Ä ‡§π‡•à ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§æ‡§£‡•ç‡§° ‡§Æ‡•á‡§Ç</td>\n",
       "      <td>‡§¨‡§ö‡•ç‡§ö‡•á ‡§ñ‡•á‡§≤ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§ï‡§ø‡§≤‡§ï‡§æ‡§∞‡§ø‡§Ø‡§æ‡§Å ‡§ó‡•Ç‡§Ç‡§ú ‡§∞‡§π‡•Ä ‡§π‡•à‡§Ç ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§æ‡§£‡•ç‡§° ‡§Æ‡•á‡§Ç</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130079</th>\n",
       "      <td>‡§ß‡§∞‡§®‡•á ‡§™‡§∞ ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø ‡§Ö‡§¨ ‡§∏‡§Æ‡§Ø ‡§Ü ‡§ó‡§Ø‡§æ ‡§π‡•à‡§Ç ‡§ï‡§ø ‡§ó‡§®‡•ç‡§®‡•á ‡§ï‡•Ä ‡§ñ‡•á‡§§‡•Ä ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§ï‡•á ‡§≤‡§ø‡§Ø‡•á ‡§®‡•Å‡§ï‡§∏‡§æ‡§® ‡§ï‡§æ ‡§∏‡•å‡§¶‡§æ ‡§π‡•ã ‡§ó‡§Ø‡•Ä ‡§π‡•à‡§Ç ‡•§</td>\n",
       "      <td>‡§ß‡§∞‡§®‡•á ‡§™‡§∞ ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø ‡§Ö‡§¨ ‡§∏‡§Æ‡§Ø ‡§Ü ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ó‡§®‡•ç‡§®‡•á ‡§ï‡•Ä ‡§ñ‡•á‡§§‡•Ä ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§ï‡•á ‡§≤‡§ø‡§Ø‡•á ‡§®‡•Å‡§ï‡§∏‡§æ‡§® ‡§ï‡§æ ‡§∏‡•å‡§¶‡§æ ‡§π‡•ã ‡§ó‡§Ø‡•Ä ‡§π‡•à ‡•§</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50146</th>\n",
       "      <td>‡§∏‡§°‡§º‡§ï ‡§Æ‡§≤‡§¨‡•á ‡§ï‡§æ ‡§¢‡•á‡§∞ ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§¶‡•Å‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ ‡§ö‡§æ‡§∞ ‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ‡§æ‡§π‡§® ‡§ö‡§æ‡§≤‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§∞‡•á‡§∂‡§æ‡§® ‡§π‡•ã‡§®‡§æ ‡§™‡§°‡§º ‡§∞‡§π‡§æ ‡§π‡•à‡§Ç ‡•§</td>\n",
       "      <td>‡§∏‡§°‡§º‡§ï ‡§Æ‡§≤‡§¨‡•á ‡§ï‡§æ ‡§¢‡•á‡§∞ ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§¶‡•Å‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ ‡§ö‡§æ‡§∞ ‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ‡§æ‡§π‡§® ‡§ö‡§æ‡§≤‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§∞‡•á‡§∂‡§æ‡§® ‡§π‡•ã‡§®‡§æ ‡§™‡§°‡§º ‡§∞‡§π‡§æ ‡§π‡•à ‡•§</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        input  \\\n",
       "40665         ‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡•Ä ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§   \n",
       "48520                                                                                           ‡§§‡§•‡•á ‡§ó‡•å‡§∂‡§æ‡§≤‡§æ ‡§ï‡§æ   \n",
       "138403                                                 ‡§¨‡§ö‡•ç‡§ö‡•á ‡§ñ‡•á‡§≤ ‡§∞‡§π‡•á ‡§π‡•à ‡§ï‡§ø‡§≤‡§ï‡§æ‡§∞‡§ø‡§Ø‡§æ‡§Å ‡§ó‡•Ç‡§Ç‡§ú ‡§∞‡§π‡•Ä ‡§π‡•à ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§æ‡§£‡•ç‡§° ‡§Æ‡•á‡§Ç   \n",
       "130079  ‡§ß‡§∞‡§®‡•á ‡§™‡§∞ ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø ‡§Ö‡§¨ ‡§∏‡§Æ‡§Ø ‡§Ü ‡§ó‡§Ø‡§æ ‡§π‡•à‡§Ç ‡§ï‡§ø ‡§ó‡§®‡•ç‡§®‡•á ‡§ï‡•Ä ‡§ñ‡•á‡§§‡•Ä ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§ï‡•á ‡§≤‡§ø‡§Ø‡•á ‡§®‡•Å‡§ï‡§∏‡§æ‡§® ‡§ï‡§æ ‡§∏‡•å‡§¶‡§æ ‡§π‡•ã ‡§ó‡§Ø‡•Ä ‡§π‡•à‡§Ç ‡•§   \n",
       "50146                   ‡§∏‡§°‡§º‡§ï ‡§Æ‡§≤‡§¨‡•á ‡§ï‡§æ ‡§¢‡•á‡§∞ ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§¶‡•Å‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ ‡§ö‡§æ‡§∞ ‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ‡§æ‡§π‡§® ‡§ö‡§æ‡§≤‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§∞‡•á‡§∂‡§æ‡§® ‡§π‡•ã‡§®‡§æ ‡§™‡§°‡§º ‡§∞‡§π‡§æ ‡§π‡•à‡§Ç ‡•§   \n",
       "\n",
       "                                                                                                      output  \\\n",
       "40665       ‡§Ö‡§Å‡§ó‡§∞‡•á‡§ú‡§º‡•Ä ‡§∏‡§π‡§≠‡§æ‡§∑‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§µ‡§π‡§§‡•ç‡§§ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§•‡•Ä ‡§™‡§∞‡§Ç‡§§‡•Å ‡§Ü‡§ú ‡§≠‡•Ä ‡§â‡§∏‡§ï‡§æ ‡§µ‡§∞‡•ç‡§ö‡§∏‡•ç‡§µ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§π‡•à ‡•§    \n",
       "48520                                                                                          ‡§§‡§•‡§æ ‡§ó‡•å‡§∂‡§æ‡§≤‡§æ ‡§ï‡§æ   \n",
       "138403                                              ‡§¨‡§ö‡•ç‡§ö‡•á ‡§ñ‡•á‡§≤ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§ï‡§ø‡§≤‡§ï‡§æ‡§∞‡§ø‡§Ø‡§æ‡§Å ‡§ó‡•Ç‡§Ç‡§ú ‡§∞‡§π‡•Ä ‡§π‡•à‡§Ç ‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§æ‡§£‡•ç‡§° ‡§Æ‡•á‡§Ç   \n",
       "130079  ‡§ß‡§∞‡§®‡•á ‡§™‡§∞ ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø ‡§Ö‡§¨ ‡§∏‡§Æ‡§Ø ‡§Ü ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ó‡§®‡•ç‡§®‡•á ‡§ï‡•Ä ‡§ñ‡•á‡§§‡•Ä ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã ‡§ï‡•á ‡§≤‡§ø‡§Ø‡•á ‡§®‡•Å‡§ï‡§∏‡§æ‡§® ‡§ï‡§æ ‡§∏‡•å‡§¶‡§æ ‡§π‡•ã ‡§ó‡§Ø‡•Ä ‡§π‡•à ‡•§    \n",
       "50146                  ‡§∏‡§°‡§º‡§ï ‡§Æ‡§≤‡§¨‡•á ‡§ï‡§æ ‡§¢‡•á‡§∞ ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§¶‡•Å‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ ‡§ö‡§æ‡§∞ ‡§™‡§π‡§ø‡§Ø‡§æ ‡§µ‡§æ‡§π‡§® ‡§ö‡§æ‡§≤‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§∞‡•á‡§∂‡§æ‡§® ‡§π‡•ã‡§®‡§æ ‡§™‡§°‡§º ‡§∞‡§π‡§æ ‡§π‡•à ‡•§    \n",
       "\n",
       "        input_token_len  \n",
       "40665                28  \n",
       "48520                 6  \n",
       "138403               14  \n",
       "130079               29  \n",
       "50146                23  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd321d92-6f3a-40ee-ba4a-907be30ea244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14000.000000\n",
       "mean        21.935000\n",
       "std         12.422581\n",
       "min          4.000000\n",
       "25%         14.000000\n",
       "50%         19.000000\n",
       "75%         27.000000\n",
       "max        308.000000\n",
       "Name: input_token_len, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['input_token_len'].describe()\n",
    "\n",
    "#train_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5e933a2-ae5f-4c20-87b4-4dbb99b2c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use a token length of 64 since it will cover the vast majority of examples\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ca6fab2-ae06-441f-807e-da87e843249b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1823"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.input.str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d75fca39-9ccb-4bce-8a14-ca5d573cb60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GrammarDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer,print_text=False):         \n",
    "        self.dataset = dataset\n",
    "        self.pad_to_max_length = False\n",
    "        self.tokenizer = tokenizer\n",
    "        self.print_text = print_text\n",
    "        #self.max_len = train_df.input.str.len().max()\n",
    "        self.max_len = 64\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "    def tokenize_data(self, example):\n",
    "        input_, target_ = example['input'], example['output']\n",
    "\n",
    "        # tokenize inputs\n",
    "        tokenized_inputs = tokenizer(input_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "    \n",
    "        tokenized_targets = tokenizer(target_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "\n",
    "        inputs={\"input_ids\": tokenized_inputs['input_ids'],\n",
    "            \"attention_mask\": tokenized_inputs['attention_mask'],\n",
    "            \"labels\": tokenized_targets['input_ids']\n",
    "        }\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenize_data(self.dataset[index])\n",
    "        \n",
    "        if self.print_text:\n",
    "            for k in inputs.keys():\n",
    "                print(k, len(inputs[k]))\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8aa2940-f691-4bc5-9c05-d70ff42e05ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids 27\n",
      "attention_mask 27\n",
      "labels 27\n",
      "{'input_ids': [104, 75159, 1154, 7899, 106524, 3768, 1169, 16015, 1250, 2316, 1419, 1123, 7132, 64219, 2115, 1125, 30361, 1438, 1228, 106524, 1117, 10171, 1194, 1254, 7544, 492, 105], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [104, 75159, 1154, 7899, 106524, 3768, 1169, 16015, 1250, 2316, 1243, 1123, 7132, 64219, 2115, 1125, 30361, 1438, 1228, 106524, 1117, 10171, 1194, 1254, 7544, 492, 105]}\n"
     ]
    }
   ],
   "source": [
    "dataset = GrammarDataset(test_dataset, tokenizer, True)\n",
    "print(dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc0564-2706-4d95-8dfb-e9970a0e2a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e6bd3fc-b11e-47a1-b473-10705ebe1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Evaluator\n",
    "#!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f50438a6-1ed2-460a-8bb0-307fa7b23806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf186c82-6bea-4bb3-b9f3-1b00df72ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model\n",
    "#data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest', return_tensors='pt')\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest', return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6dd5075a-bbc6-46af-bae1-c22cae7eb799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining training related arguments\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"models/hindi/mT5\",\n",
    "    #output_dir =None,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.1,\n",
    "    save_steps=300000,\n",
    "    num_train_epochs=5,\n",
    "    #dataloader_num_workers= 2,\n",
    "    predict_with_generate=True,\n",
    "    #bf16 = True\n",
    "    #fp16=True,\n",
    "    #logging_dir=\"logs/hindi/T5_muRIL_3\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b108ae3-2088-44fa-af0f-8b3583bd157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# defining training related arguments\n",
    "#batch_size = 8\n",
    "'''\n",
    "args = Seq2SeqTrainingArguments(output_dir=\"models/hindi/T5_muRIL_3\",\n",
    "                        evaluation_strategy=\"epoch\",\n",
    "                        per_device_train_batch_size=batch_size,\n",
    "                        per_device_eval_batch_size=batch_size,\n",
    "                        learning_rate=2e-5,\n",
    "                        num_train_epochs=1,\n",
    "                        weight_decay=0.1,\n",
    "                        save_total_limit=2,\n",
    "                        predict_with_generate=True,\n",
    "                        fp16 = True,\n",
    "                        gradient_accumulation_steps = 6,\n",
    "                        #eval_steps = 500,\n",
    "                        #save_steps = 2000,\n",
    "                        #dataloader_num_workers= 4,\n",
    "                        load_best_model_at_end=True,\n",
    "                        load_best_model_at_end=True,\n",
    "                        metric_for_best_model=\"eval_loss\",\n",
    "                        greater_is_better=False,\n",
    "                        early_stopping_patience=5,\n",
    "                        early_stopping_threshold=0.01\n",
    "                        logging_dir=\"logs/hindi/T5_muRIL_3\")\n",
    "\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"models/hindi/mT5\",\n",
    "    #output_dir =None,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy =\"epoch\",\n",
    "    learning_rate=2e-3,\n",
    "    do_eval =True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.1,\n",
    "    save_steps=5000,\n",
    "    eval_steps = 5000,\n",
    "    num_train_epochs=1,\n",
    "    #dataloader_num_workers= 2,\n",
    "    predict_with_generate=True,\n",
    "    #fp16=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    #greater_is_better=False,\n",
    "    load_best_model_at_end =True,\n",
    "    #logging_dir=\"logs/hindi/T5_muRIL_3\"\n",
    "\n",
    ")\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2a430e6-c635-43bb-a305-357ac0773191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"false\"\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfbcd6b1-bdcb-4f9d-a7ca-cd6924d7e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d9743c7-8b58-4bee-9ab1-2fcd8032b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining trainer using ü§ó\n",
    "trainer = Seq2SeqTrainer(model=model, \n",
    "                args=args, \n",
    "                train_dataset= GrammarDataset(train_dataset, tokenizer),\n",
    "                eval_dataset=GrammarDataset(test_dataset, tokenizer),\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator)\n",
    "                #compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0309b357-36c3-4a1e-8449-a7c60ccb2df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 126000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 157500\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157500' max='157500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157500/157500 25:57:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.802300</td>\n",
       "      <td>0.627577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.672700</td>\n",
       "      <td>0.494649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.483300</td>\n",
       "      <td>0.368540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.298100</td>\n",
       "      <td>0.217544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.120800</td>\n",
       "      <td>0.102051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=157500, training_loss=0.5695351831345331, metrics={'train_runtime': 93474.1085, 'train_samples_per_second': 6.74, 'train_steps_per_second': 1.685, 'total_flos': 2.176875283009536e+16, 'train_loss': 0.5695351831345331, 'epoch': 5.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"     #Disabling Wandb\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ace5b-4bb2-4210-8ada-2aef1e6f1968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to mT5_Etoori_3\n",
      "Configuration saved in mT5_Etoori_3/config.json\n",
      "Model weights saved in mT5_Etoori_3/pytorch_model.bin\n",
      "tokenizer config file saved in mT5_Etoori_3/tokenizer_config.json\n",
      "Special tokens file saved in mT5_Etoori_3/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('mT5_Etoori_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417ddf04-2f64-4680-8cf0-fd3715338aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50e692da-5dee-4325-b082-b660f82d1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c1861d5-ba02-4678-a3dd-1b4678a30680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 126000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 157500\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157500' max='157500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157500/157500 26:14:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>0.497808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.604500</td>\n",
       "      <td>0.454815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.433100</td>\n",
       "      <td>0.329243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.276800</td>\n",
       "      <td>0.194706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.095339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=157500, training_loss=0.4741298280504015, metrics={'train_runtime': 94469.5906, 'train_samples_per_second': 6.669, 'train_steps_per_second': 1.667, 'total_flos': 2.176875283009536e+16, 'train_loss': 0.4741298280504015, 'epoch': 5.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"     #Disabling Wandb\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f81846b6-91c5-4de4-b0d3-888596a66426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to mT5_Etoori_2\n",
      "Configuration saved in mT5_Etoori_2/config.json\n",
      "Model weights saved in mT5_Etoori_2/pytorch_model.bin\n",
      "tokenizer config file saved in mT5_Etoori_2/tokenizer_config.json\n",
      "Special tokens file saved in mT5_Etoori_2/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('mT5_Etoori_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf27e3-c604-4062-b629-ab3cd51f5652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcfadbb-c9f3-493f-9c68-7b58831dfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trained on 1k train data (Etoori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15a8fd57-d3c2-4769-b711-005fd50438e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 900\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 11:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.294400</td>\n",
       "      <td>0.514676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.294400</td>\n",
       "      <td>0.486894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.545500</td>\n",
       "      <td>0.451113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.235400</td>\n",
       "      <td>0.418423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 3\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 3\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 3\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 3\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 3\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.6917728068033854, metrics={'train_runtime': 710.9826, 'train_samples_per_second': 6.329, 'train_steps_per_second': 2.11, 'total_flos': 142796496107520.0, 'train_loss': 0.6917728068033854, 'epoch': 5.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"     #Disabling Wandb\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a261868a-6146-417a-9f51-f61ebaafc2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to mT5_Etoori_1\n",
      "Configuration saved in mT5_Etoori_1/config.json\n",
      "Model weights saved in mT5_Etoori_1/pytorch_model.bin\n",
      "tokenizer config file saved in mT5_Etoori_1/tokenizer_config.json\n",
      "Special tokens file saved in mT5_Etoori_1/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('mT5_Etoori_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aca191-4320-4176-adac-cdbc92a42929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3731af9b-87ee-4513-a23e-eb7dd8b4a3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ce6b8-90da-43d6-9bb9-c3561f134c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ac8fea8-69b4-4b3b-bf98-21c4921e3116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 126000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5250\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5250' max='5250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5250/5250 1:33:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 14000\n",
      "  Batch size = 24\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5250, training_loss=0.0, metrics={'train_runtime': 5599.1057, 'train_samples_per_second': 22.504, 'train_steps_per_second': 0.938, 'total_flos': 1.8612324725170176e+16, 'train_loss': 0.0, 'epoch': 1.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"     #Disabling Wandb\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed8776-52ca-4d06-9e15-158994267915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch_device = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3895631b-8f35-4ea2-930f-ef9c56e17072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56ca27-2f01-4566-bd4f-6ac6d509d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da45d37b-cd1f-43e7-a4e2-bc18ef984b14",
   "metadata": {},
   "source": [
    "# Testing Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ea2de44-6fbe-40a6-ba4f-f074a82b6767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--google--muril-base-cased/snapshots/afd9f36c7923d54e97903922ff1b260d091d202f/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google/muril-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 197285\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /DATA/gupta92/.cache/huggingface/hub/models--google--muril-base-cased/snapshots/afd9f36c7923d54e97903922ff1b260d091d202f/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--google--muril-base-cased/snapshots/afd9f36c7923d54e97903922ff1b260d091d202f/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--google--muril-base-cased/snapshots/afd9f36c7923d54e97903922ff1b260d091d202f/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--google--muril-base-cased/snapshots/afd9f36c7923d54e97903922ff1b260d091d202f/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google/muril-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 197285\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /DATA/gupta92/.cache/huggingface/hub/models--google--muril-base-cased/snapshots/afd9f36c7923d54e97903922ff1b260d091d202f/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google/muril-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 197285\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "import torch\n",
    "from transformers import T5Tokenizer, MT5ForConditionalGeneration,AutoTokenizer\n",
    "model_name = 'mT5_Etoori_2'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-base\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98143e4e-6280-4ab1-96a5-2be73cad8d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file mT5_Etoori_2/config.json\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"mT5_Etoori_1\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "loading weights file mT5_Etoori_2/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at mT5_Etoori_2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "trained_model = MT5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a92c890-da13-4043-b074-2123e81e9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_model1 = MT5ForConditionalGeneration.from_pretrained(\"google/Mt5-xl\").to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee6d65-83bc-4bb6-bdbc-8511c6becea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ac8e39b-67ff-4969-a51f-ed70c50cca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gec_prediction(incorrect,correct):\n",
    "    l = len(correct)\n",
    "    predicted_s = correct_grammar(incorrect, num_return_sequences=4)[0]\n",
    "    p = len(predicted_s)\n",
    "    if(p>l):\n",
    "        predicted_s = predicted_s[:l]\n",
    "    elif (p<l-4):\n",
    "        predicted_s = incorrect\n",
    "    return predicted_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6e20830-2f49-4050-b1cd-67ebdb5a4534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 1145.083MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in trained_model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in trained_model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9669013c-f34f-44b1-a35c-f265f106a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grammar(input_text,num_return_sequences):\n",
    "    #batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\").to(torch_device)\n",
    "    batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\",return_token_type_ids=False).to(device)\n",
    "    #translated= trained_model.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    translated= trained_model.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    #print(\"here\")\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b786ccc2-8dea-4c4d-b60c-c242ef187a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(T5ForConditionalGeneration.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b28c83a6-7420-49d1-a2c1-369c78b33644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§® ‡§¶‡•á‡§∞‡•Ä ‡§∏‡•á ‡§™‡§π‡•Å‡§Ç‡§ö‡•á ‡§î‡§∞ ‡§¨‡§ø‡§®‡§æ ‡§∏‡•Ç‡§ö‡§®‡§æ ‡§ï‡•á ‡§ó‡•à‡§∞‡§π‡§æ‡§ú‡§ø‡§∞ ‡§°‡•â‡§ï‡•ç‡§ü‡§∞‡•ã‡§Ç ‡§ï‡•ã ‡§®‡•ã‡§ü‡§ø‡§∏ ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§è ‡§π‡•à‡§Ç ‡•§\n",
      "Correct Seentence: ‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§® ‡§®‡•á ‡§¶‡•á‡§∞‡•Ä ‡§∏‡•á ‡§™‡§π‡•Å‡§Ç‡§ö‡•á ‡§î‡§∞ ‡§¨‡§ø‡§®‡§æ ‡§∏‡•Ç‡§ö‡§®‡§æ ‡§ï‡•á ‡§ó‡•à‡§∞‡§π‡§æ‡§ú‡§ø‡§∞ ‡§°‡•â‡§ï‡•ç‡§ü‡§∞‡•ã‡§Ç ‡§ï‡•ã ‡§®‡•ã‡§ü‡§ø‡§∏ ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§è ‡§π‡•à‡§Ç ‡•§ \n",
      "Predicted Sentence: ‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§® ‡§®‡•á ‡§¶‡•á‡§∞‡•Ä ‡§∏‡•á ‡§™‡§π‡•Å‡§Ç‡§ö‡•á ‡§î‡§∞ ‡§¨‡§ø‡§®‡§æ ‡§∏‡•Ç‡§ö‡§®‡§æ ‡§ï‡•á ‡§ó‡•à‡§∞‡§π‡§æ‡§ú‡§ø‡§∞ ‡§°‡•â‡§ï‡•ç‡§ü‡§∞‡•ã‡§Ç ‡§ï‡•ã ‡§®‡•ã‡§ü‡§ø‡§∏ ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§è ‡§π‡•à‡§Ç ‡•§ \n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[100]\n",
    "correct = test_df['output'].iat[100]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "\n",
    "pred = gec_prediction(text,correct)\n",
    "print(\"Predicted Sentence:\",pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44d83c4e-4a3f-4cfd-bbb6-0d2d457e825a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§á‡§∏‡§∏‡•á ‡§ï‡•á‡§∏‡•Ä ‡§ï‡§æ‡§Æ‡•ç‡§™‡•ç‡§≤‡•á‡§ï‡•ç‡§∏ ‡§Æ‡•á‡§Ç ‡§ï‡§∞‡§µ‡§æ‡§è ‡§ó‡§è ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§Ö‡§µ‡•à‡§ß ‡§ï‡•Ä ‡§∂‡•ç‡§∞‡•á‡§£‡•Ä ‡§Æ‡•á‡§Ç ‡§Ü‡§§‡•á ‡§π‡•à ‡•§\n",
      "Correct Seentence: ‡§á‡§∏‡§∏‡•á ‡§ï‡•á‡§∏‡•Ä ‡§ï‡§æ‡§Æ‡•ç‡§™‡•ç‡§≤‡•á‡§ï‡•ç‡§∏ ‡§Æ‡•á‡§Ç ‡§ï‡§∞‡§µ‡§æ‡§è ‡§ó‡§è ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§Ö‡§µ‡•à‡§ß ‡§ï‡•Ä ‡§∂‡•ç‡§∞‡•á‡§£‡•Ä ‡§Æ‡•á‡§Ç ‡§Ü‡§§‡•á ‡§π‡•à‡§Ç ‡•§ \n",
      "Predicted Sentence: ‡§á‡§∏‡§∏‡•á ‡§ï‡•á‡§∏‡•Ä ‡§ï‡§æ‡§Æ‡•ç‡§™‡•ç‡§≤‡•á‡§ï‡•ç‡§∏ ‡§Æ‡•á‡§Ç ‡§ï‡§∞‡§µ‡§æ‡§è ‡§ó‡§è ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§Ö‡§µ‡•à‡§ß ‡§ï‡•Ä ‡§∂‡•ç‡§∞‡•á‡§£‡•Ä ‡§Æ‡•á‡§Ç ‡§Ü‡§§‡•á ‡§π‡•à‡§Ç ‡•§ \n"
     ]
    }
   ],
   "source": [
    "text = test_df['input'].iat[1000]\n",
    "correct = test_df['output'].iat[1000]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "\n",
    "pred = gec_prediction(text,correct)\n",
    "print(\"Predicted Sentence:\",pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b37d9f-25df-483f-a3c3-29603e854408",
   "metadata": {},
   "source": [
    "# Performance Analysis\n",
    "1. BLEU Score\n",
    "2. GLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18317b87-03d3-4d73-ae22-9cf8cb8e1db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da058ef4-524c-4808-a1ba-bbd143f534d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f87cd-f789-4070-898d-25f6937deb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fdee8-3408-49be-a09a-f466f9599502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "acc0b282-2efa-4674-9c26-7c282fc86759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [01:43,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  100  data point 0.8816151651320783\n",
      "GELU Score =  0.8816151651320783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "GLEU_val_emb = []\n",
    "test_data = test_df.head(100)\n",
    "print(test_data.shape)\n",
    "itr = 0\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        itr+=1\n",
    "        text = str(i.input)\n",
    "        #print(text)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.output).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        GLEU_val_emb.append(b)\n",
    "        if(itr%100 ==0):\n",
    "            print(\"GELU Score for \",itr,\" data point\",np.mean(GLEU_val_emb))\n",
    "    except:\n",
    "        print(\"Error\") \n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2130e4-b422-40d3-be84-730ada29c6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a435a7a2-f401-4454-bbb9-eef3ea7dace1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4147c9-ef27-49bd-ace2-618bbfffe21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d88fd148-1e9a-462c-87d1-ca731d01098c",
   "metadata": {},
   "source": [
    "# Evaluation on Etoori's Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c922895e-82d6-43db-a2c8-113bacd42372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv('DATA/etoori_test.csv')\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab40e4eb-00a5-429e-a837-ed75ea61cc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc_input</th>\n",
       "      <th>dec_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ ‡§Æ‡§æ‡§á‡§ï‡§≤ ‡§∂‡•Ç‡§Æ‡§æ‡§ï‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ö‡§≤‡§æ‡§à ‡§ó‡§à ‡§è‡§ï ‡§´‡§∞‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§∞ ‡§≠‡•Ä ‡§ï‡•ã ‡§Æ‡§ø‡§≤‡§ø‡§Ø‡§® ‡§°‡•â‡§≤‡§∞ ‡§ï‡•ç‡§≤‡§¨ ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§®‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§</td>\n",
       "      <td>‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ ‡§Æ‡§æ‡§á‡§ï‡§≤ ‡§∂‡•Ç‡§Æ‡§æ‡§ï‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ö‡§≤‡§æ‡§à ‡§ó‡§à ‡§è‡§ï ‡§´‡§∞‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§∞ ‡§ï‡•ã ‡§≠‡•Ä ‡§Æ‡§ø‡§≤‡§ø‡§Ø‡§® ‡§°‡•â‡§≤‡§∞ ‡§ï‡•ç‡§≤‡§¨ ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§®‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡§Ø‡§π ‡§Æ‡§® ‡§ï‡•ã ‡§ï‡§æ‡§¨‡•Ç ‡§Æ‡•á‡§Ç ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§π‡•à‡§Ç ‡§á‡§∏‡•Ä‡§≤‡§ø‡§è ‡§á‡§∏‡•á ‡§ö‡§ø‡§§‡•ç‡§§ ‡§π‡§∏‡•ç‡§§ ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§Ø‡•ã‡§ó ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç‡§Ç ‡•§</td>\n",
       "      <td>‡§Ø‡§π ‡§Æ‡§® ‡§ï‡•ã ‡§ï‡§æ‡§¨‡•Ç ‡§Æ‡•á‡§Ç ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§π‡•à ‡§á‡§∏‡•Ä‡§≤‡§ø‡§è ‡§á‡§∏‡•á ‡§ö‡§ø‡§§‡•ç‡§§ ‡§π‡§∏‡•ç‡§§ ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§Ø‡•ã‡§ó ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡§Ü‡§™ ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§™‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡§≤‡•ç‡§≤‡•Ä‡§® ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§¨‡§ö‡•ç‡§ö‡§æ ‡§â‡§∏‡§®‡•á‡§æ‡§Å ‡§™‡§∞ ‡§∂‡•ã‡§∞ ‡§Æ‡§ö‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§</td>\n",
       "      <td>‡§Ü‡§™ ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§™‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡§≤‡•ç‡§≤‡•Ä‡§® ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§¨‡§ö‡•ç‡§ö‡§æ ‡§µ‡§π‡§æ‡§Å ‡§™‡§∞ ‡§∂‡•ã‡§∞ ‡§Æ‡§ö‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§ï‡•á ‡§Æ‡•Å‡§§‡§æ‡§¨‡§ø‡§ï ‡§ß‡§Æ‡§æ‡§ï‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§≤‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§ï‡•ã‡§à ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§≤‡•ã‡§ó ‡§•‡•á ‡§î‡§∞ ‡§Ø‡•á ‡§ó‡§≤‡§§ ‡§™‡§π‡§ö‡§æ‡§® ‡§ï‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡§æ ‡§π‡•à‡§Ç ‡•§</td>\n",
       "      <td>‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§ï‡•á ‡§Æ‡•Å‡§§‡§æ‡§¨‡§ø‡§ï ‡§ß‡§Æ‡§æ‡§ï‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§≤‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§ï‡•ã‡§à ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§≤‡•ã‡§ó ‡§•‡•á ‡§î‡§∞ ‡§Ø‡•á ‡§ó‡§≤‡§§ ‡§™‡§π‡§ö‡§æ‡§® ‡§ï‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡§æ ‡§π‡•à ‡•§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡§â‡§®‡§ï‡•Ä ‡§µ‡•ã ‡§µ‡§æ‡§≤‡•Ä ‡§¨‡§æ‡§§ ‡§≠‡•Ä ‡§Ö‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§∞‡§π‡§§‡•Ä ‡§π‡•à‡§Ç ‡•§</td>\n",
       "      <td>‡§â‡§®‡§ï‡•Ä ‡§µ‡•ã ‡§µ‡§æ‡§≤‡•Ä ‡§¨‡§æ‡§§ ‡§≠‡•Ä ‡§Ö‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§∞‡§π‡§§‡•Ä ‡§π‡•à ‡•§</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         enc_input  \\\n",
       "0  ‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ ‡§Æ‡§æ‡§á‡§ï‡§≤ ‡§∂‡•Ç‡§Æ‡§æ‡§ï‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ö‡§≤‡§æ‡§à ‡§ó‡§à ‡§è‡§ï ‡§´‡§∞‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§∞ ‡§≠‡•Ä ‡§ï‡•ã ‡§Æ‡§ø‡§≤‡§ø‡§Ø‡§® ‡§°‡•â‡§≤‡§∞ ‡§ï‡•ç‡§≤‡§¨ ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§®‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§   \n",
       "1              ‡§Ø‡§π ‡§Æ‡§® ‡§ï‡•ã ‡§ï‡§æ‡§¨‡•Ç ‡§Æ‡•á‡§Ç ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§π‡•à‡§Ç ‡§á‡§∏‡•Ä‡§≤‡§ø‡§è ‡§á‡§∏‡•á ‡§ö‡§ø‡§§‡•ç‡§§ ‡§π‡§∏‡•ç‡§§ ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§Ø‡•ã‡§ó ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç‡§Ç ‡•§   \n",
       "2                               ‡§Ü‡§™ ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§™‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡§≤‡•ç‡§≤‡•Ä‡§® ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§¨‡§ö‡•ç‡§ö‡§æ ‡§â‡§∏‡§®‡•á‡§æ‡§Å ‡§™‡§∞ ‡§∂‡•ã‡§∞ ‡§Æ‡§ö‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§   \n",
       "3    ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§ï‡•á ‡§Æ‡•Å‡§§‡§æ‡§¨‡§ø‡§ï ‡§ß‡§Æ‡§æ‡§ï‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§≤‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§ï‡•ã‡§à ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§≤‡•ã‡§ó ‡§•‡•á ‡§î‡§∞ ‡§Ø‡•á ‡§ó‡§≤‡§§ ‡§™‡§π‡§ö‡§æ‡§® ‡§ï‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡§æ ‡§π‡•à‡§Ç ‡•§   \n",
       "4                                                          ‡§â‡§®‡§ï‡•Ä ‡§µ‡•ã ‡§µ‡§æ‡§≤‡•Ä ‡§¨‡§æ‡§§ ‡§≠‡•Ä ‡§Ö‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§∞‡§π‡§§‡•Ä ‡§π‡•à‡§Ç ‡•§   \n",
       "\n",
       "                                                                                          dec_input  \n",
       "0  ‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ ‡§Æ‡§æ‡§á‡§ï‡§≤ ‡§∂‡•Ç‡§Æ‡§æ‡§ï‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ö‡§≤‡§æ‡§à ‡§ó‡§à ‡§è‡§ï ‡§´‡§∞‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§∞ ‡§ï‡•ã ‡§≠‡•Ä ‡§Æ‡§ø‡§≤‡§ø‡§Ø‡§® ‡§°‡•â‡§≤‡§∞ ‡§ï‡•ç‡§≤‡§¨ ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§®‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§   \n",
       "1                ‡§Ø‡§π ‡§Æ‡§® ‡§ï‡•ã ‡§ï‡§æ‡§¨‡•Ç ‡§Æ‡•á‡§Ç ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§π‡•à ‡§á‡§∏‡•Ä‡§≤‡§ø‡§è ‡§á‡§∏‡•á ‡§ö‡§ø‡§§‡•ç‡§§ ‡§π‡§∏‡•ç‡§§ ‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ ‡§Ø‡•ã‡§ó ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç ‡•§   \n",
       "2                                 ‡§Ü‡§™ ‡§™‡•Å‡§∏‡•ç‡§§‡§ï ‡§™‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡§≤‡•ç‡§≤‡•Ä‡§® ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§¨‡§ö‡•ç‡§ö‡§æ ‡§µ‡§π‡§æ‡§Å ‡§™‡§∞ ‡§∂‡•ã‡§∞ ‡§Æ‡§ö‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§   \n",
       "3     ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§ï‡•á ‡§Æ‡•Å‡§§‡§æ‡§¨‡§ø‡§ï ‡§ß‡§Æ‡§æ‡§ï‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§≤‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§ï‡•ã‡§à ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§≤‡•ã‡§ó ‡§•‡•á ‡§î‡§∞ ‡§Ø‡•á ‡§ó‡§≤‡§§ ‡§™‡§π‡§ö‡§æ‡§® ‡§ï‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡§æ ‡§π‡•à ‡•§   \n",
       "4                                                           ‡§â‡§®‡§ï‡•Ä ‡§µ‡•ã ‡§µ‡§æ‡§≤‡•Ä ‡§¨‡§æ‡§§ ‡§≠‡•Ä ‡§Ö‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§∞‡§π‡§§‡•Ä ‡§π‡•à ‡•§   "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9216427e-77e4-4d58-a4c1-0fd582745e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ ‡§Æ‡§æ‡§á‡§ï‡§≤ ‡§∂‡•Ç‡§Æ‡§æ‡§ï‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ö‡§≤‡§æ‡§à ‡§ó‡§à ‡§è‡§ï ‡§´‡§∞‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§∞ ‡§≠‡•Ä ‡§ï‡•ã ‡§Æ‡§ø‡§≤‡§ø‡§Ø‡§® ‡§°‡•â‡§≤‡§∞ ‡§ï‡•ç‡§≤‡§¨ ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§®‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§\n",
      "Correct Seentence: ‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ ‡§Æ‡§æ‡§á‡§ï‡§≤ ‡§∂‡•Ç‡§Æ‡§æ‡§ï‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ö‡§≤‡§æ‡§à ‡§ó‡§à ‡§è‡§ï ‡§´‡§∞‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§∞ ‡§ï‡•ã ‡§≠‡•Ä ‡§Æ‡§ø‡§≤‡§ø‡§Ø‡§® ‡§°‡•â‡§≤‡§∞ ‡§ï‡•ç‡§≤‡§¨ ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§®‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§ \n",
      "Predicted Sentence: ‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ ‡§Æ‡§æ‡§á‡§ï‡§≤ ‡§∂‡•Ç‡§Æ‡§æ‡§ï‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ö‡§≤‡§æ‡§à ‡§ó‡§à ‡§è‡§ï ‡§´‡§∞‡§æ‡§∞‡•Ä ‡§ï‡§æ‡§∞ ‡§ï‡•ã ‡§≠‡•Ä ‡§Æ‡§ø‡§≤‡§ø‡§Ø‡§® ‡§°‡•â‡§≤‡§∞ ‡§ï‡•ç‡§≤‡§¨ ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§®‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‡•§ \n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[0]\n",
    "correct = df_test['dec_input'].iat[0]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9eef891-a11f-47ae-9f15-17cf039ec8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sentence: ‡§ò‡§∞ ‡§Æ‡•á‡§Ç ‡§ò‡•Å‡§∏ ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§≠‡•Ä ‡§∏‡•Å‡§ï‡•Ç‡§® ‡§®‡§π‡•Ä‡§Ç ‡•§\n",
      "Correct Seentence: ‡§ò‡§∞ ‡§Æ‡•á‡§Ç ‡§ò‡•Å‡§∏‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§≠‡•Ä ‡§∏‡•Å‡§ï‡•Ç‡§® ‡§®‡§π‡•Ä‡§Ç ‡•§ \n",
      "Predicted Sentence: ‡§ò‡§∞ ‡§Æ‡•á‡§Ç ‡§ò‡•Å‡§∏‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§≠‡•Ä ‡§∏‡•Å‡§ï‡•Ç‡§® ‡§®‡§π‡•Ä‡§Ç ‡•§ \n"
     ]
    }
   ],
   "source": [
    "text = df_test['enc_input'].iat[50]\n",
    "correct = df_test['dec_input'].iat[50]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7fa5d601-9ba5-4961-8b1c-cb658cbcfdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 999 ms, sys: 52.1 ms, total: 1.05 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d972ce-dab2-46ac-9de8-f0ffcfb53f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f63ceef-8b22-4af4-85b7-3b8ea69b275b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffee1d-4065-4509-98cf-069cb233593b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1c130-a68b-4bf1-86d9-d677f8ed9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "BLEU = []\n",
    "index = []\n",
    "test_data = df_test.head(10000)\n",
    "np.random.seed(1)\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        #print(act)\n",
    "        #print(pred_s)\n",
    "        b = bleu.sentence_bleu(act,pred_s)\n",
    "        BLEU.append(b)\n",
    "    except:\n",
    "        index.append(ind)\n",
    "        continue\n",
    "print(\"BELU = \", np.mean(BLEU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f19ddaa2-1873-4cab-9974-353a53fdc8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [17:39,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score =  0.9016537992385885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test.head(1000)\n",
    "print(test_data.shape)\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "    \n",
    "        #print(act)\n",
    "        #print(pred_s)\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        #print(b)\n",
    "        GLEU_val_emb.append(b)\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb825de7-9bcd-4d6e-8ba4-433f7ac00ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  1  data point 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [17:34,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  1001  data point 0.9017520471913971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [34:47,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  2001  data point 0.9048548314014055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [51:51,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  3001  data point 0.904020091428648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [1:09:07,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  4001  data point 0.9024014608107644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [1:26:39,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  5001  data point 0.9025879238101148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6001it [1:44:02,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  6001  data point 0.902656529745989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7001it [2:00:57,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  7001  data point 0.9028826783465198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8001it [2:17:46,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  8001  data point 0.9031189247194619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9001it [2:34:34,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score for  9001  data point 0.9025801389217523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [2:51:36,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score =  0.9023208220868466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test.head(10000)\n",
    "print(test_data.shape)\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        GLEU_val_emb.append(b)\n",
    "        if(ind%1000 ==0):\n",
    "            print(\"GELU Score for \",ind+1,\" data point\",np.mean(GLEU_val_emb))\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f776a52-2f84-4b17-a268-62f049e9cc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU Score =  0.9023208220868466\n"
     ]
    }
   ],
   "source": [
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b12e786-b7d7-4c70-8bce-592d1b146bcc",
   "metadata": {},
   "source": [
    "# Testing HiWikEd test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3252f34-628a-4e12-9569-b81228062c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test1 = pd.read_csv('DATA/HiWikEd.csv')\n",
    "df_test1.shape\n",
    "df_test1 = df_test1.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875fa5f4-1261-42a1-a57e-021fe3debaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2982ca8d-6fdb-4c1c-b153-1006de98725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_test1['enc_input'].iat[45]\n",
    "correct = df_test1['dec_input'].iat[45]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1632b-6e87-4f62-a306-097d41556f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_test1['enc_input'].iat[40]\n",
    "correct = df_test1['dec_input'].iat[40]\n",
    "print(\"Incorrect Sentence:\", text)\n",
    "print(\"Correct Seentence:\",correct)\n",
    "predicted_s = correct_grammar(text, num_return_sequences=1)[0]\n",
    "l = len(correct)\n",
    "p = len(predicted_s)\n",
    "if(p>l):\n",
    "    predicted_s = predicted_s[:l]\n",
    "\n",
    "print(\"Predicted Sentence:\",predicted_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff017077-1056-4b4b-a24e-966a05fbc2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test1.head(100)\n",
    "print(test_data.shape)\n",
    "\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        GLEU_val_emb.append(b)\n",
    "        if(ind%100 ==0):\n",
    "            print(\"GELU Score for \",ind+1,\" data point\",np.mean(GLEU_val_emb))\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece44c76-ddbd-4136-8ab0-c2f84f33caea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131416a-f3d2-484b-8a46-af8d0b6f7056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test_pron_infl = pd.read_csv('DATA/Wikiedits_PRON_INFL.csv')\n",
    "print(\"PRON_INFL:\", df_test_pron_infl.shape)\n",
    "\n",
    "df_test_adj_infl = pd.read_csv('DATA/Wikiedits_ADJ_INFL.csv')\n",
    "print(\"ADJ_INFL:\", df_test_adj_infl.shape)\n",
    "\n",
    "df_test_adp_infl = pd.read_csv('DATA/Wikiedits_ADP_INFL.csv')\n",
    "print(\"ADP_INFL:\", df_test_adp_infl.shape)\n",
    "\n",
    "df_test_verb_infl = pd.read_csv('DATA/Wikiedits_VERB_INFL.csv')\n",
    "print(\"VERB_INFL:\", df_test_verb_infl.shape)\n",
    "\n",
    "df_test_noun_infl = pd.read_csv('DATA/Wikiedits_NOUN_INFL.csv')\n",
    "print(\"NOUN_INFL:\", df_test_noun_infl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07c8d1-c006-4718-a6eb-9fb4004319ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pron_infl.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff85c3-855f-4c3b-97d7-1dda8a7f6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_adj_infl.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a82a24-47bb-427a-a3d9-13d8bf9062b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test_adj_infl\n",
    "print(test_data.shape)\n",
    "\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        GLEU_val_emb.append(b)\n",
    "        if(ind%50 ==0):\n",
    "            print(\"GELU Score for \",ind+1,\" data point\",np.mean(GLEU_val_emb))\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9cba8-9fc0-425b-a12a-9cf6d051116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GELU Score for ADJ_INFL Error= \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050f72db-9bb9-40d3-97af-ee0d85a3678a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d3387c-c24c-4275-bc5e-efa3f9755f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test_pron_infl\n",
    "print(test_data.shape)\n",
    "\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        GLEU_val_emb.append(b)\n",
    "        if(ind%100 ==0):\n",
    "            print(\"GELU Score for \",ind+1,\" data point\",np.mean(GLEU_val_emb))\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da7565-b8c4-48c2-9d03-10e333bdd0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GELU Score for PRON_INFL Error= \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a058e9-f76a-4fc2-ae80-c4228f9bc61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df3b060-3da3-4c7d-a469-dcd1c3e4b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test_adp_infl.head(100)\n",
    "print(test_data.shape)\n",
    "\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        GLEU_val_emb.append(b)\n",
    "        if(ind%200 ==0):\n",
    "            print(\"GELU Score for \",ind+1,\" data point\",np.mean(GLEU_val_emb))\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152590f-11cb-44b5-8ccd-44e032bcea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GELU Score for ADP_INFL Error= \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4637706-5c90-4f2c-b7b8-25936a08af65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32093bfa-50f9-4d5c-8df0-ac9fecfca16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test_adp_infl\n",
    "print(test_data.shape)\n",
    "\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        GLEU_val_emb.append(b)\n",
    "        if(ind%200 ==0):\n",
    "            print(\"GELU Score for \",ind+1,\" data point\",np.mean(GLEU_val_emb))\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1921f5d8-7445-4d60-8374-7908ef93654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GELU Score for ADP_INFL Error= \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73af636-f27a-480b-a6a5-4339d2c7c23f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d211e56c-9bd1-4d2b-8723-da6e715e50db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test_verb_infl\n",
    "print(test_data.shape)\n",
    "\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        GLEU_val_emb.append(b)\n",
    "        if(ind%1000 ==0):\n",
    "            print(\"GELU Score for \",ind+1,\" data point\",np.mean(GLEU_val_emb))\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab474a7-400b-4eac-a752-1d7c3de32b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GELU Score for VERB_INFL Error= \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2cd3d7-1af4-4f5d-8993-33876292444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test_noun_infl\n",
    "print(test_data.shape)\n",
    "\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        GLEU_val_emb.append(b)\n",
    "        if(ind%50 ==0):\n",
    "            print(\"GELU Score for \",ind+1,\" data point\",np.mean(GLEU_val_emb))\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf2bac-ff9d-4673-81da-1af6588e898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GELU Score for NOUN_INFL Error= \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc57a01-df6a-4982-b7b3-1c2be5ed2e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e14f5-5def-40b4-89a3-be9dc31126c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc825034-6750-4534-bf2a-710b7471f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test1.head(100)\n",
    "print(df_test1.shape)\n",
    "\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        GLEU_val_emb.append(b)\n",
    "        if(ind%20 ==0):\n",
    "            print(\"GELU Score for \",ind+1,\" data point\",np.mean(GLEU_val_emb))\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc574f-b357-4e0c-a9d5-68f7c1284733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad474e87-62ca-401e-96e6-dad1e0179408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "\n",
    "GLEU_val_emb = []\n",
    "test_data = df_test1\n",
    "print(df_test1.shape)\n",
    "\n",
    "for ind,i in tqdm(test_data.iterrows(),position=0):\n",
    "    try:\n",
    "        text = str(i.enc_input)\n",
    "        #pred = predict(str(i.enc_input),model)[0].split()\n",
    "        pred = correct_grammar(text, num_return_sequences=1)[0]\n",
    "        l = len(text)\n",
    "        p = len(pred)\n",
    "        if(p>l):\n",
    "            pred = pred[:l]\n",
    "            \n",
    "        act = [str(i.dec_input).split()]\n",
    "        pred_s = str(pred).split()\n",
    "        b = sentence_gleu(act,pred_s)\n",
    "        GLEU_val_emb.append(b)\n",
    "        if(ind%1000 ==0):\n",
    "            print(\"GELU Score for \",ind+1,\" data point\",np.mean(GLEU_val_emb))\n",
    "    except:\n",
    "        continue\n",
    "print(\"GELU Score = \",np.mean(GLEU_val_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839044f5-f648-4646-9c09-1b6951cec19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F0.5 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c2948-e503-4ad4-b9e2-c9f38b04e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# Define true translations and predicted translations for a set of source texts\n",
    "true_translations = [\"‡§á‡§∏‡•á ‡§ó‡•å‡§∞‡•Ä ‡§∂‡§Ç‡§ï‡§∞ ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§≠‡•Ä ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç.\"]\n",
    "predicted_translations = [\"‡§á‡§∏‡•á ‡§ó‡•å‡§∞‡•Ä ‡§∂‡§Ç‡§ï‡§∞ ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§≠‡•Ä ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç.\"]\n",
    "\n",
    "# Calculate the F0.5 score\n",
    "f_score = fbeta_score(true_translations, predicted_translations, beta=0.5, average='weighted')\n",
    "\n",
    "print(\"F0.5 Score: {:.2f}\".format(f_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
